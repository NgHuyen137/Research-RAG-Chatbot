[
  {
    "user_input": "Halueval được tạo ra để làm gì?",
    "retrieved_contexts": [
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com"
    ],
    "response": "```markdown\n1.  **Mục tiêu của Halueval:**\n\n    *   Halueval được tạo ra để hiểu loại nội dung nào và mức độ mà các LLM (Large Language Models) có xu hướng tạo ra ảo giác (hallucinate).\n\n2.  **Đánh giá hiệu suất:**\n\n    *   Halueval là một bộ sưu tập lớn các mẫu ảo giác (hallucinated) được tạo ra và chú thích bởi con người, dùng để đánh giá hiệu suất của các LLM trong việc nhận biết ảo giác.\n\n3.  **Nội dung của Halueval:**\n\n    *   Halueval bao gồm 35,000 mẫu bình thường và bị ảo giác để phân tích và đánh giá LLM.\n    *   Gồm 5,000 truy vấn người dùng *chung* với phản hồi từ ChatGPT và 30,000 ví dụ *dành riêng cho từng nhiệm vụ* từ ba nhiệm vụ: hỏi đáp, đối thoại dựa trên tri thức và tóm tắt văn bản.\n\n4.  **Mục đích sử dụng:**\n\n    *   Halueval có thể hỗ trợ nghiên cứu về việc LLM có xu hướng tạo ra ảo giác với loại nội dung nào và ở mức độ nào.\n    *   Từ đó, mở đường cho việc xây dựng các LLM hiệu quả và đáng tin cậy hơn trong tương lai.\n```",
    "reference": "Halueval là một chuẩn đánh giá quy mô lớn về hiện tượng ảo giác cho các mô hình ngôn ngữ lớn."
  },
  {
    "user_input": "Có bao nhiêu nhà nghiên cứu đã tham gia vào công trình này?",
    "retrieved_contexts": [
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "**labeler details.** annotating the hallucination\nin chatgpt responses is a very challenging task,\nwhich requires good reading comprehension skills\nand using search engine to look up relevant information for judgement. thus, from an initial pool\nof labeler candidates, we select labelers who are\ngood at english passage reading with at least an\nundergraduate-level education. besides, following\n(ouyang et al., 2022), we have labelers annotate a\nsmall number of test examples and measure their\nagreement with the labels of researchers, and finally\nwe choose thirty human labelers with the highest\nagreement scores. we report fleiss’s kappa ( *κ* ) to\nindicate the reliability of agreement between human labelers. we compute *κ* on 5 *,* 000 annotated\nsamples and obtain *κ* = 0 *.* 811 ( 0 *.* 80 *≤* *κ ≤* 1 *.* 00 )\nshowing a perfect agreement.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a."
    ],
    "response": "Không có tài liệu nào cung cấp thông tin về số lượng nhà nghiên cứu tham gia vào công trình này.",
    "reference": "Có sáu nhà nghiên cứu đã tham gia vào công trình này."
  },
  {
    "user_input": "Đại học Renmin của Trung Quốc tham gia vào nghiên cứu này như thế nào?",
    "retrieved_contexts": [
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a.",
      "|refined response|1. august 2, 1776 - declaration of independence si- gning 2. april 12, 1861 - start of the civil war 3. august 18, 1920 - 19th amendment ratification gr- anting women the right to vote 4. december 7, 1941 - pearl harbor attack during wo- rld war ii 5. september 11, 2001 - terrorist attacks on new york city and washington d.c.|according to world bank data, india and china have shown significant economic growth in the past two de- cades. in 1998, india’s gdp growth rate was 6.2%, while china’s gdp growth rate was 7.8%. by 2019, china’s gdp growth rate was 6.0%, while india’s gdp growth rate was 3.7%.|",
      "or normal output ( *e.g.,* an answer) of each sample\nfor classification. the evaluation instructions of\nqa, dialogue, and summarization are presented in\ntable 13, table 14 and table 15 in appendix c."
    ],
    "response": "```markdown\n1.  **Vai trò của Đại học Renmin của Trung Quốc:**\n\n    *   Trường Gaoling về Trí tuệ Nhân tạo, Đại học Renmin của Trung Quốc tham gia vào công trình nghiên cứu này.\n    *   Trường Thông tin, Đại học Renmin của Trung Quốc cũng tham gia.\n    *   Phòng thí nghiệm trọng điểm Bắc Kinh về Quản lý và Phân tích Dữ liệu Lớn cũng thuộc Đại học Renmin của Trung Quốc.\n```",
    "reference": "Đại học Renmin của Trung Quốc tham gia thông qua Trường Trí tuệ Nhân tạo Gaoling, Trường Thông tin và Phòng thí nghiệm trọng điểm Bắc Kinh về Quản lý và Phân tích Dữ liệu Lớn."
  },
  {
    "user_input": "Các mô hình ngôn ngữ lớn như ChatGPT có xu hướng tạo ra lỗi gì?",
    "retrieved_contexts": [
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "chatgpt cannot access the web to obtain the official data. after providing official information\nretrieved from world bank, the refined span displays answers that contain the correct information.\nthe above two examples illustrate that retrieving\nknowledge related to queries can help chatgpt significantly reduce the hallucinations in the response,\nespecially those factual errors.",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "with respect to the hallucinated samples where\nchatgpt fails to recognize, we present the number\nof each hallucination pattern in table 6. based on\nthe results, we can observe that the hallucination\npatterns of failed samples are unevenly distributed.\nfor example, over half of failures in qa, dialogue,\nand summarization originate from the first hallucination pattern ( *i.e., comprehension*, *extrinsic-soft*,\nand *factual* ), which refers to the hallucinations that\nare factually correct but conflict with the context.\nthis indicates that llms lack or cannot associate"
    ],
    "response": "```markdown\n1.  **Xu hướng tạo ra ảo giác (Hallucinations):**\n\n    *   Các mô hình ngôn ngữ lớn (LLM) như ChatGPT có xu hướng tạo ra \"ảo giác\" (hallucinations).\n    *   Ảo giác ở đây là nội dung mâu thuẫn với nguồn hoặc không thể xác minh bằng kiến thức thực tế.\n    *   Nội dung được tạo ra có thể mâu thuẫn với nguồn hiện có hoặc không thể xác minh bằng các nguồn kiến thức có sẵn.\n\n2.  **Mức độ ảo giác:**\n\n    *   ChatGPT có khả năng tạo ra nội dung bịa đặt, không thể kiểm chứng trong khoảng 19.5% phản hồi.\n\n3.  **Các chủ đề dễ bị ảo giác:**\n\n    *   Các văn bản bịa đặt từ ChatGPT bao gồm các chủ đề như ngôn ngữ, khí hậu và công nghệ.\n\n4.  **Ví dụ về ảo giác:**\n\n    *   ChatGPT có thể tạo ra các thông tin sai lệch, chẳng hạn như trong ví dụ về Alpaca, ChatGPT tạo ra nội dung không thể xác minh bằng nguồn hiện có.\n```",
    "reference": "Các mô hình ngôn ngữ lớn như ChatGPT có xu hướng tạo ra các ảo giác (hallucinations)."
  },
  {
    "user_input": "Nội dung do các mô hình ngôn ngữ lớn tạo ra được coi là 'ảo giác' khi nào?",
    "retrieved_contexts": [
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449"
    ],
    "response": "```markdown\n1.  **Định nghĩa \"ảo giác\" (Hallucination):**\n\n    *   Nội dung do các mô hình ngôn ngữ lớn (LLM) tạo ra được coi là \"ảo giác\" khi nó mâu thuẫn với nguồn thông tin hiện có hoặc không thể kiểm chứng bằng kiến thức thực tế.    *   Điều này có nghĩa là thông tin đó không chính xác, sai sự thật hoặc không có căn cứ.\n```",
    "reference": "Nội dung được coi là 'ảo giác' khi nó mâu thuẫn với nguồn hoặc không thể xác minh bằng kiến thức thực tế."
  },
  {
    "user_input": "Mục đích của việc giới thiệu tiêu chuẩn đánh giá 'hallucination evaluation benchmark' là gì?",
    "retrieved_contexts": [
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "hallucinated content. finally, our benchmark can\nbe further paired with human annotation to assess\nwhether the llms’ output contains hallucinations,\nsince the samples in our benchmark are specially\ndesigned for testing the hallucinations of llms.\nto use our benchmark, users can run the code in\nour project repository to conduct the corresponding\nevaluation and analysis. users can use our provided\ninstructions on their own datasets to evaluate llms\n\non hallucinations."
    ],
    "response": "```markdown\n1.  **Mục đích của Halueval:**\n\n    *   Halueval được giới thiệu để hiểu rõ hơn về các loại nội dung mà LLM (mô hình ngôn ngữ lớn) có xu hướng tạo ra \"ảo giác\" (hallucinate) và mức độ chúng tạo ra những ảo giác này.\n    *   Nói cách khác, Halueval giúp xác định những điểm yếu của LLM trong việc tạo ra thông tin chính xác và đáng tin cậy.\n```",
    "reference": "Mục đích là để hiểu rõ hơn về các loại nội dung và mức độ mà các mô hình ngôn ngữ lớn có xu hướng tạo ra ảo giác."
  },
  {
    "user_input": "Halueval là gì?",
    "retrieved_contexts": [
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future."
    ],
    "response": "```markdown\n1.  **Định nghĩa Halueval:**\n\n    *   Halueval là một bộ tiêu chuẩn đánh giá (benchmark) lớn, được thiết kế để đánh giá khả năng tạo ra \"ảo giác\" (hallucination) của các mô hình ngôn ngữ lớn (LLM).\n    *   Đây là một tập hợp lớn gồm các mẫu dữ liệu, bao gồm cả các mẫu \"ảo giác\" và các mẫu bình thường, được sử dụng để phân tích và đánh giá LLM.\n\n2.  **Quy mô của Halueval:**\n\n    *   Halueval bao gồm 35,000 mẫu, trong đó có 5,000 truy vấn người dùng thông thường với phản hồi từ ChatGPT và 30,000 ví dụ cụ thể cho các nhiệm vụ khác nhau.\n\n3.  **Các nhiệm vụ cụ thể trong Halueval:**\n\n    *   Các nhiệm vụ này bao gồm trả lời câu hỏi (question answering), đối thoại dựa trên kiến thức (knowledge-grounded dialogue) và tóm tắt văn bản (text summarization).\n\n4.  **Phương pháp tạo mẫu:**\n\n    *   Các mẫu trong Halueval được tạo ra thông qua hai phương pháp: tự động tạo và chú thích thủ công bởi con người.\n\n5.  **Mục tiêu của Halueval:**\n\n    *   Halueval được tạo ra để giúp nghiên cứu về việc LLM có xu hướng tạo ra loại nội dung nào và ở mức độ nào.\n    *   Nó cũng nhằm mục đích hỗ trợ xây dựng các LLM hiệu quả và đáng tin cậy hơn trong tương lai.\n```",
    "reference": "Halueval là một tập hợp lớn các mẫu ảo giác được tạo và chú thích bởi con người, dùng để đánh giá khả năng nhận biết ảo giác của các mô hình ngôn ngữ lớn."
  },
  {
    "user_input": "ChatGPT có dễ tạo ra thông tin sai lệch hay không?",
    "retrieved_contexts": [
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "cording to recent work (manakul et al., 2023), hallucinations are likely to appear in diverged and conflicting responses of llms. based on the filtered\nuser queries and chatgpt responses, we invite\nhuman labelers to annotate whether the response\ncontains hallucinated information and mark cor\nresponding spans. as shown in table 1, for the\nuser query “ *retrieve the oldest photo of a cat* ”, the\nresponse generated by chatgpt contains unverifiable information. these human-annotated queries\nand responses can be used to analyze what types\nof content llms tend to hallucinate and further",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "chatgpt cannot access the web to obtain the official data. after providing official information\nretrieved from world bank, the refined span displays answers that contain the correct information.\nthe above two examples illustrate that retrieving\nknowledge related to queries can help chatgpt significantly reduce the hallucinations in the response,\nespecially those factual errors.",
      "**cot reasoning.** in previous work (wei et al.,\n2022), chain-of-thought (cot) has been proposed\nto improve the ability of llms to perform reasoning and derive the final answer by introducing a series of intermediate reasoning steps. here, besides\nproducing the recognition result, we also require\nchatgpt to generate the reasoning steps. while,\nfrom the results in table 8, we observe that generating reasoning steps can mildly improve the performance but makes the model perform worse in qa\nand dialogue ( *e.g.,* dropping from 62.59 to 59.58).\ncompared to retrieving knowledge, adding chainof-thought before output might interfere with the\nfinal judgement. while, in text summarization, generating reasoning steps improve the accuracy from\n58.53 to 61.21. the reason might be that the factual\ncontradiction between document and summary can\nbe identified through logic reasoning.\n\n**sample contrast.** we further provide ground-truth",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations."
    ],
    "response": "```markdown\n1.  **Khả năng tạo thông tin sai lệch của ChatGPT:**\n\n    *   ChatGPT có khả năng tạo ra nội dung sai lệch bằng cách tạo ra các thông tin không thể kiểm chứng.\n    *   Khoảng 19.5% phản hồi từ ChatGPT có thể chứa thông tin sai lệch.\n\n2.  **Chủ đề dễ bị sai lệch:**\n\n    *   Các chủ đề như ngôn ngữ, khí hậu và công nghệ đặc biệt dễ bị tạo ra thông tin sai lệch.\n\n3.  **Đánh giá khả năng nhận biết thông tin sai lệch của LLM:**\n\n    *   Các LLM hiện tại gặp nhiều khó khăn trong việc nhận biết thông tin sai lệch, ngay cả đối với ChatGPT (ví dụ: độ chính xác chỉ khoảng 62.59% trong việc trả lời câu hỏi).\n\n4.  **Cải thiện khả năng nhận biết thông tin sai lệch:**\n\n    *   Hiệu suất của LLM trong việc nhận biết thông tin sai lệch có thể được cải thiện bằng cách cung cấp kiến thức rõ ràng và thêm các bước suy luận trung gian.\n```",
    "reference": "Có vẻ như ChatGPT có khả năng tạo ra thông tin sai lệch, cụ thể là khoảng 19.5% các phản hồi chứa nội dung bịa đặt."
  },
  {
    "user_input": "Những phương pháp nào có thể giúp các mô hình ngôn ngữ lớn (LLM) nhận biết ảo giác tốt hơn?",
    "retrieved_contexts": [
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **4 related work**\n\n**hallucination in llms.** hallucination in llms is\n\nconcerning since it hinders performance and raises\nsafety risks in real-world application. to alleviate\nthis issue, prior studies have proposed to use a verification system to identify non-factual entities in\ntext summarization (zhao et al., 2020), invoke interfaces of structured data ( *e.g.,* knowledge graph,\ndatabase) to obtain related evidence (jiang et al.,\n2023; lan et al., 2022), and train a token-level fact\ncritic to recognize hallucination and rectify them\nin dialogue (dziri et al., 2021). to enhance the\nunderstanding of hallucination in llms and pro\n\n6455",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "llms in three aspects. firstly, based on our generated and annotated samples, researchers can use\nthem to analyze what types of content llms tend\nto generate hallucinations. second, researchers can\nfurther evaluate the ability of llms to recognize\nhallucinations in the generated samples. for example, given a question and an answer, llms can\nbe asked to determine whether the answer contains",
      "table 2: instruction of hallucination sampling for question answering. the blue text denotes the intention descrip\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration.",
      "-----\n\n**tasks** **#sample** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 10000 2280 1378 5102 1240\n**dialogue** 10000 8330 1196 474   **summa.** 10000 2614 3562 3824   \ntable 16: number of generated samples for each hallucination pattern (p-i/ii/iii/iv). “‘summa.” is short\nfor summarization. “-” is due to that we consider three\n\npatterns in dialogue and summarization.\n\n6464\n\n\n-----",
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat",
      "|---|---|---|\n|chatgpt response|1. july 4, 1776 - declaration of independence sign- ing 2. april 12, 1861 - start of the civil war 3. august 18, 1920 - 19th amendment ratification gr- anting women the right to vote 4. december 7, 1941 - pearl harbor attack during wo- rld war ii 5. september 11, 2001 - terrorist attacks on new york city and washington d.c.|as an ai language model, i cannot create visualization but i can provide you with the following information: according to world bank data, india and china have shown significant economic growth in the past two de- cades. in 1998, india’s gdp growth rate was 4.8%, while china’s gdp growth rate was 9.3%. in the following years, the gdp growth rate fluctuat- ed, but china maintained a higher growth rate than india. by 2019, china’s gdp growth rate was 6.1%, while india’s gdp growth rate was 4.2%.|",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "**3.2.2** **improvement strategies**\n\nin this part, we design several strategies to improve\nthe ability of llms to recognize hallucination. the\nresults are shown in table 8.\n\n**knowledge retrieval.** retrieving relevant knowledge is a widely used strategy to eliminate hallucination (lewis et al., 2020; li et al., 2023a). therefore, we supply chatgpt with the knowledge facts\nretrieved from wikipedia (except for that summarization does not need external information besides",
      "hallucination recognition and analyze the information blind spots of llms.\nto better understand the performance of llms\nin halueval, we conduct experiments with several\nexisting powerful llms ( *e.g.,* chatgpt, gpt-3).",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms."
    ],
    "response": "Không có tài liệu nào cung cấp thông tin trực tiếp và đầy đủ về các phương pháp cụ thể để giúp LLM nhận biết \"ảo giác\" tốt hơn.",
    "reference": "Việc cung cấp kiến thức bên ngoài hoặc thêm các bước suy luận có thể giúp các mô hình ngôn ngữ lớn nhận biết ảo giác tốt hơn."
  },
  {
    "user_input": "Sự ra đời của các mô hình ngôn ngữ lớn (LLMs) đã tác động như thế nào đến lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)?",
    "retrieved_contexts": [
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "## **4 related work**\n\n**hallucination in llms.** hallucination in llms is\n\nconcerning since it hinders performance and raises\nsafety risks in real-world application. to alleviate\nthis issue, prior studies have proposed to use a verification system to identify non-factual entities in\ntext summarization (zhao et al., 2020), invoke interfaces of structured data ( *e.g.,* knowledge graph,\ndatabase) to obtain related evidence (jiang et al.,\n2023; lan et al., 2022), and train a token-level fact\ncritic to recognize hallucination and rectify them\nin dialogue (dziri et al., 2021). to enhance the\nunderstanding of hallucination in llms and pro\n\n6455",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations."
    ],
    "response": "```markdown\n1.  **Tác động của LLMs đến NLP:**\n\n    *   Sự ra đời của các mô hình ngôn ngữ lớn (LLMs) đã tạo ra một sự thay đổi lớn trong lĩnh vực xử lý ngôn ngữ tự nhiên (NLP).\n    *   LLMs đã đạt được những tiến bộ chưa từng có trong việc tạo và hiểu văn bản.\n    *   Khả năng ngôn ngữ đáng kể của LLMs đã khiến chúng trở thành cốt lõi trong nhiều sản phẩm có hàng triệu người dùng.\n```",
    "reference": "Sự ra đời của các mô hình ngôn ngữ lớn (LLMs) đã tạo ra một sự thay đổi mô hình trong lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)."
  },
  {
    "user_input": "Điều gì khiến các mô hình ngôn ngữ lớn (LLMs) trở nên quan trọng trong nhiều sản phẩm được hàng triệu người dùng sử dụng?",
    "retrieved_contexts": [
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com"
    ],
    "response": "Không có tài liệu nào trong số các tài liệu trên cung cấp thông tin chi tiết về lý do cụ thể khiến các mô hình ngôn ngữ lớn (LLM) trở nên quan trọng trong nhiều sản phẩm mà hàng triệu người dùng sử dụng.",
    "reference": "Khả năng ngôn ngữ vượt trội của chúng đã khiến LLMs trở thành cốt lõi trong nhiều sản phẩm được hàng triệu người dùng sử dụng."
  },
  {
    "user_input": "Mặc dù có những khả năng nổi bật, các mô hình ngôn ngữ lớn (LLMs) vẫn có thể gặp phải vấn đề gì?",
    "retrieved_contexts": [
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text."
    ],
    "response": "```markdown\n1.  **Vấn đề \"ảo giác\" của LLMs:**\n\n    *   Mặc dù có những khả năng nổi bật, các LLMs vẫn có thể gặp phải vấn đề \"ảo giác\" (hallucination).\n    *   \"Ảo giác\" ở đây là việc LLMs tạo ra các nội dung sai lệch, mâu thuẫn với nguồn thông tin hoặc không thể kiểm chứng bằng kiến thức thực tế.\n    *   ChatGPT có thể tạo ra nội dung \"ảo giác\" trong khoảng 19.5% các phản hồi.\n    *   Các chủ đề như ngôn ngữ, khí hậu và công nghệ đặc biệt dễ bị \"ảo giác\".\n```",
    "reference": "Các mô hình ngôn ngữ lớn (LLMs) có thể dễ bị tạo ra các nội dung 'ảo' (hallucination generations) trong nhiều ứng dụng khác nhau."
  },
  {
    "user_input": "Vấn đề tạo ra nội dung ảo giác (hallucination) có thể gây ra rủi ro gì khi triển khai các mô hình ngôn ngữ lớn (LLMs)?",
    "retrieved_contexts": [
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "## **4 related work**\n\n**hallucination in llms.** hallucination in llms is\n\nconcerning since it hinders performance and raises\nsafety risks in real-world application. to alleviate\nthis issue, prior studies have proposed to use a verification system to identify non-factual entities in\ntext summarization (zhao et al., 2020), invoke interfaces of structured data ( *e.g.,* knowledge graph,\ndatabase) to obtain related evidence (jiang et al.,\n2023; lan et al., 2022), and train a token-level fact\ncritic to recognize hallucination and rectify them\nin dialogue (dziri et al., 2021). to enhance the\nunderstanding of hallucination in llms and pro\n\n6455",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for"
    ],
    "response": "```markdown\n1.  **Rủi ro từ \"ảo giác\" của LLMs:**\n\n    *   Vấn đề \"ảo giác\" có thể gây ra rủi ro khi triển khai LLMs trong các ứng dụng thực tế.\n    *   \"Ảo giác\" cản trở hiệu suất và làm tăng rủi ro về an toàn.\n```",
    "reference": "Vấn đề tạo ra nội dung ảo giác có thể làm cho việc triển khai các mô hình ngôn ngữ lớn trở nên rủi ro trong các ứng dụng thực tế."
  },
  {
    "user_input": "Các nghiên cứu hiện tại chủ yếu tập trung vào việc gì liên quan đến hiện tượng tạo nội dung ảo giác của các mô hình ngôn ngữ?",
    "retrieved_contexts": [
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449"
    ],
    "response": "```markdown\n1.  **Các Nghiên Cứu Hiện Tại:**\n\n    *   Các nghiên cứu chủ yếu tập trung vào việc điều tra nguyên nhân gây ra \"ảo giác\" đối với các nhiệm vụ cụ thể và các mô hình ngôn ngữ nhỏ.\n    *   Vẫn chưa rõ LLMs có xu hướng tạo \"ảo giác\" với loại nội dung nào và ở mức độ nào.\n```",
    "reference": "Các nghiên cứu hiện tại chủ yếu tập trung vào việc điều tra nguyên nhân của hiện tượng tạo nội dung ảo giác (hallucination) đối với các tác vụ cụ thể và các mô hình ngôn ngữ nhỏ."
  },
  {
    "user_input": "HALUEVAL bao gồm bao nhiêu truy vấn người dùng chung và bao nhiêu ví dụ cụ thể cho từng nhiệm vụ?",
    "retrieved_contexts": [
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms."
    ],
    "response": "```markdown\n1.  **Thống kê về Halueval:**\n\n    *   Halueval bao gồm 5.000 truy vấn người dùng chung.\n    *   Có 30.000 ví dụ cụ thể cho từng nhiệm vụ.\n        *   Câu hỏi và trả lời (QA).\n        *   Đối thoại dựa trên tri thức.\n        *   Tóm tắt văn bản.\n```",
    "reference": "HALUEVAL bao gồm 5.000 truy vấn người dùng chung và 30.000 ví dụ cụ thể cho từng nhiệm vụ."
  },
  {
    "user_input": "Tại sao HALUEVAL lại chọn các truy vấn có độ tương đồng thấp giữa các phản hồi của ChatGPT?",
    "retrieved_contexts": [
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "table 4: a generated hallucinated qa example and a\nhuman-labeled chatgpt response for a user query.\n\nselection procedure. specifically, we use chatgpt\nto sample three responses for each user query and\ncompute their average semantic similarity using\nbertscore (zhang et al., 2020). we finally retain 5 *,* 000 user queries with the lowest similarities.\naccording to recent work (manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of llms. for each query and\nchatgpt response, human labelers will annotate\nwhether the response contains hallucinated information (“yes” or “no”) and list the corresponding\n\n\n6452",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) ."
    ],
    "response": "```markdown\n1.  **Lý do chọn truy vấn có độ tương đồng thấp:**\n\n    *   Để sàng lọc các truy vấn mà LLMs có khả năng tạo ra \"ảo giác\" nhất.\n    *   Sử dụng ChatGPT để lấy mẫu ba phản hồi cho mỗi truy vấn.\n    *   Chỉ giữ lại 5.000 truy vấn có độ tương đồng thấp nhất giữa ba phản hồi.\n    *   Theo nghiên cứu gần đây, \"ảo giác\" có khả năng xuất hiện trong các phản hồi khác nhau và mâu thuẫn của LLMs.\n```",
    "reference": "HALUEVAL chọn các truy vấn có độ tương đồng thấp giữa các phản hồi của ChatGPT vì độ tương đồng thấp có thể là dấu hiệu cho thấy mô hình đang tạo ra các thông tin sai lệch (hallucination)."
  },
  {
    "user_input": "Những nhiệm vụ cụ thể nào được sử dụng trong HALUEVAL để đánh giá các mô hình ngôn ngữ lớn?",
    "retrieved_contexts": [
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use"
    ],
    "response": "```markdown\n1.  **Các nhiệm vụ cụ thể trong HALUEVAL:**\n\n    *   Trả lời câu hỏi (Question Answering - QA).\n    *   Đối thoại dựa trên tri thức (Knowledge-Grounded Dialogue).\n    *   Tóm tắt văn bản (Text Summarization).\n```",
    "reference": "HALUEVAL sử dụng ba nhiệm vụ cụ thể: trả lời câu hỏi, đối thoại dựa trên tri thức và tóm tắt văn bản."
  },
  {
    "user_input": "Theo nghiên cứu gần đây, loại phản hồi nào của mô hình ngôn ngữ lớn (LLM) dễ xuất hiện ảo giác?",
    "retrieved_contexts": [
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "cording to recent work (manakul et al., 2023), hallucinations are likely to appear in diverged and conflicting responses of llms. based on the filtered\nuser queries and chatgpt responses, we invite\nhuman labelers to annotate whether the response\ncontains hallucinated information and mark cor\nresponding spans. as shown in table 1, for the\nuser query “ *retrieve the oldest photo of a cat* ”, the\nresponse generated by chatgpt contains unverifiable information. these human-annotated queries\nand responses can be used to analyze what types\nof content llms tend to hallucinate and further"
    ],
    "response": "```markdown\n1.  **Loại phản hồi dễ xuất hiện ảo giác:**\n\n    *   Các phản hồi khác nhau và mâu thuẫn của LLMs.```",
    "reference": "Ảo giác có khả năng xuất hiện trong các phản hồi khác biệt và mâu thuẫn của mô hình ngôn ngữ lớn (LLM)."
  },
  {
    "user_input": "Trong quá trình xác định thông tin ảo giác trong phản hồi của ChatGPT, vai trò của người đánh giá (human labelers) là gì?",
    "retrieved_contexts": [
      "table 4: a generated hallucinated qa example and a\nhuman-labeled chatgpt response for a user query.\n\nselection procedure. specifically, we use chatgpt\nto sample three responses for each user query and\ncompute their average semantic similarity using\nbertscore (zhang et al., 2020). we finally retain 5 *,* 000 user queries with the lowest similarities.\naccording to recent work (manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of llms. for each query and\nchatgpt response, human labelers will annotate\nwhether the response contains hallucinated information (“yes” or “no”) and list the corresponding\n\n\n6452",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "**labeler details.** annotating the hallucination\nin chatgpt responses is a very challenging task,\nwhich requires good reading comprehension skills\nand using search engine to look up relevant information for judgement. thus, from an initial pool\nof labeler candidates, we select labelers who are\ngood at english passage reading with at least an\nundergraduate-level education. besides, following\n(ouyang et al., 2022), we have labelers annotate a\nsmall number of test examples and measure their\nagreement with the labels of researchers, and finally\nwe choose thirty human labelers with the highest\nagreement scores. we report fleiss’s kappa ( *κ* ) to\nindicate the reliability of agreement between human labelers. we compute *κ* on 5 *,* 000 annotated\nsamples and obtain *κ* = 0 *.* 811 ( 0 *.* 80 *≤* *κ ≤* 1 *.* 00 )\nshowing a perfect agreement.",
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat",
      "cording to recent work (manakul et al., 2023), hallucinations are likely to appear in diverged and conflicting responses of llms. based on the filtered\nuser queries and chatgpt responses, we invite\nhuman labelers to annotate whether the response\ncontains hallucinated information and mark cor\nresponding spans. as shown in table 1, for the\nuser query “ *retrieve the oldest photo of a cat* ”, the\nresponse generated by chatgpt contains unverifiable information. these human-annotated queries\nand responses can be used to analyze what types\nof content llms tend to hallucinate and further",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "figure 3: topic distribution for chatgpt responses.\n\nspans. the hallucination is considered from the following three aspects: *unverifiable*, *non-factual*, and\n*irrelevant* . each response is labeled by three human labelers, and we adopt the max-voting strategy\nto determine the final hallucination label.",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language."
    ],
    "response": "```markdown\n1.  **Vai trò của người đánh giá (Human Labelers):**\n\n    *   Đánh giá xem phản hồi của ChatGPT có chứa thông tin bịa đặt hay không (\"có\" hoặc \"không\").\n    *   Liệt kê các đoạn thông tin bịa đặt tương ứng.\n    *   Đảm bảo kỹ năng đọc hiểu tốt và sử dụng công cụ tìm kiếm để tra cứu thông tin liên quan để đưa ra phán đoán.\n```",
    "reference": "Người đánh giá có vai trò xác định xem phản hồi có chứa thông tin ảo giác hay không và đánh dấu các đoạn văn bản tương ứng chứa thông tin đó."
  },
  {
    "user_input": "Dữ liệu truy vấn và phản hồi được đánh dấu bởi người đánh giá được sử dụng để làm gì?",
    "retrieved_contexts": [
      "cording to recent work (manakul et al., 2023), hallucinations are likely to appear in diverged and conflicting responses of llms. based on the filtered\nuser queries and chatgpt responses, we invite\nhuman labelers to annotate whether the response\ncontains hallucinated information and mark cor\nresponding spans. as shown in table 1, for the\nuser query “ *retrieve the oldest photo of a cat* ”, the\nresponse generated by chatgpt contains unverifiable information. these human-annotated queries\nand responses can be used to analyze what types\nof content llms tend to hallucinate and further",
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat",
      "table 4: a generated hallucinated qa example and a\nhuman-labeled chatgpt response for a user query.\n\nselection procedure. specifically, we use chatgpt\nto sample three responses for each user query and\ncompute their average semantic similarity using\nbertscore (zhang et al., 2020). we finally retain 5 *,* 000 user queries with the lowest similarities.\naccording to recent work (manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of llms. for each query and\nchatgpt response, human labelers will annotate\nwhether the response contains hallucinated information (“yes” or “no”) and list the corresponding\n\n\n6452",
      "**labeler details.** annotating the hallucination\nin chatgpt responses is a very challenging task,\nwhich requires good reading comprehension skills\nand using search engine to look up relevant information for judgement. thus, from an initial pool\nof labeler candidates, we select labelers who are\ngood at english passage reading with at least an\nundergraduate-level education. besides, following\n(ouyang et al., 2022), we have labelers annotate a\nsmall number of test examples and measure their\nagreement with the labels of researchers, and finally\nwe choose thirty human labelers with the highest\nagreement scores. we report fleiss’s kappa ( *κ* ) to\nindicate the reliability of agreement between human labelers. we compute *κ* on 5 *,* 000 annotated\nsamples and obtain *κ* = 0 *.* 811 ( 0 *.* 80 *≤* *κ ≤* 1 *.* 00 )\nshowing a perfect agreement.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449"
    ],
    "response": "```markdown\n1.  **Mục đích sử dụng dữ liệu truy vấn và phản hồi được đánh dấu:**\n\n    *   Phân tích loại nội dung mà LLM (Large Language Models) có xu hướng tạo ra thông tin sai lệch.\n    *   Nghiên cứu sâu hơn về lý do LLM tạo ra thông tin sai lệch.\n```",
    "reference": "Dữ liệu này được sử dụng để phân tích các loại nội dung mà mô hình ngôn ngữ lớn (LLM) có xu hướng tạo ra ảo giác, và có thể được sử dụng cho các mục đích khác nữa."
  },
  {
    "user_input": "ChatGPT được sử dụng để tạo ra các mẫu ảo giác với những kiểu hướng dẫn đặc trưng cho từng nhiệm vụ nào?",
    "retrieved_contexts": [
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "ods to generate diverse samples. for each method,\nchatgpt follows the instruction of hallucination\n\nsampling in different manners. as shown in figure 1, the first method adopts a *one-pass* instruction following schema, where we directly feed the\ncomplete instruction (table 2) into chatgpt and\ngenerate a hallucinated answer. on the other hand,\nthe second method uses a *conversational* schema,\nwhere we teach chatgpt to successively learn part\nof the instruction and make sure it has mastered.\n\nbased on the learned instructions, chatgpt will\ngenerate another hallucinated answer. through the\ntwo different sampling strategies, we can obtain\ndiverse and multi-facet hallucinated answers for\neach question, which will be further filtered and\nselected for the most plausible and difficult one.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms."
    ],
    "response": "```markdown\n1.  **Các kiểu hướng dẫn đặc trưng cho từng nhiệm vụ mà ChatGPT sử dụng để tạo mẫu ảo giác:**\n\n    *   **One-pass:** ChatGPT nhận trực tiếp và đầy đủ hướng dẫn để tạo ra câu trả lời ảo giác.\n    *   **Conversational:** ChatGPT được hướng dẫn từng phần, đảm bảo nắm vững trước khi tạo ra câu trả lời ảo giác.\n\n    *   Hai phương pháp này tạo ra các câu trả lời ảo giác đa dạng cho mỗi câu hỏi, sau đó được lọc và chọn ra những câu trả lời правдоподобные và khó nhất.\n```",
    "reference": "ChatGPT được sử dụng để tạo ra các mẫu ảo giác với hai kiểu hướng dẫn đặc trưng cho từng nhiệm vụ: onepass và conversational."
  },
  {
    "user_input": "Trong quá trình chọn mẫu ảo giác, ChatGPT được sử dụng để làm gì, và điều này ngụ ý gì về quy trình lựa chọn?",
    "retrieved_contexts": [
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "ods to generate diverse samples. for each method,\nchatgpt follows the instruction of hallucination\n\nsampling in different manners. as shown in figure 1, the first method adopts a *one-pass* instruction following schema, where we directly feed the\ncomplete instruction (table 2) into chatgpt and\ngenerate a hallucinated answer. on the other hand,\nthe second method uses a *conversational* schema,\nwhere we teach chatgpt to successively learn part\nof the instruction and make sure it has mastered.\n\nbased on the learned instructions, chatgpt will\ngenerate another hallucinated answer. through the\ntwo different sampling strategies, we can obtain\ndiverse and multi-facet hallucinated answers for\neach question, which will be further filtered and\nselected for the most plausible and difficult one.",
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark."
    ],
    "response": "Không có tài liệu nào đề cập đến việc ChatGPT được sử dụng như thế nào trong quá trình chọn mẫu ảo giác.",
    "reference": "ChatGPT được sử dụng để lựa chọn các mẫu ảo giác có vẻ hợp lý và khó nhất để đánh giá LLMs. Điều này ngụ ý rằng quy trình lựa chọn bao gồm việc xếp hạng hoặc chấm điểm các mẫu dựa trên tính hợp lý và độ khó của chúng."
  },
  {
    "user_input": "Các mẫu ảo giác được tạo ra nhằm mục đích gì đối với LLMs?",
    "retrieved_contexts": [
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets."
    ],
    "response": "Dựa trên các tài liệu đã cung cấp, mục đích của việc tạo ra các mẫu ảo giác cho LLMs là:\n\n1.  **Đánh giá hiệu suất nhận biết ảo giác của LLMs:**\n\n    *   Halueval là một bộ sưu tập lớn các mẫu ảo giác được tạo và chú thích bởi con người, được sử dụng để đánh giá khả năng của LLMs trong việc nhận biết ảo giác.\n    *   Bộ dữ liệu này bao gồm 35.000 mẫu ảo giác/bình thường, được sử dụng để phân tích và đánh giá LLMs.\n    *   Halueval bao gồm 5.000 truy vấn người dùng chung với các phản hồi từ ChatGPT và 30.000 ví dụ cụ thể theo tác vụ từ ba tác vụ: trả lời câu hỏi, đối thoại dựa trên kiến thức và tóm tắt văn bản.\n\n2.  **Nghiên cứu và giảm thiểu vấn đề ảo giác:**\n\n    *   Các nhà nghiên cứu có thể sử dụng Halueval để nghiên cứu hoặc giảm thiểu vấn đề ảo giác cho LLMs.\n    *   Halueval có thể giúp các nhà nghiên cứu hiểu loại nội dung nào và LLMs có xu hướng tạo ảo giác đến mức độ nào.\n\n3.  **Xây dựng LLMs hiệu quả và đáng tin cậy hơn:**\n\n    *   Mục tiêu cuối cùng là xây dựng các LLMs hiệu quả và đáng tin cậy hơn trong tương lai.",
    "reference": "Các mẫu ảo giác được tạo ra nhằm mục đích thách thức khả năng nhận biết ảo giác của LLMs và phân tích các điểm mù thông tin của chúng."
  },
  {
    "user_input": "Những LLM mạnh mẽ nào đã được sử dụng trong các thử nghiệm được đề cập?",
    "retrieved_contexts": [
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "hallucination recognition and analyze the information blind spots of llms.\nto better understand the performance of llms\nin halueval, we conduct experiments with several\nexisting powerful llms ( *e.g.,* chatgpt, gpt-3).",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark."
    ],
    "response": "Dựa trên tài liệu được cung cấp, các LLM mạnh mẽ đã được sử dụng trong các thử nghiệm bao gồm:\n\n1.  **Các LLM nguồn đóng:**\n\n    *   GPT-3 (davinci)\n    *   InstructGPT (text-davinci-002/003)    *   ChatGPT (gpt-3.5-turbo)\n    *   Claude\n    *   Claude 2\n\n2.  **Các LLM mã nguồn mở:**\n\n    *   Alpaca (7b)\n    *   Vicuna (7b)\n    *   ChatGLM (7b)\n    *   Falcon (7b)\n    *   Llama 2-chat (7b)",
    "reference": "ChatGPT và GPT-3 là những LLM mạnh mẽ đã được sử dụng trong các thử nghiệm."
  },
  {
    "user_input": "Mục đích chính của việc nghiên cứu này là gì?",
    "retrieved_contexts": [
      "-----\n\ni want you act as a summary judge. given a document and a summary, your objective is to determine if the\nprovided summary contains non-factual or hallucinated information. you should give your judgement based\non the following hallucination types and the world knowledge.\n\nyou are trying to determine if there exists some non-factual and incorrect information in the summary.\n\n<demonstrations>\n\nyou are trying to determine if there is a factual contradiction between the summary and the document.\n\n<demonstrations>\n\nyou should try your best to determine if the summary contains non-factual or hallucinated information according to the above hallucination types. the answer you give must be “yes” or “no”.\n\n**#document#:** <here is the test document>\n**#summary#:** <here is the hallucinated summary or right summary>\n**#your judgement#:**\n\ntable 15: instruction of hallucination recognition for text summarization.\n\n6463",
      "-----\n\ni want you act as a response judge. given a dialogue history, two responses, and related knowledge, your\nobjective is to select the best and correct response without hallucination and non-factual information. here are\nsome examples:\n\n...\n\n<demonstrations>\n\n...\n\nyou should try your best to select the best and correct response. if the two responses are the same, you can\nrandomly choose one. if both responses are incorrect, choose the better one. you must select a response from\nthe provided two responses.\n\n**#knowledge#:** <here is the related knowledge>\n**#dialogue history#:** <here is the dialogue history>\n**#response 1#:** <here is the hallucinated response generated by the first channel>\n**#response 2#:** <here is the hallucinated response generated by the second channel>\n**#your choice#:**",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "-----\n\ni want you act as an answer judge. given a question and an answer, your objective is to determine if the\nprovided answer contains non-factual or hallucinated information. you should give your judgement based\non the following hallucination types and the world knowledge.\n\nyou are trying to determine if the answer misunderstands the question context and intention.\n\n<demonstrations>\n\nyou are trying to determine if the answer cannot be inferred from the knowledge correctly.\n\n<demonstrations>\n\nyou should try your best to determine if the answer contains non-factual or hallucinated information according\nto the above hallucination types. the answer you give must be “yes” or “no”.\n\n**#question#:** <here is the test question>\n**#answer#:** <here is the hallucinated answer or right answer>\n**#your judgement#:**",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,"
    ],
    "response": "Mục đích chính của nghiên cứu này là:\n\n1.  **Đánh giá và hiểu rõ hơn về hiện tượng \"ảo giác\" trong LLMs:**\n\n    *   Nghiên cứu tập trung vào việc xác định loại nội dung nào và mức độ mà LLMs có xu hướng tạo ra ảo giác.\n    *   Điều này bao gồm việc xây dựng các chuẩn đánh giá (benchmarks) để đo lường và so sánh khả năng tạo ảo giác của các LLMs khác nhau.2.  **Cung cấp một nền tảng công khai để nghiên cứu và giảm thiểu ảo giác:**\n\n    *   Halueval được tạo ra như một bộ dữ liệu công khai để các nhà nghiên cứu có thể sử dụng để điều tra và giảm thiểu vấn đề ảo giác trong LLMs.\n    *   Bộ dữ liệu này bao gồm các mẫu ảo giác và các mẫu \"bình thường\" (không ảo giác) để hỗ trợ việc phân tích và đánh giá.\n\n3.  **Cải thiện độ tin cậy và tính chính xác của LLMs:**\n\n    *   Mục tiêu cuối cùng là xây dựng các LLMs đáng tin cậy hơn, ít bịa đặt thông tin sai lệch hoặc không chính xác.\n    *   Điều này có ý nghĩa quan trọng đối với nhiều ứng dụng thực tế, nơi tính chính xác và độ tin cậy là yếu tố then chốt.",
    "reference": "Mục đích chính là nhận diện ảo giác và phân tích các điểm mù thông tin của LLM."
  },
  {
    "user_input": "Việc thực hiện các thử nghiệm với LLM giúp hiểu rõ hơn điều gì về hiệu suất của chúng?",
    "retrieved_contexts": [
      "hallucination recognition and analyze the information blind spots of llms.\nto better understand the performance of llms\nin halueval, we conduct experiments with several\nexisting powerful llms ( *e.g.,* chatgpt, gpt-3).",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations."
    ],
    "response": "Việc thực hiện các thử nghiệm với LLM trong Halueval giúp hiểu rõ hơn về:\n\n1.  **Khả năng nhận biết ảo giác:**\n\n    *   Các thử nghiệm cho thấy LLM hiện tại gặp nhiều khó khăn trong việc nhận biết ảo giác trong văn bản do chúng tạo ra, ngay cả với ChatGPT (mô hình được sử dụng để tạo ra các mẫu ảo giác).\n    *   Độ chính xác của ChatGPT trong việc nhận biết ảo giác trong nhiệm vụ trả lời câu hỏi chỉ đạt khoảng 62,59%.\n\n2.  **Xu hướng tạo ra nội dung ảo giác:**\n\n    *   ChatGPT có khả năng tạo ra nội dung ảo giác bằng cách bịa đặt thông tin không thể kiểm chứng trong khoảng 19,5% số phản hồi.\n    *   Các văn bản ảo giác này bao gồm các chủ đề như ngôn ngữ, khí hậu và công nghệ.\n\n3.  **Các yếu tố ảnh hưởng đến khả năng nhận biết ảo giác:**\n\n    *   Việc cung cấp kiến thức bên ngoài hoặc thêm các bước suy luận trung gian có thể giúp LLM cải thiện khả năng nhận biết ảo giác. Ví dụ, độ chính xác của ChatGPT trong việc trả lời câu hỏi tăng từ 62,59% lên 76,83% khi được cung cấp kiến thức.\n    *   Tuy nhiên, việc so sánh các mẫu ảo giác với dữ liệu thực tế (ground-truth) có thể khiến LLM bối rối hơn và dẫn đến hiệu suất kém hơn.\n\n4.  **Hiệu suất của các LLM khác nhau:**\n\n    *   Các thử nghiệm được thực hiện trên nhiều LLM khác nhau (ví dụ: GPT-3, InstructGPT, Claude, Alpaca, Vicuna, ChatGLM, Falcon, Llama 2-chat) để so sánh khả năng của chúng trong việc nhận biết và tránh ảo giác.",
    "reference": "Việc thực hiện các thử nghiệm với LLM giúp hiểu rõ hơn về hiệu suất của chúng trong việc đánh giá ảo giác."
  },
  {
    "user_input": "Khoảng bao nhiêu phần trăm phản hồi từ ChatGPT có khả năng chứa nội dung bịa đặt, không thể kiểm chứng?",
    "retrieved_contexts": [
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "cording to recent work (manakul et al., 2023), hallucinations are likely to appear in diverged and conflicting responses of llms. based on the filtered\nuser queries and chatgpt responses, we invite\nhuman labelers to annotate whether the response\ncontains hallucinated information and mark cor\nresponding spans. as shown in table 1, for the\nuser query “ *retrieve the oldest photo of a cat* ”, the\nresponse generated by chatgpt contains unverifiable information. these human-annotated queries\nand responses can be used to analyze what types\nof content llms tend to hallucinate and further",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "table 4: a generated hallucinated qa example and a\nhuman-labeled chatgpt response for a user query.\n\nselection procedure. specifically, we use chatgpt\nto sample three responses for each user query and\ncompute their average semantic similarity using\nbertscore (zhang et al., 2020). we finally retain 5 *,* 000 user queries with the lowest similarities.\naccording to recent work (manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of llms. for each query and\nchatgpt response, human labelers will annotate\nwhether the response contains hallucinated information (“yes” or “no”) and list the corresponding\n\n\n6452",
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat"
    ],
    "response": "Khoảng 19,5% phản hồi từ ChatGPT có khả năng chứa nội dung bịa đặt, không thể kiểm chứng. Các chủ đề thường gặp bao gồm ngôn ngữ, khí hậu và công nghệ.",
    "reference": "Khoảng 19,5% phản hồi từ ChatGPT có khả năng chứa nội dung bịa đặt, không thể kiểm chứng."
  },
  {
    "user_input": "Các mô hình ngôn ngữ lớn (LLM) có gặp khó khăn trong việc phát hiện thông tin sai lệch do chính chúng tạo ra không?",
    "retrieved_contexts": [
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language."
    ],
    "response": "Đúng, các mô hình ngôn ngữ lớn (LLM) gặp khó khăn đáng kể trong việc phát hiện thông tin sai lệch do chính chúng tạo ra. Nghiên cứu chỉ ra rằng ngay cả ChatGPT, mô hình được sử dụng để tạo ra các mẫu \"ảo giác\", cũng gặp khó khăn trong việc phân biệt giữa thông tin thực tế và thông tin sai lệch. Ví dụ, trong nhiệm vụ tóm tắt văn bản, ChatGPT chỉ đạt độ chính xác 58,53%, nhỉnh hơn một chút so với mức ngẫu nhiên. Các mô hình khác như GPT-3, Alpaca và Vicuna thậm chí còn có hiệu suất kém hơn.",
    "reference": "Có, các mô hình ngôn ngữ lớn gặp nhiều khó khăn trong việc phát hiện thông tin sai lệch do chính chúng tạo ra."
  },
  {
    "user_input": "Những phương pháp nào có thể giúp cải thiện khả năng nhận biết thông tin sai lệch của các mô hình ngôn ngữ lớn (LLM)?",
    "retrieved_contexts": [
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "## **4 related work**\n\n**hallucination in llms.** hallucination in llms is\n\nconcerning since it hinders performance and raises\nsafety risks in real-world application. to alleviate\nthis issue, prior studies have proposed to use a verification system to identify non-factual entities in\ntext summarization (zhao et al., 2020), invoke interfaces of structured data ( *e.g.,* knowledge graph,\ndatabase) to obtain related evidence (jiang et al.,\n2023; lan et al., 2022), and train a token-level fact\ncritic to recognize hallucination and rectify them\nin dialogue (dziri et al., 2021). to enhance the\nunderstanding of hallucination in llms and pro\n\n6455",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "**3.2.2** **improvement strategies**\n\nin this part, we design several strategies to improve\nthe ability of llms to recognize hallucination. the\nresults are shown in table 8.\n\n**knowledge retrieval.** retrieving relevant knowledge is a widely used strategy to eliminate hallucination (lewis et al., 2020; li et al., 2023a). therefore, we supply chatgpt with the knowledge facts\nretrieved from wikipedia (except for that summarization does not need external information besides"
    ],
    "response": "1.  **Sử dụng truy xuất kiến thức:**\n\n    *   Truy xuất kiến thức liên quan là một chiến lược được sử dụng rộng rãi để loại bỏ thông tin sai lệch.\n    *   Phương pháp này cung cấp cho ChatGPT các dữ kiện kiến thức được truy xuất từ Wikipedia (ngoại trừ việc tóm tắt không cần thông tin bên ngoài).\n2.  **Thêm các bước suy luận trung gian:**\n\n    *   Việc thêm các bước suy luận trung gian có thể giúp LLM nhận biết thông tin sai lệch tốt hơn.\n3.  **Điều chỉnh hướng dẫn và căn chỉnh với con người:**\n\n    *   Từ GPT-3 đến InstructGPT và ChatGPT, việc điều chỉnh hướng dẫn và căn chỉnh với con người có thể tăng cường khả năng của LLM trong việc xác định thông tin sai lệch trong văn bản.",
    "reference": "Cung cấp kiến thức rõ ràng và thêm các bước suy luận trung gian có thể cải thiện khả năng nhận biết thông tin sai lệch của các mô hình ngôn ngữ lớn. So sánh các mẫu sai lệch với dữ liệu thực tế có thể làm giảm hiệu suất."
  },
  {
    "user_input": "Halueval benchmark chứa những loại mẫu nào?",
    "retrieved_contexts": [
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,"
    ],
    "response": "1.  **Tổng quan về Halueval:**\n\n    *   Halueval là một bộ sưu tập lớn gồm 35.000 mẫu \"ảo giác\"/bình thường, được sử dụng để phân tích và đánh giá các LLM.\n    *   Bộ dữ liệu này bao gồm 5.000 truy vấn người dùng *chung* với các phản hồi từ ChatGPT.\n    *   Ngoài ra, nó còn chứa 30.000 ví dụ *cụ thể theo nhiệm vụ* từ ba loại nhiệm vụ:\n        *   Trả lời câu hỏi.\n        *   Đối thoại dựa trên kiến thức.\n        *   Tóm tắt văn bản.",
    "reference": "Halueval benchmark chứa rất nhiều mẫu đúng và các mẫu bị hallucination."
  },
  {
    "user_input": "Bộ sưu tập các mẫu trong halueval benchmark được tạo ra bằng những cách nào?",
    "retrieved_contexts": [
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms."
    ],
    "response": "1.  **Hai phương pháp tạo mẫu:**\n\n    *   **Tạo tự động:** Sử dụng phương pháp \"lấy mẫu rồi lọc\" hai bước để tạo ra các mẫu đa dạng bằng cách sử dụng hướng dẫn, sau đó lọc và chọn ra những mẫu khó.\n    *   **Chú thích thủ công:** Mời những người có trình độ để chú thích các \"ảo giác\" trong phản hồi của ChatGPT đối với các truy vấn của người dùng.",
    "reference": "Bộ sưu tập này được tạo ra bằng hai cách: tạo tự động và chú thích thủ công."
  },
  {
    "user_input": "Mục tiêu của halueval benchmark là gì?",
    "retrieved_contexts": [
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) ."
    ],
    "response": "1.  **Mục tiêu của Halueval:**\n\n    *   Hiểu loại nội dung và mức độ mà LLM có xu hướng tạo ra \"ảo giác\".\n    *   Tạo điều kiện cho nghiên cứu về hướng này.\n    *   Đánh giá hiệu suất của LLM trong việc nhận biết \"ảo giác\".\n    *   Xây dựng các LLM hiệu quả và đáng tin cậy hơn trong tương lai.",
    "reference": "Mục tiêu của halueval là để hiểu loại nội dung nào và mức độ mà LLM có xu hướng hallucination."
  },
  {
    "user_input": "Quy trình tạo sinh của chúng tôi bao gồm mấy bước và đó là những bước nào?",
    "retrieved_contexts": [
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "table 4: a generated hallucinated qa example and a\nhuman-labeled chatgpt response for a user query.\n\nselection procedure. specifically, we use chatgpt\nto sample three responses for each user query and\ncompute their average semantic similarity using\nbertscore (zhang et al., 2020). we finally retain 5 *,* 000 user queries with the lowest similarities.\naccording to recent work (manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of llms. for each query and\nchatgpt response, human labelers will annotate\nwhether the response contains hallucinated information (“yes” or “no”) and list the corresponding\n\n\n6452"
    ],
    "response": "Không có tài liệu nào cung cấp thông tin về quy trình tạo sinh.",
    "reference": "Quy trình tạo sinh của chúng tôi bao gồm hai bước: 1) lấy mẫu ảo giác đa dạng và 2) lọc ảo giác chất lượng cao."
  },
  {
    "user_input": "Công cụ nào được sử dụng để tự động hóa quy trình tạo sinh?",
    "retrieved_contexts": [
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "**2.1** **automatic generation**\n\nour generation pipeline includes two steps: 1) diverse hallucination sampling, and 2) high-quality\nhallucination filtering. we employ chatgpt to\nexecute the creation pipeline automatically.\n\n**diverse hallucination sampling.** since a factual\ntext can be hallucinated from different aspects, we\npropose two different hallucination sampling meth\n\n6450",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use"
    ],
    "response": "1.  **Công cụ sử dụng:**\n\n    *   ChatGPT API của Azure OpenAI được sử dụng để thực hiện quy trình tạo sinh các mẫu \"ảo giác\" một cách tự động.",
    "reference": "ChatGPT được sử dụng để tự động hóa quy trình tạo sinh."
  },
  {
    "user_input": "Tại sao cần có các phương pháp lấy mẫu ảo giác khác nhau?",
    "retrieved_contexts": [
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "ods to generate diverse samples. for each method,\nchatgpt follows the instruction of hallucination\n\nsampling in different manners. as shown in figure 1, the first method adopts a *one-pass* instruction following schema, where we directly feed the\ncomplete instruction (table 2) into chatgpt and\ngenerate a hallucinated answer. on the other hand,\nthe second method uses a *conversational* schema,\nwhere we teach chatgpt to successively learn part\nof the instruction and make sure it has mastered.\n\nbased on the learned instructions, chatgpt will\ngenerate another hallucinated answer. through the\ntwo different sampling strategies, we can obtain\ndiverse and multi-facet hallucinated answers for\neach question, which will be further filtered and\nselected for the most plausible and difficult one.",
      "to select the most plausible and difficult hallucinated samples from the above two sampling methods. as shown in table 3, we design the instruction\nof hallucination filtering enhanced by ground-truth\nanswers to select the best answer from two hallucinated candidates. in the instruction of filtering,\nthe demonstration includes the ground-truth correct\nanswer ( *e.g.,* you.s. highway 60) and a hallucinated\ncounterpart ( *e.g.,* you.s. highway 70). while, in the\ntest example, we input two hallucinated answers.\nfollowing the demonstrations, we expect chatgpt\nto select one of the hallucinated answers that is the",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "**2.1** **automatic generation**\n\nour generation pipeline includes two steps: 1) diverse hallucination sampling, and 2) high-quality\nhallucination filtering. we employ chatgpt to\nexecute the creation pipeline automatically.\n\n**diverse hallucination sampling.** since a factual\ntext can be hallucinated from different aspects, we\npropose two different hallucination sampling meth\n\n6450",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "llms in three aspects. firstly, based on our generated and annotated samples, researchers can use\nthem to analyze what types of content llms tend\nto generate hallucinations. second, researchers can\nfurther evaluate the ability of llms to recognize\nhallucinations in the generated samples. for example, given a question and an answer, llms can\nbe asked to determine whether the answer contains"
    ],
    "response": "1.  **Mục đích của các phương pháp lấy mẫu ảo giác khác nhau:**\n\n    *   Để tạo ra các mẫu \"ảo giác\" đa dạng.\n    *   Phương pháp \"One-Pass\" và \"Conversational\" được sử dụng để ChatGPT tạo ra các câu trả lời \"ảo giác\" từ các khía cạnh khác nhau.",
    "reference": "Cần có các phương pháp lấy mẫu ảo giác khác nhau vì một văn bản thực tế có thể bị tạo ảo giác từ nhiều khía cạnh khác nhau."
  },
  {
    "user_input": "Phương pháp đầu tiên được sử dụng để tạo mẫu đa dạng có tên là gì?",
    "retrieved_contexts": [
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "ods to generate diverse samples. for each method,\nchatgpt follows the instruction of hallucination\n\nsampling in different manners. as shown in figure 1, the first method adopts a *one-pass* instruction following schema, where we directly feed the\ncomplete instruction (table 2) into chatgpt and\ngenerate a hallucinated answer. on the other hand,\nthe second method uses a *conversational* schema,\nwhere we teach chatgpt to successively learn part\nof the instruction and make sure it has mastered.\n\nbased on the learned instructions, chatgpt will\ngenerate another hallucinated answer. through the\ntwo different sampling strategies, we can obtain\ndiverse and multi-facet hallucinated answers for\neach question, which will be further filtered and\nselected for the most plausible and difficult one.",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "**2.1** **automatic generation**\n\nour generation pipeline includes two steps: 1) diverse hallucination sampling, and 2) high-quality\nhallucination filtering. we employ chatgpt to\nexecute the creation pipeline automatically.\n\n**diverse hallucination sampling.** since a factual\ntext can be hallucinated from different aspects, we\npropose two different hallucination sampling meth\n\n6450",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a.",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms."
    ],
    "response": "Không có tài liệu nào được cung cấp chứa thông tin về tên của phương pháp đầu tiên được sử dụng để tạo mẫu đa dạng.",
    "reference": "Phương pháp đầu tiên được gọi là lược đồ tuân theo hướng dẫn *một lượt*."
  },
  {
    "user_input": "Mục đích của việc sử dụng lược đồ *đàm thoại* là gì?",
    "retrieved_contexts": [
      "ods to generate diverse samples. for each method,\nchatgpt follows the instruction of hallucination\n\nsampling in different manners. as shown in figure 1, the first method adopts a *one-pass* instruction following schema, where we directly feed the\ncomplete instruction (table 2) into chatgpt and\ngenerate a hallucinated answer. on the other hand,\nthe second method uses a *conversational* schema,\nwhere we teach chatgpt to successively learn part\nof the instruction and make sure it has mastered.\n\nbased on the learned instructions, chatgpt will\ngenerate another hallucinated answer. through the\ntwo different sampling strategies, we can obtain\ndiverse and multi-facet hallucinated answers for\neach question, which will be further filtered and\nselected for the most plausible and difficult one.",
      "-----\n\nyou should try your best to select the best and correct answer. if the two answers are the same, you can randomly\nchoose one. if both answers are incorrect, choose the better one. you must select an answer from the provided\ntwo answers.\n\n**#knowledge#:** <insert the related knowledge>\n**#question#:** <insert the question>\n**#answer 1#:** <insert the hallucinated answer generated by the one-pass schema>\n**#answer 2#:** <insert the hallucinated answer generated by the conversational schema>\n**#your choice#:**",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "-----\n\nconversational instruction\n\n\nhigh-quality hallucination filtering\n\n\n\nmax-voting\n\n\n\nhuman annotation\n\nfigure 1: construction pipeline of halueval, including automatic generation (top) and human annotation (bottom).",
      "-----\n\ni want you act as an assistant in a conversation with human. given a dialogue history, the true response, and\nrelated knowledge, your objective is to write a hallucinated response that sounds plausible but is factually\nincorrect. you should write the hallucinated response using the following method (each with some examples):\n\nor\nyou are trying to write a response to human but you replace the true entity with a dissimilar entity.\n\n<demonstrations>\n\nor\n\nyou should try your best to make the response become hallucinated.\n\n**#knowledge#:** <here is the related knowledge>\n**#dialogue history#:** <here is the dialogue history>\n**#true response#:** <here is the true response of the dialogue history>\n**#hallucinated response#:**",
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "table 9: instruction of hallucination sampling for knowledge-grounded dialogue.\n\ni want you act as a hallucination summary generator. given a document and the right summary, your objective\nis to write a hallucinated summary that sounds plausible but is factually incorrect. you should write the\nhallucinated summary using the following method (each with some examples):\n\nor\n\nor\nyou are trying to write a summary but there is a factual contradiction between the summary and the document.\n\n<demonstrations>\n\nyou should try your best to make the summary become hallucinated. #hallucinated summary# can only have\nabout 5 more words than #right summary#.\n\n**#document#:** <here is the test document>\n**#right summary#:** <here is the right summary of the test document>\n**#hallucinated summary#:**\n\ntable 10: instruction of hallucination sampling for text summarization.\n\n6460",
      "-----\n\ni want you act as a response judge. given a dialogue history, two responses, and related knowledge, your\nobjective is to select the best and correct response without hallucination and non-factual information. here are\nsome examples:\n\n...\n\n<demonstrations>\n\n...\n\nyou should try your best to select the best and correct response. if the two responses are the same, you can\nrandomly choose one. if both responses are incorrect, choose the better one. you must select a response from\nthe provided two responses.\n\n**#knowledge#:** <here is the related knowledge>\n**#dialogue history#:** <here is the dialogue history>\n**#response 1#:** <here is the hallucinated response generated by the first channel>\n**#response 2#:** <here is the hallucinated response generated by the second channel>\n**#your choice#:**",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets.",
      "or normal output ( *e.g.,* an answer) of each sample\nfor classification. the evaluation instructions of\nqa, dialogue, and summarization are presented in\ntable 13, table 14 and table 15 in appendix c."
    ],
    "response": "1.  **Mục đích của lược đồ \"Conversational\" (Đàm thoại):**\n\n    *   Để ChatGPT từng bước học một phần của hướng dẫn và đảm bảo rằng nó đã nắm vững phần đó.\n    *   Sau khi đã học các hướng dẫn, ChatGPT sẽ tạo ra một câu trả lời \"ảo giác\" khác.\n    *   Lược đồ \"Conversational\" được sử dụng để tạo ra các câu trả lời \"ảo giác\" đa dạng và nhiều mặt cho mỗi câu hỏi.",
    "reference": "Mục đích là để dạy ChatGPT từng bước một, đảm bảo nó nắm vững từng phần của hướng dẫn."
  },
  {
    "user_input": "Theo phương pháp thiết kế được đề cập, hướng dẫn lấy mẫu ảo giác bao gồm những phần quan trọng nào?",
    "retrieved_contexts": [
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "## **appendix**\n\nwe provide some extra information about our\nbenchmark as supplementary materials. the appendix is organized into three sections:\n\n  - instructions of hallucination sampling are presented in appendix a;\n\n  instructions of hallucination filtering are presented in appendix b;\n\n  - instructions of evaluation are presented in appendix c;\n\n  - details of our benchmark are presented in appendix d.\n## **a hallucination sampling**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 9 and\ntable 10, respectively.\n## **b hallucination filtering**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 11\nand table 12, respectively.\n## **c hallucination recognition**\n\nthe hallucination recognition instructions for qa,\ndialogue and summarization are shown in table 13,\ntable 14 and table 15, respectively.",
      "to select the most plausible and difficult hallucinated samples from the above two sampling methods. as shown in table 3, we design the instruction\nof hallucination filtering enhanced by ground-truth\nanswers to select the best answer from two hallucinated candidates. in the instruction of filtering,\nthe demonstration includes the ground-truth correct\nanswer ( *e.g.,* you.s. highway 60) and a hallucinated\ncounterpart ( *e.g.,* you.s. highway 70). while, in the\ntest example, we input two hallucinated answers.\nfollowing the demonstrations, we expect chatgpt\nto select one of the hallucinated answers that is the",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "table 2: instruction of hallucination sampling for question answering. the blue text denotes the intention descrip\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration.",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a."
    ],
    "response": "1.  **Các phần quan trọng của hướng dẫn lấy mẫu ảo giác:**\n\n    *   **Mô tả ý định (Intention Description):** Mô tả vai trò của hệ thống và xác định đầu vào cũng như mục tiêu của quá trình tạo dữ liệu.\n    *   **Mẫu ảo giác (Hallucination Pattern):** Kiểm soát loại và chất lượng của ảo giác.\n    *   **Minh họa ảo giác (Hallucination Demonstration):** Ví dụ minh họa về cách tạo ra ảo giác.",
    "reference": "Hướng dẫn lấy mẫu ảo giác bao gồm ba phần quan trọng: mô tả ý định, mẫu ảo giác và trình diễn ảo giác."
  },
  {
    "user_input": "Mô tả ý định được sử dụng để làm gì trong quá trình tạo mẫu ảo giác?",
    "retrieved_contexts": [
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "table 2: instruction of hallucination sampling for question answering. the blue text denotes the intention descrip\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration.",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "ods to generate diverse samples. for each method,\nchatgpt follows the instruction of hallucination\n\nsampling in different manners. as shown in figure 1, the first method adopts a *one-pass* instruction following schema, where we directly feed the\ncomplete instruction (table 2) into chatgpt and\ngenerate a hallucinated answer. on the other hand,\nthe second method uses a *conversational* schema,\nwhere we teach chatgpt to successively learn part\nof the instruction and make sure it has mastered.\n\nbased on the learned instructions, chatgpt will\ngenerate another hallucinated answer. through the\ntwo different sampling strategies, we can obtain\ndiverse and multi-facet hallucinated answers for\neach question, which will be further filtered and\nselected for the most plausible and difficult one."
    ],
    "response": "1.  **Mục đích của mô tả ý định:**\n\n    *   Mô tả ý định được sử dụng để mô tả vai trò của hệ thống.\n    *   Xác định đầu vào.\n    *   Xác định mục tiêu của quá trình tạo dữ liệu ảo giác.",
    "reference": "Mô tả ý định được sử dụng để xác định vai trò của hệ thống và xác định đầu vào và mục tiêu của quá trình tạo mẫu."
  },
  {
    "user_input": "Mục tiêu chính của việc thiết kế hướng dẫn trong phương pháp này là gì?",
    "retrieved_contexts": [
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "to select the most plausible and difficult hallucinated samples from the above two sampling methods. as shown in table 3, we design the instruction\nof hallucination filtering enhanced by ground-truth\nanswers to select the best answer from two hallucinated candidates. in the instruction of filtering,\nthe demonstration includes the ground-truth correct\nanswer ( *e.g.,* you.s. highway 60) and a hallucinated\ncounterpart ( *e.g.,* you.s. highway 70). while, in the\ntest example, we input two hallucinated answers.\nfollowing the demonstrations, we expect chatgpt\nto select one of the hallucinated answers that is the",
      "table 13: instruction of hallucination recognition for question answering.\n\ni want you act as a response judge. given a dialogue history and a response, your objective is to determine if the\nprovided response contains non-factual or hallucinated information. you should give your judgement based\non the following hallucination types and the world knowledge.\n\nyou are trying to determine if the true entity in the response is replaced with a dissimilar entity.\n\n<demonstrations>\n\nyou should try your best to determine if the response contains non-factual or hallucinated information according\nto the above hallucination types. the answer you give must be “yes” or “no”.\n\n**#dialogue history#:** <here is the dialogue history>\n**#response#:** <here is the hallucinated response or right response>\n**#your judgement#:**\n\ntable 14: instruction of hallucination recognition for knowledge-grounded dialogue.\n\n6462",
      "table 11: instruction of hallucination filtering for knowledge-grounded dialogue.\n\ni want you act as a summary judge. given a document and two summaries, your objective is to select the best\nand correct summary without hallucination and non-factual information. here are some examples:\n\n...\n\n<demonstrations>\n\n...\n\nyou should try your best to select the best and correct summary. if both summaries are incorrect, choose the\nbetter one. you must select a summary from the provided two summaries.\n\n**#document#:** <here is the test document>\n**#summary 1#:** <here is the hallucinated summary generated by the first channel>\n**#summary 2#:** <here is the hallucinated summary generated by the second channel>\n**#your choice#:**\n\ntable 12: instruction of hallucination filtering for text summarization.\n\n6461",
      "table 9: instruction of hallucination sampling for knowledge-grounded dialogue.\n\ni want you act as a hallucination summary generator. given a document and the right summary, your objective\nis to write a hallucinated summary that sounds plausible but is factually incorrect. you should write the\nhallucinated summary using the following method (each with some examples):\n\nor\n\nor\nyou are trying to write a summary but there is a factual contradiction between the summary and the document.\n\n<demonstrations>\n\nyou should try your best to make the summary become hallucinated. #hallucinated summary# can only have\nabout 5 more words than #right summary#.\n\n**#document#:** <here is the test document>\n**#right summary#:** <here is the right summary of the test document>\n**#hallucinated summary#:**\n\ntable 10: instruction of hallucination sampling for text summarization.\n\n6460",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark."
    ],
    "response": "Không có tài liệu nào cung cấp thông tin trực tiếp về mục tiêu *chính* của việc thiết kế hướng dẫn trong phương pháp này. Các tài liệu tập trung vào các thành phần và mục đích cụ thể của hướng dẫn, nhưng không nêu rõ mục tiêu bao trùm hoặc quan trọng nhất của việc thiết kế hướng dẫn nói chung.",
    "reference": "Mục tiêu chính là thiết kế một hướng dẫn hiệu quả cho ChatGPT để tạo ra các mẫu ảo giác."
  },
  {
    "user_input": "Bài báo này tự động tạo ra các mẫu 'ảo giác' cho những tác vụ nào?",
    "retrieved_contexts": [
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "**2.1** **automatic generation**\n\nour generation pipeline includes two steps: 1) diverse hallucination sampling, and 2) high-quality\nhallucination filtering. we employ chatgpt to\nexecute the creation pipeline automatically.\n\n**diverse hallucination sampling.** since a factual\ntext can be hallucinated from different aspects, we\npropose two different hallucination sampling meth\n\n6450"
    ],
    "response": "Bài báo này tự động tạo ra các mẫu \"ảo giác\" cho ba tác vụ sau:\n\n1.  **Trả lời câu hỏi**\n2.  **Đối thoại dựa trên tri thức**\n3.  **Tóm tắt văn bản**",
    "reference": "Bài báo này tự động tạo ra các mẫu 'ảo giác' cho ba tác vụ: trả lời câu hỏi, đối thoại dựa trên tri thức và tóm tắt văn bản."
  },
  {
    "user_input": "Những loại mẫu 'ảo giác' nào được xem xét cho tác vụ trả lời câu hỏi?",
    "retrieved_contexts": [
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "table 13: instruction of hallucination recognition for question answering.\n\ni want you act as a response judge. given a dialogue history and a response, your objective is to determine if the\nprovided response contains non-factual or hallucinated information. you should give your judgement based\non the following hallucination types and the world knowledge.\n\nyou are trying to determine if the true entity in the response is replaced with a dissimilar entity.\n\n<demonstrations>\n\nyou should try your best to determine if the response contains non-factual or hallucinated information according\nto the above hallucination types. the answer you give must be “yes” or “no”.\n\n**#dialogue history#:** <here is the dialogue history>\n**#response#:** <here is the hallucinated response or right response>\n**#your judgement#:**\n\ntable 14: instruction of hallucination recognition for knowledge-grounded dialogue.\n\n6462",
      "table 2: instruction of hallucination sampling for question answering. the blue text denotes the intention descrip\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration.",
      "-----\n\ni want you act as an answer judge. given a question and an answer, your objective is to determine if the\nprovided answer contains non-factual or hallucinated information. you should give your judgement based\non the following hallucination types and the world knowledge.\n\nyou are trying to determine if the answer misunderstands the question context and intention.\n\n<demonstrations>\n\nyou are trying to determine if the answer cannot be inferred from the knowledge correctly.\n\n<demonstrations>\n\nyou should try your best to determine if the answer contains non-factual or hallucinated information according\nto the above hallucination types. the answer you give must be “yes” or “no”.\n\n**#question#:** <here is the test question>\n**#answer#:** <here is the hallucinated answer or right answer>\n**#your judgement#:**",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets."
    ],
    "response": "Các loại mẫu \"ảo giác\" được xem xét cho tác vụ trả lời câu hỏi bao gồm:\n\n1.  **Comprehension (Hiểu)**\n2.  **Factualness (Tính xác thực)**\n3.  **Specificity (Tính cụ thể)**\n4.  **Inference (Suy luận)**",
    "reference": "Có bốn loại mẫu 'ảo giác' được xem xét cho tác vụ trả lời câu hỏi: comprehension, factual-ness, specificity và inference."
  },
  {
    "user_input": "Số lượng mẫu 'ảo giác' cho tác vụ tóm tắt văn bản là bao nhiêu, và chúng là những loại nào?",
    "retrieved_contexts": [
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets.",
      "-----\n\n**tasks** **#sample** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 10000 2280 1378 5102 1240\n**dialogue** 10000 8330 1196 474   **summa.** 10000 2614 3562 3824   \ntable 16: number of generated samples for each hallucination pattern (p-i/ii/iii/iv). “‘summa.” is short\nfor summarization. “-” is due to that we consider three\n\npatterns in dialogue and summarization.\n\n6464\n\n\n-----",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "-----\n\ni want you act as a summary judge. given a document and a summary, your objective is to determine if the\nprovided summary contains non-factual or hallucinated information. you should give your judgement based\non the following hallucination types and the world knowledge.\n\nyou are trying to determine if there exists some non-factual and incorrect information in the summary.\n\n<demonstrations>\n\nyou are trying to determine if there is a factual contradiction between the summary and the document.\n\n<demonstrations>\n\nyou should try your best to determine if the summary contains non-factual or hallucinated information according to the above hallucination types. the answer you give must be “yes” or “no”.\n\n**#document#:** <here is the test document>\n**#summary#:** <here is the hallucinated summary or right summary>\n**#your judgement#:**\n\ntable 15: instruction of hallucination recognition for text summarization.\n\n6463",
      "table 2: instruction of hallucination sampling for question answering. the blue text denotes the intention descrip\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration."
    ],
    "response": "Số lượng mẫu \"ảo giác\" được tạo ra cho tác vụ tóm tắt văn bản là **10,000**. Chúng thuộc ba loại sau:1.  **Factual (Dựa trên sự thật)**\n2.  **Non-factual (Không dựa trên sự thật)**\n3.  **Intrinsic (Bản chất)**",
    "reference": "Có ba loại mẫu 'ảo giác' cho tác vụ tóm tắt văn bản: factual, non-factual và intrinsic."
  },
  {
    "user_input": "Có bao nhiêu mẫu được lấy ngẫu nhiên từ tập huấn luyện?",
    "retrieved_contexts": [
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a.",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "-----\n\n**tasks** **#sample** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 10000 2280 1378 5102 1240\n**dialogue** 10000 8330 1196 474   **summa.** 10000 2614 3562 3824   \ntable 16: number of generated samples for each hallucination pattern (p-i/ii/iii/iv). “‘summa.” is short\nfor summarization. “-” is due to that we consider three\n\npatterns in dialogue and summarization.\n\n6464\n\n\n-----",
      "44\n\n**claude 2** 69.78 64.73 57.75 75.00\n\n**claude** 67.60 64.83 53.76 73.88\n\n**davinci002** 60.05 60.81 47.77 80.42\n\n**davinci003** 49.65 68.37 48.07 80.40\n\n**gpt-3** 49.21 50.02 51.23 72.72\n\n**llama 2** 49.60 43.99 49.55 20.46\n\n**chatglm** 47.93 44.41 48.57 30.92\n\n**falcon** 39.66 29.08 42.71 18.98\n\n**vicuna** 60.34 46.35 45.62 19.48\n**alpaca** 6.68 17.55 20.63 9.54\n\ntable 5: accuracy (%) of classifying whether a sample\ncontains hallucinated contents.",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets."
    ],
    "response": "Để tạo ra các mẫu \"ảo giác\", 30,000 mẫu đã được lấy ngẫu nhiên từ tập huấn luyện.",
    "reference": "30.000 mẫu."
  },
  {
    "user_input": "Mục đích của việc tạo ra các 'hallucinated examples' là gì?",
    "retrieved_contexts": [
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a.",
      "table 9: instruction of hallucination sampling for knowledge-grounded dialogue.\n\ni want you act as a hallucination summary generator. given a document and the right summary, your objective\nis to write a hallucinated summary that sounds plausible but is factually incorrect. you should write the\nhallucinated summary using the following method (each with some examples):\n\nor\n\nor\nyou are trying to write a summary but there is a factual contradiction between the summary and the document.\n\n<demonstrations>\n\nyou should try your best to make the summary become hallucinated. #hallucinated summary# can only have\nabout 5 more words than #right summary#.\n\n**#document#:** <here is the test document>\n**#right summary#:** <here is the right summary of the test document>\n**#hallucinated summary#:**\n\ntable 10: instruction of hallucination sampling for text summarization.\n\n6460",
      "table 11: instruction of hallucination filtering for knowledge-grounded dialogue.\n\ni want you act as a summary judge. given a document and two summaries, your objective is to select the best\nand correct summary without hallucination and non-factual information. here are some examples:\n\n...\n\n<demonstrations>\n\n...\n\nyou should try your best to select the best and correct summary. if both summaries are incorrect, choose the\nbetter one. you must select a summary from the provided two summaries.\n\n**#document#:** <here is the test document>\n**#summary 1#:** <here is the hallucinated summary generated by the first channel>\n**#summary 2#:** <here is the hallucinated summary generated by the second channel>\n**#your choice#:**\n\ntable 12: instruction of hallucination filtering for text summarization.\n\n6461",
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of"
    ],
    "response": "Mục đích của việc tạo ra các \"hallucinated examples\" (ví dụ ảo giác) là để:\n\n1.  **Thách thức khả năng của LLMs trong việc nhận biết ảo giác:** Các mẫu này được thiết kế để kiểm tra xem LLMs có thể phân biệt được thông tin thật và thông tin sai lệch hay không.\n2.  **Phân tích các điểm mù thông tin của LLMs:** Việc sử dụng các ví dụ ảo giác giúp xác định những lĩnh vực mà LLMs dễ bị đánh lừa hoặc không có đủ kiến thức để đưa ra câu trả lời chính xác.",
    "reference": "Để tạo ra dữ liệu nhân tạo hoặc tăng cường dữ liệu hiện có."
  },
  {
    "user_input": "Tôi có thể tìm thấy hướng dẫn lấy mẫu ảo giác cho hội thoại và tóm tắt ở đâu?",
    "retrieved_contexts": [
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a.",
      "## **appendix**\n\nwe provide some extra information about our\nbenchmark as supplementary materials. the appendix is organized into three sections:\n\n  - instructions of hallucination sampling are presented in appendix a;\n\n  instructions of hallucination filtering are presented in appendix b;\n\n  - instructions of evaluation are presented in appendix c;\n\n  - details of our benchmark are presented in appendix d.\n## **a hallucination sampling**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 9 and\ntable 10, respectively.\n## **b hallucination filtering**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 11\nand table 12, respectively.\n## **c hallucination recognition**\n\nthe hallucination recognition instructions for qa,\ndialogue and summarization are shown in table 13,\ntable 14 and table 15, respectively.",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets.",
      "table 9: instruction of hallucination sampling for knowledge-grounded dialogue.\n\ni want you act as a hallucination summary generator. given a document and the right summary, your objective\nis to write a hallucinated summary that sounds plausible but is factually incorrect. you should write the\nhallucinated summary using the following method (each with some examples):\n\nor\n\nor\nyou are trying to write a summary but there is a factual contradiction between the summary and the document.\n\n<demonstrations>\n\nyou should try your best to make the summary become hallucinated. #hallucinated summary# can only have\nabout 5 more words than #right summary#.\n\n**#document#:** <here is the test document>\n**#right summary#:** <here is the right summary of the test document>\n**#hallucinated summary#:**\n\ntable 10: instruction of hallucination sampling for text summarization.\n\n6460",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "llms in three aspects. firstly, based on our generated and annotated samples, researchers can use\nthem to analyze what types of content llms tend\nto generate hallucinations. second, researchers can\nfurther evaluate the ability of llms to recognize\nhallucinations in the generated samples. for example, given a question and an answer, llms can\nbe asked to determine whether the answer contains",
      "table 2: instruction of hallucination sampling for question answering. the blue text denotes the intention descrip\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration.",
      "-----\n\n**tasks** **#sample** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 10000 2280 1378 5102 1240\n**dialogue** 10000 8330 1196 474   **summa.** 10000 2614 3562 3824   \ntable 16: number of generated samples for each hallucination pattern (p-i/ii/iii/iv). “‘summa.” is short\nfor summarization. “-” is due to that we consider three\n\npatterns in dialogue and summarization.\n\n6464\n\n\n-----",
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat",
      "|---|---|---|\n|chatgpt response|1. july 4, 1776 - declaration of independence sign- ing 2. april 12, 1861 - start of the civil war 3. august 18, 1920 - 19th amendment ratification gr- anting women the right to vote 4. december 7, 1941 - pearl harbor attack during wo- rld war ii 5. september 11, 2001 - terrorist attacks on new york city and washington d.c.|as an ai language model, i cannot create visualization but i can provide you with the following information: according to world bank data, india and china have shown significant economic growth in the past two de- cades. in 1998, india’s gdp growth rate was 4.8%, while china’s gdp growth rate was 9.3%. in the following years, the gdp growth rate fluctuat- ed, but china maintained a higher growth rate than india. by 2019, china’s gdp growth rate was 6.1%, while india’s gdp growth rate was 4.2%.|",
      "table 11: instruction of hallucination filtering for knowledge-grounded dialogue.\n\ni want you act as a summary judge. given a document and two summaries, your objective is to select the best\nand correct summary without hallucination and non-factual information. here are some examples:\n\n...\n\n<demonstrations>\n\n...\n\nyou should try your best to select the best and correct summary. if both summaries are incorrect, choose the\nbetter one. you must select a summary from the provided two summaries.\n\n**#document#:** <here is the test document>\n**#summary 1#:** <here is the hallucinated summary generated by the first channel>\n**#summary 2#:** <here is the hallucinated summary generated by the second channel>\n**#your choice#:**\n\ntable 12: instruction of hallucination filtering for text summarization.\n\n6461"
    ],
    "response": "1.  **Hướng dẫn lấy mẫu ảo giác cho hội thoại:** Có thể tìm thấy trong **Bảng 9** của **Phụ lục A**.\n2.  **Hướng dẫn lấy mẫu ảo giác cho tóm tắt:** Có thể tìm thấy trong **Bảng 10** của **Phụ lục A**.",
    "reference": "Trong bảng 9-10 ở phụ lục A."
  },
  {
    "user_input": "Mục đích của việc thiết kế hướng dẫn lọc ảo giác được tăng cường bằng các câu trả lời thực tế là gì?",
    "retrieved_contexts": [
      "to select the most plausible and difficult hallucinated samples from the above two sampling methods. as shown in table 3, we design the instruction\nof hallucination filtering enhanced by ground-truth\nanswers to select the best answer from two hallucinated candidates. in the instruction of filtering,\nthe demonstration includes the ground-truth correct\nanswer ( *e.g.,* you.s. highway 60) and a hallucinated\ncounterpart ( *e.g.,* you.s. highway 70). while, in the\ntest example, we input two hallucinated answers.\nfollowing the demonstrations, we expect chatgpt\nto select one of the hallucinated answers that is the",
      "table 11: instruction of hallucination filtering for knowledge-grounded dialogue.\n\ni want you act as a summary judge. given a document and two summaries, your objective is to select the best\nand correct summary without hallucination and non-factual information. here are some examples:\n\n...\n\n<demonstrations>\n\n...\n\nyou should try your best to select the best and correct summary. if both summaries are incorrect, choose the\nbetter one. you must select a summary from the provided two summaries.\n\n**#document#:** <here is the test document>\n**#summary 1#:** <here is the hallucinated summary generated by the first channel>\n**#summary 2#:** <here is the hallucinated summary generated by the second channel>\n**#your choice#:**\n\ntable 12: instruction of hallucination filtering for text summarization.\n\n6461",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "table 13: instruction of hallucination recognition for question answering.\n\ni want you act as a response judge. given a dialogue history and a response, your objective is to determine if the\nprovided response contains non-factual or hallucinated information. you should give your judgement based\non the following hallucination types and the world knowledge.\n\nyou are trying to determine if the true entity in the response is replaced with a dissimilar entity.\n\n<demonstrations>\n\nyou should try your best to determine if the response contains non-factual or hallucinated information according\nto the above hallucination types. the answer you give must be “yes” or “no”.\n\n**#dialogue history#:** <here is the dialogue history>\n**#response#:** <here is the hallucinated response or right response>\n**#your judgement#:**\n\ntable 14: instruction of hallucination recognition for knowledge-grounded dialogue.\n\n6462",
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets."
    ],
    "response": "Mục đích của việc thiết kế hướng dẫn lọc ảo giác được tăng cường bằng các câu trả lời thực tế là chọn mẫu ảo giác правдоподобные và khó nhất để đánh giá LLM trong việc nhận biết ảo giác.",
    "reference": "Để chọn câu trả lời tốt nhất từ hai ứng viên ảo giác."
  },
  {
    "user_input": "Trong hướng dẫn lọc, phần trình diễn bao gồm những gì?",
    "retrieved_contexts": [
      "to select the most plausible and difficult hallucinated samples from the above two sampling methods. as shown in table 3, we design the instruction\nof hallucination filtering enhanced by ground-truth\nanswers to select the best answer from two hallucinated candidates. in the instruction of filtering,\nthe demonstration includes the ground-truth correct\nanswer ( *e.g.,* you.s. highway 60) and a hallucinated\ncounterpart ( *e.g.,* you.s. highway 70). while, in the\ntest example, we input two hallucinated answers.\nfollowing the demonstrations, we expect chatgpt\nto select one of the hallucinated answers that is the",
      "## **appendix**\n\nwe provide some extra information about our\nbenchmark as supplementary materials. the appendix is organized into three sections:\n\n  - instructions of hallucination sampling are presented in appendix a;\n\n  instructions of hallucination filtering are presented in appendix b;\n\n  - instructions of evaluation are presented in appendix c;\n\n  - details of our benchmark are presented in appendix d.\n## **a hallucination sampling**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 9 and\ntable 10, respectively.\n## **b hallucination filtering**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 11\nand table 12, respectively.\n## **c hallucination recognition**\n\nthe hallucination recognition instructions for qa,\ndialogue and summarization are shown in table 13,\ntable 14 and table 15, respectively.",
      "table 11: instruction of hallucination filtering for knowledge-grounded dialogue.\n\ni want you act as a summary judge. given a document and two summaries, your objective is to select the best\nand correct summary without hallucination and non-factual information. here are some examples:\n\n...\n\n<demonstrations>\n\n...\n\nyou should try your best to select the best and correct summary. if both summaries are incorrect, choose the\nbetter one. you must select a summary from the provided two summaries.\n\n**#document#:** <here is the test document>\n**#summary 1#:** <here is the hallucinated summary generated by the first channel>\n**#summary 2#:** <here is the hallucinated summary generated by the second channel>\n**#your choice#:**\n\ntable 12: instruction of hallucination filtering for text summarization.\n\n6461",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "-----\n\nconversational instruction\n\n\nhigh-quality hallucination filtering\n\n\n\nmax-voting\n\n\n\nhuman annotation\n\nfigure 1: construction pipeline of halueval, including automatic generation (top) and human annotation (bottom).",
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "table 2: instruction of hallucination sampling for question answering. the blue text denotes the intention descrip\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration.",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets."
    ],
    "response": "Trong hướng dẫn lọc, phần trình diễn bao gồm:\n\n1.  **Câu trả lời đúng thực tế:** Ví dụ: \"you.s. highway 60\".\n2.  **Đối tác ảo giác:** Ví dụ: \"you.s. highway 70\".",
    "reference": "Phần trình diễn bao gồm câu trả lời đúng thực tế và một câu trả lời ảo giác tương ứng."
  },
  {
    "user_input": "Sau khi xem các minh họa, ChatGPT được kỳ vọng sẽ làm gì?",
    "retrieved_contexts": [
      "to select the most plausible and difficult hallucinated samples from the above two sampling methods. as shown in table 3, we design the instruction\nof hallucination filtering enhanced by ground-truth\nanswers to select the best answer from two hallucinated candidates. in the instruction of filtering,\nthe demonstration includes the ground-truth correct\nanswer ( *e.g.,* you.s. highway 60) and a hallucinated\ncounterpart ( *e.g.,* you.s. highway 70). while, in the\ntest example, we input two hallucinated answers.\nfollowing the demonstrations, we expect chatgpt\nto select one of the hallucinated answers that is the",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "llms in three aspects. firstly, based on our generated and annotated samples, researchers can use\nthem to analyze what types of content llms tend\nto generate hallucinations. second, researchers can\nfurther evaluate the ability of llms to recognize\nhallucinations in the generated samples. for example, given a question and an answer, llms can\nbe asked to determine whether the answer contains",
      "table 2: instruction of hallucination sampling for question answering. the blue text denotes the intention descrip\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration.",
      "-----\n\n**tasks** **#sample** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 10000 2280 1378 5102 1240\n**dialogue** 10000 8330 1196 474   **summa.** 10000 2614 3562 3824   \ntable 16: number of generated samples for each hallucination pattern (p-i/ii/iii/iv). “‘summa.” is short\nfor summarization. “-” is due to that we consider three\n\npatterns in dialogue and summarization.\n\n6464\n\n\n-----",
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat",
      "|---|---|---|\n|chatgpt response|1. july 4, 1776 - declaration of independence sign- ing 2. april 12, 1861 - start of the civil war 3. august 18, 1920 - 19th amendment ratification gr- anting women the right to vote 4. december 7, 1941 - pearl harbor attack during wo- rld war ii 5. september 11, 2001 - terrorist attacks on new york city and washington d.c.|as an ai language model, i cannot create visualization but i can provide you with the following information: according to world bank data, india and china have shown significant economic growth in the past two de- cades. in 1998, india’s gdp growth rate was 4.8%, while china’s gdp growth rate was 9.3%. in the following years, the gdp growth rate fluctuat- ed, but china maintained a higher growth rate than india. by 2019, china’s gdp growth rate was 6.1%, while india’s gdp growth rate was 4.2%.|",
      "chatgpt cannot access the web to obtain the official data. after providing official information\nretrieved from world bank, the refined span displays answers that contain the correct information.\nthe above two examples illustrate that retrieving\nknowledge related to queries can help chatgpt significantly reduce the hallucinations in the response,\nespecially those factual errors.",
      "ods to generate diverse samples. for each method,\nchatgpt follows the instruction of hallucination\n\nsampling in different manners. as shown in figure 1, the first method adopts a *one-pass* instruction following schema, where we directly feed the\ncomplete instruction (table 2) into chatgpt and\ngenerate a hallucinated answer. on the other hand,\nthe second method uses a *conversational* schema,\nwhere we teach chatgpt to successively learn part\nof the instruction and make sure it has mastered.\n\nbased on the learned instructions, chatgpt will\ngenerate another hallucinated answer. through the\ntwo different sampling strategies, we can obtain\ndiverse and multi-facet hallucinated answers for\neach question, which will be further filtered and\nselected for the most plausible and difficult one.",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language.",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate."
    ],
    "response": "Sau khi xem các minh họa, ChatGPT được kỳ vọng sẽ chọn một trong các câu trả lời ảo giác правдоподобные nhất.",
    "reference": "Chọn một trong các câu trả lời ảo giác."
  },
  {
    "user_input": "Tổng cộng có bao nhiêu mẫu ảo giác đã được tạo ra cho ba nhiệm vụ?",
    "retrieved_contexts": [
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets.",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "-----\n\n**tasks** **#sample** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 10000 2280 1378 5102 1240\n**dialogue** 10000 8330 1196 474   **summa.** 10000 2614 3562 3824   \ntable 16: number of generated samples for each hallucination pattern (p-i/ii/iii/iv). “‘summa.” is short\nfor summarization. “-” is due to that we consider three\n\npatterns in dialogue and summarization.\n\n6464\n\n\n-----",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449"
    ],
    "response": "```markdown\n1.  **Tổng số mẫu ảo giác:**\n    *   Tổng cộng có 30.000 mẫu ảo giác đã được tạo ra cho ba nhiệm vụ.\n```",
    "reference": "Tổng cộng có 30.000 mẫu ảo giác đã được tạo ra."
  },
  {
    "user_input": "Mục đích của việc tạo ra các mẫu ảo giác này là gì?",
    "retrieved_contexts": [
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance."
    ],
    "response": "```markdown\n1.  **Mục đích của việc tạo mẫu ảo giác:**\n    *   Các mẫu ảo giác được thiết kế để thách thức khả năng nhận biết ảo giác của các LLM và phân tích các điểm mù thông tin của LLM.\n    *   Chọn các mẫu правдоподобные và khó nhất để đánh giá khả năng phát hiện ảo giác của LLM.\n```",
    "reference": "Các mẫu ảo giác này được sử dụng để đánh giá khả năng nhận biết ảo giác của các mô hình ngôn ngữ lớn (LLMs)."
  },
  {
    "user_input": "Quá trình tạo ra các mẫu ảo giác bao gồm những bước chính nào?",
    "retrieved_contexts": [
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "**2.1** **automatic generation**\n\nour generation pipeline includes two steps: 1) diverse hallucination sampling, and 2) high-quality\nhallucination filtering. we employ chatgpt to\nexecute the creation pipeline automatically.\n\n**diverse hallucination sampling.** since a factual\ntext can be hallucinated from different aspects, we\npropose two different hallucination sampling meth\n\n6450",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for"
    ],
    "response": "```markdown\n1.  **Các bước chính trong quá trình tạo mẫu ảo giác:**\n\n    *   **Lấy mẫu ảo giác đa dạng:** Sử dụng hai phương pháp khác nhau (One-Pass và Conversational) để tạo ra các mẫu ảo giác từ nhiều khía cạnh khác nhau.\n    *   **Lọc ảo giác chất lượng cao:** Lọc và chọn các mẫu правдоподобные và khó nhất để đánh giá LLM.\n```",
    "reference": "Quá trình tạo ra các mẫu ảo giác bao gồm việc lấy mẫu (sampling) và sau đó lọc (filtering)."
  },
  {
    "user_input": "Mục đích của việc mời người đánh giá (labelers) là gì?",
    "retrieved_contexts": [
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "cording to recent work (manakul et al., 2023), hallucinations are likely to appear in diverged and conflicting responses of llms. based on the filtered\nuser queries and chatgpt responses, we invite\nhuman labelers to annotate whether the response\ncontains hallucinated information and mark cor\nresponding spans. as shown in table 1, for the\nuser query “ *retrieve the oldest photo of a cat* ”, the\nresponse generated by chatgpt contains unverifiable information. these human-annotated queries\nand responses can be used to analyze what types\nof content llms tend to hallucinate and further",
      "**labeler details.** annotating the hallucination\nin chatgpt responses is a very challenging task,\nwhich requires good reading comprehension skills\nand using search engine to look up relevant information for judgement. thus, from an initial pool\nof labeler candidates, we select labelers who are\ngood at english passage reading with at least an\nundergraduate-level education. besides, following\n(ouyang et al., 2022), we have labelers annotate a\nsmall number of test examples and measure their\nagreement with the labels of researchers, and finally\nwe choose thirty human labelers with the highest\nagreement scores. we report fleiss’s kappa ( *κ* ) to\nindicate the reliability of agreement between human labelers. we compute *κ* on 5 *,* 000 annotated\nsamples and obtain *κ* = 0 *.* 811 ( 0 *.* 80 *≤* *κ ≤* 1 *.* 00 )\nshowing a perfect agreement.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat",
      "hallucinated content. finally, our benchmark can\nbe further paired with human annotation to assess\nwhether the llms’ output contains hallucinations,\nsince the samples in our benchmark are specially\ndesigned for testing the hallucinations of llms.\nto use our benchmark, users can run the code in\nour project repository to conduct the corresponding\nevaluation and analysis. users can use our provided\ninstructions on their own datasets to evaluate llms\n\non hallucinations.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,"
    ],
    "response": "```markdown\n1.  **Mục đích của việc mời người đánh giá (labelers):**\n\n    *   **Đánh giá nội dung ảo giác:** Mời người đánh giá để xác định xem phản hồi của ChatGPT có chứa nội dung ảo giác hay không.\n    *   **Đánh dấu các đoạn văn bản liên quan:** Người đánh giá sẽ đánh dấu các đoạn văn bản tương ứng chứa thông tin ảo giác.\n    *   **Phân tích nội dung và nguyên nhân:** Dữ liệu từ người đánh giá được sử dụng để phân tích loại nội dung mà LLM có xu hướng tạo ra ảo giác và xác định nguyên nhân cơ bản của những ảo giác này.\n```",
    "reference": "Để đánh giá xem các phản hồi của ChatGPT có chứa nội dung bịa đặt (hallucinated) hay không."
  },
  {
    "user_input": "Dữ liệu từ bộ Alpaca được sử dụng cho mục đích gì trong nghiên cứu này?",
    "retrieved_contexts": [
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text."
    ],
    "response": "```markdown\n1.  **Sử dụng dữ liệu từ bộ Alpaca:**\n\n    *   **Chọn lọc truy vấn người dùng:** Dữ liệu từ bộ Alpaca (52k instruction tuning dataset) được sử dụng để chọn lọc ra các truy vấn người dùng mà LLM có khả năng tạo ra ảo giác cao nhất.\n    *   **Gán nhãn bởi con người:** Các truy vấn người dùng đã chọn lọc và phản hồi từ ChatGPT được gán nhãn bởi con người để xác định nội dung ảo giác.\n```",
    "reference": "Để huấn luyện hoặc đánh giá các mô hình ngôn ngữ lớn."
  },
  {
    "user_input": "Tại sao lại cần sàng lọc trước các truy vấn của người dùng trước khi dán nhãn (labeling)?",
    "retrieved_contexts": [
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat",
      "cording to recent work (manakul et al., 2023), hallucinations are likely to appear in diverged and conflicting responses of llms. based on the filtered\nuser queries and chatgpt responses, we invite\nhuman labelers to annotate whether the response\ncontains hallucinated information and mark cor\nresponding spans. as shown in table 1, for the\nuser query “ *retrieve the oldest photo of a cat* ”, the\nresponse generated by chatgpt contains unverifiable information. these human-annotated queries\nand responses can be used to analyze what types\nof content llms tend to hallucinate and further",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "table 4: a generated hallucinated qa example and a\nhuman-labeled chatgpt response for a user query.\n\nselection procedure. specifically, we use chatgpt\nto sample three responses for each user query and\ncompute their average semantic similarity using\nbertscore (zhang et al., 2020). we finally retain 5 *,* 000 user queries with the lowest similarities.\naccording to recent work (manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of llms. for each query and\nchatgpt response, human labelers will annotate\nwhether the response contains hallucinated information (“yes” or “no”) and list the corresponding\n\n\n6452",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use"
    ],
    "response": "```markdown\n1.  **Lý do cần sàng lọc trước các truy vấn của người dùng:**\n\n    *   **Tăng khả năng xuất hiện ảo giác:** Để chọn ra các truy vấn mà LLM có khả năng cao tạo ra ảo giác, từ đó tập trung vào các trường hợp có khả năng gây ảo giác cao nhất trong quá trình gán nhãn.\n    *   **Đảm bảo tính đa dạng và mâu thuẫn:** Dựa trên nghiên cứu gần đây, ảo giác có khả năng xuất hiện trong các phản hồi khác nhau và mâu thuẫn của LLM. Việc sàng lọc giúp chọn ra các truy vấn có phản hồi đa dạng và mâu thuẫn, từ đó tăng khả năng phát hiện ảo giác.```",
    "reference": "Để loại bỏ các truy vấn mà mô hình ngôn ngữ lớn (LLM) có khả năng tạo ra thông tin sai lệch (hallucination) cao, từ đó cải thiện độ tin cậy và chính xác của các phản hồi."
  },
  {
    "user_input": "ChatGPT được sử dụng để lấy mẫu bao nhiêu phản hồi cho mỗi truy vấn của người dùng?",
    "retrieved_contexts": [
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "table 4: a generated hallucinated qa example and a\nhuman-labeled chatgpt response for a user query.\n\nselection procedure. specifically, we use chatgpt\nto sample three responses for each user query and\ncompute their average semantic similarity using\nbertscore (zhang et al., 2020). we finally retain 5 *,* 000 user queries with the lowest similarities.\naccording to recent work (manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of llms. for each query and\nchatgpt response, human labelers will annotate\nwhether the response contains hallucinated information (“yes” or “no”) and list the corresponding\n\n\n6452",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future."
    ],
    "response": "```markdown\n1.  **Số lượng phản hồi ChatGPT tạo ra:**\n    *   ChatGPT được sử dụng để lấy mẫu ba phản hồi cho mỗi truy vấn của người dùng.\n    *   Sau đó, tính toán độ tương đồng ngữ nghĩa trung bình bằng BERTScore để chọn ra 5.000 truy vấn có độ tương đồng thấp nhất.\n```",
    "reference": "ChatGPT được sử dụng để lấy mẫu ba phản hồi cho mỗi truy vấn của người dùng."
  },
  {
    "user_input": "BERTScore được sử dụng để làm gì trong quy trình này?",
    "retrieved_contexts": [
      "table 4: a generated hallucinated qa example and a\nhuman-labeled chatgpt response for a user query.\n\nselection procedure. specifically, we use chatgpt\nto sample three responses for each user query and\ncompute their average semantic similarity using\nbertscore (zhang et al., 2020). we finally retain 5 *,* 000 user queries with the lowest similarities.\naccording to recent work (manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of llms. for each query and\nchatgpt response, human labelers will annotate\nwhether the response contains hallucinated information (“yes” or “no”) and list the corresponding\n\n\n6452",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "**3.2.2** **improvement strategies**\n\nin this part, we design several strategies to improve\nthe ability of llms to recognize hallucination. the\nresults are shown in table 8.\n\n**knowledge retrieval.** retrieving relevant knowledge is a widely used strategy to eliminate hallucination (lewis et al., 2020; li et al., 2023a). therefore, we supply chatgpt with the knowledge facts\nretrieved from wikipedia (except for that summarization does not need external information besides",
      "## **4 related work**\n\n**hallucination in llms.** hallucination in llms is\n\nconcerning since it hinders performance and raises\nsafety risks in real-world application. to alleviate\nthis issue, prior studies have proposed to use a verification system to identify non-factual entities in\ntext summarization (zhao et al., 2020), invoke interfaces of structured data ( *e.g.,* knowledge graph,\ndatabase) to obtain related evidence (jiang et al.,\n2023; lan et al., 2022), and train a token-level fact\ncritic to recognize hallucination and rectify them\nin dialogue (dziri et al., 2021). to enhance the\nunderstanding of hallucination in llms and pro\n\n6455",
      "cording to recent work (manakul et al., 2023), hallucinations are likely to appear in diverged and conflicting responses of llms. based on the filtered\nuser queries and chatgpt responses, we invite\nhuman labelers to annotate whether the response\ncontains hallucinated information and mark cor\nresponding spans. as shown in table 1, for the\nuser query “ *retrieve the oldest photo of a cat* ”, the\nresponse generated by chatgpt contains unverifiable information. these human-annotated queries\nand responses can be used to analyze what types\nof content llms tend to hallucinate and further",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations."
    ],
    "response": "```markdown\n1.  **Vai trò của BERTScore:**\n\n    *   **Đo độ tương đồng ngữ nghĩa:** BERTScore được sử dụng để tính toán độ tương đồng ngữ nghĩa trung bình giữa ba phản hồi mà ChatGPT tạo ra cho mỗi truy vấn của người dùng.\n    *   **Chọn truy vấn:** Dựa trên điểm BERTScore, quy trình này giữ lại 5.000 truy vấn của người dùng có độ tương đồng thấp nhất giữa các phản hồi. Điều này là do các phản hồi khác nhau và mâu thuẫn có khả năng chứa thông tin gây ảo giác cao hơn.\n```",
    "reference": "BERTScore được sử dụng để tính toán độ tương đồng ngữ nghĩa trung bình giữa các phản hồi."
  },
  {
    "user_input": "Dấu hiệu nào cho thấy một phản hồi có thể chứa thông tin sai lệch (hallucination)?",
    "retrieved_contexts": [
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate."
    ],
    "response": "```markdown\nKhông có tài liệu nào đề cập trực tiếp đến các dấu hiệu cụ thể cho thấy một phản hồi có thể chứa thông tin sai lệch (hallucination).\n```",
    "reference": "Thông tin sai lệch có khả năng xuất hiện trong các phản hồi khác biệt và mâu thuẫn."
  },
  {
    "user_input": "Những khía cạnh nào được xem xét để đánh giá hiện tượng ảo giác (hallucination)?",
    "retrieved_contexts": [
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "hallucinated content. finally, our benchmark can\nbe further paired with human annotation to assess\nwhether the llms’ output contains hallucinations,\nsince the samples in our benchmark are specially\ndesigned for testing the hallucinations of llms.\nto use our benchmark, users can run the code in\nour project repository to conduct the corresponding\nevaluation and analysis. users can use our provided\ninstructions on their own datasets to evaluate llms\n\non hallucinations.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for"
    ],
    "response": "```markdown\n1.  **Đánh giá ảo giác (Hallucination evaluation):**    *   Các nghiên cứu tập trung vào việc đánh giá hiện tượng ảo giác của các mô hình trong các tác vụ NLP khác nhau.\n    *   Halueval được sử dụng để đánh giá hiệu suất của LLM trong việc nhận biết ảo giác.\n    *   Việc đánh giá bao gồm việc sử dụng các bộ dữ liệu được thiết kế đặc biệt để kiểm tra khả năng tạo ra thông tin sai lệch của LLM.\n    *   Đánh giá có thể được thực hiện tự động hoặc kết hợp với đánh giá của con người.\n2.  **Các khía cạnh cụ thể được xem xét trong Halueval:**\n\n    *   **Độ xác thực (Factualness):** Kiểm tra xem thông tin có chính xác và dựa trên thực tế hay không.\n    *   **Tính đầy đủ (Comprehension):** Đánh giá xem phản hồi có đầy đủ và dễ hiểu hay không.\n    *   **Tính cụ thể (Specificity):** Xem xét mức độ chi tiết và rõ ràng của thông tin.\n    *   **Suy luận (Inference):** Đánh giá khả năng suy luận hợp lý và chính xác.\n3.  **Mục tiêu của Halueval:**\n\n    *   Xác định loại nội dung và mức độ mà LLM có xu hướng tạo ra ảo giác.\n    *   Cung cấp một nền tảng công khai để thể hiện những điểm mù của LLM trong việc giải quyết ảo giác.\n    *   Đánh giá khả năng của LLM trong việc nhận biết ảo giác trong văn bản.\n```",
    "reference": "Hiện tượng ảo giác được xem xét trên ba khía cạnh: không thể kiểm chứng, không đúng sự thật và không liên quan."
  },
  {
    "user_input": "Nếu có ba người đánh giá một phản hồi, làm thế nào để xác định nhãn ảo giác cuối cùng?",
    "retrieved_contexts": [
      "figure 3: topic distribution for chatgpt responses.\n\nspans. the hallucination is considered from the following three aspects: *unverifiable*, *non-factual*, and\n*irrelevant* . each response is labeled by three human labelers, and we adopt the max-voting strategy\nto determine the final hallucination label.",
      "**labeler details.** annotating the hallucination\nin chatgpt responses is a very challenging task,\nwhich requires good reading comprehension skills\nand using search engine to look up relevant information for judgement. thus, from an initial pool\nof labeler candidates, we select labelers who are\ngood at english passage reading with at least an\nundergraduate-level education. besides, following\n(ouyang et al., 2022), we have labelers annotate a\nsmall number of test examples and measure their\nagreement with the labels of researchers, and finally\nwe choose thirty human labelers with the highest\nagreement scores. we report fleiss’s kappa ( *κ* ) to\nindicate the reliability of agreement between human labelers. we compute *κ* on 5 *,* 000 annotated\nsamples and obtain *κ* = 0 *.* 811 ( 0 *.* 80 *≤* *κ ≤* 1 *.* 00 )\nshowing a perfect agreement.",
      "table 4: a generated hallucinated qa example and a\nhuman-labeled chatgpt response for a user query.\n\nselection procedure. specifically, we use chatgpt\nto sample three responses for each user query and\ncompute their average semantic similarity using\nbertscore (zhang et al., 2020). we finally retain 5 *,* 000 user queries with the lowest similarities.\naccording to recent work (manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of llms. for each query and\nchatgpt response, human labelers will annotate\nwhether the response contains hallucinated information (“yes” or “no”) and list the corresponding\n\n\n6452",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets."
    ],
    "response": "```markdown\n1.  **Chiến lược Max-Voting:**\n\n    *   Khi có ba người đánh giá một phản hồi, chiến lược \"max-voting\" (bỏ phiếu đa số) được sử dụng để xác định nhãn ảo giác cuối cùng.\n    *   Điều này có nghĩa là nhãn được gán cho phản hồi sẽ là nhãn được nhiều người đánh giá chọn nhất.\n2.  **Độ tin cậy giữa những người đánh giá:**\n\n    *   Độ tin cậy giữa những người đánh giá được đo bằng Fleiss's Kappa (κ).\n    *   Trong Halueval, κ = 0.811 (0.80 ≤ κ ≤ 1.00), cho thấy sự đồng ý gần như hoàn hảo giữa những người đánh giá.\n```",
    "reference": "Nhãn ảo giác cuối cùng được xác định bằng cách sử dụng chiến lược bỏ phiếu đa số (max-voting)."
  },
  {
    "user_input": "Hình 3 thể hiện điều gì?",
    "retrieved_contexts": [
      "figure 3: topic distribution for chatgpt responses.\n\nspans. the hallucination is considered from the following three aspects: *unverifiable*, *non-factual*, and\n*irrelevant* . each response is labeled by three human labelers, and we adopt the max-voting strategy\nto determine the final hallucination label.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "## **appendix**\n\nwe provide some extra information about our\nbenchmark as supplementary materials. the appendix is organized into three sections:\n\n  - instructions of hallucination sampling are presented in appendix a;\n\n  instructions of hallucination filtering are presented in appendix b;\n\n  - instructions of evaluation are presented in appendix c;\n\n  - details of our benchmark are presented in appendix d.\n## **a hallucination sampling**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 9 and\ntable 10, respectively.\n## **b hallucination filtering**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 11\nand table 12, respectively.\n## **c hallucination recognition**\n\nthe hallucination recognition instructions for qa,\ndialogue and summarization are shown in table 13,\ntable 14 and table 15, respectively.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "-----\n\npc2\n\n\n\n\n\n\n\n\n\n\npolice\n\n\n\n\n\n\n\n\n\n\ncar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|3 8 ce r family animal dog ficer child mother|club|\n|---|---|\n|family pc1|club seaso sterl ci 9in tyg fan7 g2am old player ent|\n|ampaign scho foo4d officialstu1100d ernmen5t6 leader party||\n\n\n\n(a) qa topics\n\n\n(b) dialogue topics\n\n\n(c) summarization topics\n\n\nfigure 2: topic distributions for qa, knowledge-grounded dialogue, and text summarization. the samples of each\ntask are classified into 10 topics, and the red circles denote the topics of failed recognized samples by chatgpt.",
      "table 2: instruction of hallucination sampling for question answering. the blue text denotes the intention descrip\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration."
    ],
    "response": "```markdown\nHình 3 thể hiện sự phân bố chủ đề cho các phản hồi của ChatGPT.\n```",
    "reference": "Hình 3 thể hiện sự phân bố chủ đề cho các phản hồi của ChatGPT."
  },
  {
    "user_input": "Có bao nhiêu người được chọn làm người đánh giá sau khi kiểm tra độ chính xác?",
    "retrieved_contexts": [
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "**2.2** **human annotation**\n\nbesides generating hallucinated samples, we also\ninvite human labelers to annotate whether chatgpt\n\nresponses contain hallucinated content.\nwe annotate the general user queries and chatgpt responses from the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023), which\nhas been widely used by recent llms. to screen\nout user queries where llms are most likely to\nproduce hallucination for labeling, we design a pre\n\n**question** in what political party was the man who officially opened royal spa centre in 1972?\n\n**right answer** conservative\n\n**hallucinated**\n**answer** [labour party]\n\n**user query** retrieve the oldest photo of a cat",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "**labeler details.** annotating the hallucination\nin chatgpt responses is a very challenging task,\nwhich requires good reading comprehension skills\nand using search engine to look up relevant information for judgement. thus, from an initial pool\nof labeler candidates, we select labelers who are\ngood at english passage reading with at least an\nundergraduate-level education. besides, following\n(ouyang et al., 2022), we have labelers annotate a\nsmall number of test examples and measure their\nagreement with the labels of researchers, and finally\nwe choose thirty human labelers with the highest\nagreement scores. we report fleiss’s kappa ( *κ* ) to\nindicate the reliability of agreement between human labelers. we compute *κ* on 5 *,* 000 annotated\nsamples and obtain *κ* = 0 *.* 811 ( 0 *.* 80 *≤* *κ ≤* 1 *.* 00 )\nshowing a perfect agreement.",
      "figure 3: topic distribution for chatgpt responses.\n\nspans. the hallucination is considered from the following three aspects: *unverifiable*, *non-factual*, and\n*irrelevant* . each response is labeled by three human labelers, and we adopt the max-voting strategy\nto determine the final hallucination label.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "table 4: a generated hallucinated qa example and a\nhuman-labeled chatgpt response for a user query.\n\nselection procedure. specifically, we use chatgpt\nto sample three responses for each user query and\ncompute their average semantic similarity using\nbertscore (zhang et al., 2020). we finally retain 5 *,* 000 user queries with the lowest similarities.\naccording to recent work (manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of llms. for each query and\nchatgpt response, human labelers will annotate\nwhether the response contains hallucinated information (“yes” or “no”) and list the corresponding\n\n\n6452",
      "44\n\n**claude 2** 69.78 64.73 57.75 75.00\n\n**claude** 67.60 64.83 53.76 73.88\n\n**davinci002** 60.05 60.81 47.77 80.42\n\n**davinci003** 49.65 68.37 48.07 80.40\n\n**gpt-3** 49.21 50.02 51.23 72.72\n\n**llama 2** 49.60 43.99 49.55 20.46\n\n**chatglm** 47.93 44.41 48.57 30.92\n\n**falcon** 39.66 29.08 42.71 18.98\n\n**vicuna** 60.34 46.35 45.62 19.48\n**alpaca** 6.68 17.55 20.63 9.54\n\ntable 5: accuracy (%) of classifying whether a sample\ncontains hallucinated contents.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,"
    ],
    "response": "```markdown\n1.  **Số lượng người đánh giá:**\n    *   Từ một nhóm ứng viên ban đầu, 30 người đánh giá có điểm số đánh giá phù hợp cao nhất so với nhãn của các nhà nghiên cứu đã được chọn.\n```",
    "reference": "Có ba mươi người đánh giá được chọn."
  },
  {
    "user_input": "Những kỹ năng nào là cần thiết để đánh giá độ tin cậy của các phản hồi từ ChatGPT?",
    "retrieved_contexts": [
      "**labeler details.** annotating the hallucination\nin chatgpt responses is a very challenging task,\nwhich requires good reading comprehension skills\nand using search engine to look up relevant information for judgement. thus, from an initial pool\nof labeler candidates, we select labelers who are\ngood at english passage reading with at least an\nundergraduate-level education. besides, following\n(ouyang et al., 2022), we have labelers annotate a\nsmall number of test examples and measure their\nagreement with the labels of researchers, and finally\nwe choose thirty human labelers with the highest\nagreement scores. we report fleiss’s kappa ( *κ* ) to\nindicate the reliability of agreement between human labelers. we compute *κ* on 5 *,* 000 annotated\nsamples and obtain *κ* = 0 *.* 811 ( 0 *.* 80 *≤* *κ ≤* 1 *.* 00 )\nshowing a perfect agreement.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text."
    ],
    "response": "```markdown\n1.  **Kỹ năng cần thiết cho người đánh giá:**\n    *   **Kỹ năng đọc hiểu tốt:** Để hiểu rõ nội dung của các phản hồi.\n    *   **Sử dụng công cụ tìm kiếm:** Để tra cứu và xác minh thông tin liên quan, nhằm đánh giá tính xác thực của các phản hồi từ ChatGPT.\n    *   **Trình độ học vấn:** Tối thiểu là trình độ đại học về khả năng đọc hiểu các đoạn văn tiếng Anh.\n```",
    "reference": "Cần có kỹ năng đọc hiểu tốt và khả năng sử dụng công cụ tìm kiếm để đánh giá thông tin."
  },
  {
    "user_input": "Giá trị Kappa Fleiss là bao nhiêu và nó cho thấy điều gì về sự đồng thuận giữa những người đánh giá?",
    "retrieved_contexts": [
      "**labeler details.** annotating the hallucination\nin chatgpt responses is a very challenging task,\nwhich requires good reading comprehension skills\nand using search engine to look up relevant information for judgement. thus, from an initial pool\nof labeler candidates, we select labelers who are\ngood at english passage reading with at least an\nundergraduate-level education. besides, following\n(ouyang et al., 2022), we have labelers annotate a\nsmall number of test examples and measure their\nagreement with the labels of researchers, and finally\nwe choose thirty human labelers with the highest\nagreement scores. we report fleiss’s kappa ( *κ* ) to\nindicate the reliability of agreement between human labelers. we compute *κ* on 5 *,* 000 annotated\nsamples and obtain *κ* = 0 *.* 811 ( 0 *.* 80 *≤* *κ ≤* 1 *.* 00 )\nshowing a perfect agreement.",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "44\n\n**claude 2** 69.78 64.73 57.75 75.00\n\n**claude** 67.60 64.83 53.76 73.88\n\n**davinci002** 60.05 60.81 47.77 80.42\n\n**davinci003** 49.65 68.37 48.07 80.40\n\n**gpt-3** 49.21 50.02 51.23 72.72\n\n**llama 2** 49.60 43.99 49.55 20.46\n\n**chatglm** 47.93 44.41 48.57 30.92\n\n**falcon** 39.66 29.08 42.71 18.98\n\n**vicuna** 60.34 46.35 45.62 19.48\n**alpaca** 6.68 17.55 20.63 9.54\n\ntable 5: accuracy (%) of classifying whether a sample\ncontains hallucinated contents.",
      "or normal output ( *e.g.,* an answer) of each sample\nfor classification. the evaluation instructions of\nqa, dialogue, and summarization are presented in\ntable 13, table 14 and table 15 in appendix c.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,"
    ],
    "response": "```markdown\n1.  **Giá trị Kappa Fleiss:**\n    *   Giá trị Kappa Fleiss (κ) được tính trên 5.000 mẫu đã được đánh giá và đạt được κ = 0.811.\n\n2.  **Ý nghĩa của giá trị Kappa:**\n    *   Giá trị này (0.80 ≤ κ ≤ 1.00) cho thấy sự đồng thuận hoàn hảo giữa những người đánh giá.\n```",
    "reference": "Giá trị Kappa Fleiss là 0.811, cho thấy sự đồng thuận hoàn hảo giữa những người đánh giá."
  },
  {
    "user_input": "Tổng cộng có bao nhiêu mẫu được tạo ra cho mỗi nhiệm vụ hỏi đáp, đối thoại và tóm tắt?",
    "retrieved_contexts": [
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "-----\n\n**tasks** **#sample** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 10000 2280 1378 5102 1240\n**dialogue** 10000 8330 1196 474   **summa.** 10000 2614 3562 3824   \ntable 16: number of generated samples for each hallucination pattern (p-i/ii/iii/iv). “‘summa.” is short\nfor summarization. “-” is due to that we consider three\n\npatterns in dialogue and summarization.\n\n6464\n\n\n-----",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a.",
      "-----\n\npc2\n\n\n\n\n\n\n\n\n\n\npolice\n\n\n\n\n\n\n\n\n\n\ncar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|3 8 ce r family animal dog ficer child mother|club|\n|---|---|\n|family pc1|club seaso sterl ci 9in tyg fan7 g2am old player ent|\n|ampaign scho foo4d officialstu1100d ernmen5t6 leader party||\n\n\n\n(a) qa topics\n\n\n(b) dialogue topics\n\n\n(c) summarization topics\n\n\nfigure 2: topic distributions for qa, knowledge-grounded dialogue, and text summarization. the samples of each\ntask are classified into 10 topics, and the red circles denote the topics of failed recognized samples by chatgpt."
    ],
    "response": "```markdown\n1.  **Tổng số mẫu:**\n    *   Tổng cộng có 30.000 mẫu \"hallucination\" được tạo ra.\n    *   Mỗi nhiệm vụ (hỏi đáp, đối thoại, và tóm tắt) có 10.000 ví dụ.\n```",
    "reference": "Có 10,000 mẫu cho mỗi nhiệm vụ hỏi đáp, đối thoại và tóm tắt."
  },
  {
    "user_input": "Trong số các phản hồi được chú thích từ ChatGPT, tỷ lệ phản hồi bị coi là chứa thông tin sai lệch (hallucination) là bao nhiêu?",
    "retrieved_contexts": [
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "**labeler details.** annotating the hallucination\nin chatgpt responses is a very challenging task,\nwhich requires good reading comprehension skills\nand using search engine to look up relevant information for judgement. thus, from an initial pool\nof labeler candidates, we select labelers who are\ngood at english passage reading with at least an\nundergraduate-level education. besides, following\n(ouyang et al., 2022), we have labelers annotate a\nsmall number of test examples and measure their\nagreement with the labels of researchers, and finally\nwe choose thirty human labelers with the highest\nagreement scores. we report fleiss’s kappa ( *κ* ) to\nindicate the reliability of agreement between human labelers. we compute *κ* on 5 *,* 000 annotated\nsamples and obtain *κ* = 0 *.* 811 ( 0 *.* 80 *≤* *κ ≤* 1 *.* 00 )\nshowing a perfect agreement.",
      "table 4: a generated hallucinated qa example and a\nhuman-labeled chatgpt response for a user query.\n\nselection procedure. specifically, we use chatgpt\nto sample three responses for each user query and\ncompute their average semantic similarity using\nbertscore (zhang et al., 2020). we finally retain 5 *,* 000 user queries with the lowest similarities.\naccording to recent work (manakul et al., 2023),\nhallucinations are likely to appear in diverged and\nconflicting responses of llms. for each query and\nchatgpt response, human labelers will annotate\nwhether the response contains hallucinated information (“yes” or “no”) and list the corresponding\n\n\n6452",
      "cording to recent work (manakul et al., 2023), hallucinations are likely to appear in diverged and conflicting responses of llms. based on the filtered\nuser queries and chatgpt responses, we invite\nhuman labelers to annotate whether the response\ncontains hallucinated information and mark cor\nresponding spans. as shown in table 1, for the\nuser query “ *retrieve the oldest photo of a cat* ”, the\nresponse generated by chatgpt contains unverifiable information. these human-annotated queries\nand responses can be used to analyze what types\nof content llms tend to hallucinate and further"
    ],
    "response": "```markdown\n1.  **Tỷ lệ phản hồi chứa thông tin sai lệch:**\n    *   Trong số các phản hồi được chú thích từ ChatGPT, có 977 phản hồi (19.5%) bị coi là chứa thông tin sai lệch (hallucination).\n```",
    "reference": "Khoảng 19.5% phản hồi được đánh dấu là chứa thông tin sai lệch."
  },
  {
    "user_input": "Các nhà nghiên cứu có thể sử dụng benchmark này để làm gì liên quan đến vấn đề 'ảo giác' (hallucination) ở các mô hình ngôn ngữ lớn?",
    "retrieved_contexts": [
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future."
    ],
    "response": "```markdown\n1.  **Mục đích sử dụng Halueval:**\n    *   Các nhà nghiên cứu có thể sử dụng benchmark Halueval để nghiên cứu hoặc giảm thiểu vấn đề \"ảo giác\" (hallucination) ở các mô hình ngôn ngữ lớn (LLMs).\n\n2.  **Các khía cạnh nghiên cứu:**\n    *   Dựa trên các mẫu đã tạo và chú thích, các nhà nghiên cứu có thể sử dụng Halueval để:\n        *   Nghiên cứu những loại nội dung và mức độ mà LLMs có xu hướng tạo ra \"ảo giác\".\n        *   Xây dựng các LLMs hiệu quả và đáng tin cậy hơn trong tương lai.\n```",
    "reference": "Các nhà nghiên cứu có thể sử dụng benchmark này để điều tra hoặc giảm thiểu vấn đề 'ảo giác' ở các mô hình ngôn ngữ lớn."
  },
  {
    "user_input": "Các mẫu đã tạo và chú thích có thể được sử dụng để làm gì trong việc nghiên cứu về LLM?",
    "retrieved_contexts": [
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "llms in three aspects. firstly, based on our generated and annotated samples, researchers can use\nthem to analyze what types of content llms tend\nto generate hallucinations. second, researchers can\nfurther evaluate the ability of llms to recognize\nhallucinations in the generated samples. for example, given a question and an answer, llms can\nbe asked to determine whether the answer contains",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author"
    ],
    "response": "```markdown\n1.  **Phân tích nội dung:**\n    *   Sử dụng các mẫu đã tạo và chú thích để phân tích loại nội dung mà LLMs có xu hướng tạo ra \"ảo giác\".\n\n2.  **Đánh giá khả năng nhận biết \"ảo giác\":**\n    *   Đánh giá khả năng của LLMs trong việc nhận biết \"ảo giác\" trong các mẫu đã tạo. Ví dụ: yêu cầu LLMs xác định xem một câu trả lời có chứa \"ảo giác\" hay không khi đưa ra một câu hỏi và câu trả lời.\n```",
    "reference": "Các nhà nghiên cứu có thể sử dụng chúng để phân tích loại nội dung mà LLM có xu hướng tạo ra ảo giác."
  },
  {
    "user_input": "Ngoài việc tạo ra nội dung, LLM còn có thể làm gì với các mẫu đã tạo?",
    "retrieved_contexts": [
      "llms in three aspects. firstly, based on our generated and annotated samples, researchers can use\nthem to analyze what types of content llms tend\nto generate hallucinations. second, researchers can\nfurther evaluate the ability of llms to recognize\nhallucinations in the generated samples. for example, given a question and an answer, llms can\nbe asked to determine whether the answer contains",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author"
    ],
    "response": "Không có tài liệu nào đề cập đến việc LLM có thể làm gì khác với các mẫu đã tạo ngoài việc tạo ra nội dung.",
    "reference": "LLM có thể được đánh giá khả năng nhận biết các ảo giác trong các mẫu đã tạo."
  },
  {
    "user_input": "Để đánh giá khả năng nhận biết ảo giác của LLM, điều gì cần được cung cấp cho LLM?",
    "retrieved_contexts": [
      "hallucinated content. finally, our benchmark can\nbe further paired with human annotation to assess\nwhether the llms’ output contains hallucinations,\nsince the samples in our benchmark are specially\ndesigned for testing the hallucinations of llms.\nto use our benchmark, users can run the code in\nour project repository to conduct the corresponding\nevaluation and analysis. users can use our provided\ninstructions on their own datasets to evaluate llms\n\non hallucinations.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "**3.2.1** **hallucination recognition**\n\nto evaluate the ability of llms to recognize hallucinations, we randomly select the hallucinated",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "**3.2.2** **improvement strategies**\n\nin this part, we design several strategies to improve\nthe ability of llms to recognize hallucination. the\nresults are shown in table 8.\n\n**knowledge retrieval.** retrieving relevant knowledge is a widely used strategy to eliminate hallucination (lewis et al., 2020; li et al., 2023a). therefore, we supply chatgpt with the knowledge facts\nretrieved from wikipedia (except for that summarization does not need external information besides"
    ],
    "response": "```markdown\n1.  **Đánh giá khả năng nhận biết \"ảo giác\":**\n    *   Để đánh giá khả năng nhận biết \"ảo giác\" của LLM, cần cung cấp cho LLM:\n        *   Một câu hỏi (user query).\n        *   Câu trả lời tương ứng (ChatGPT response).\n        *   Yêu cầu LLM xác định xem câu trả lời có chứa \"ảo giác\" hay không.\n```",
    "reference": "Một câu hỏi và một câu trả lời cần được cung cấp cho LLM để xác định xem câu trả lời có chứa ảo giác hay không."
  },
  {
    "user_input": "Người dùng có thể sử dụng benchmark này bằng cách nào?",
    "retrieved_contexts": [
      "hallucinated content. finally, our benchmark can\nbe further paired with human annotation to assess\nwhether the llms’ output contains hallucinations,\nsince the samples in our benchmark are specially\ndesigned for testing the hallucinations of llms.\nto use our benchmark, users can run the code in\nour project repository to conduct the corresponding\nevaluation and analysis. users can use our provided\ninstructions on their own datasets to evaluate llms\n\non hallucinations.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms."
    ],
    "response": "```markdown\n1.  **Cách sử dụng Halueval Benchmark:**\n    *   Người dùng có thể sử dụng mã nguồn trong kho lưu trữ dự án để tiến hành đánh giá và phân tích tương ứng.\n    *   Người dùng có thể sử dụng các hướng dẫn được cung cấp trên bộ dữ liệu của riêng họ để đánh giá LLM về \"ảo giác\".\n```",
    "reference": "Người dùng có thể chạy mã trong kho lưu trữ dự án để tiến hành đánh giá và phân tích tương ứng."
  },
  {
    "user_input": "Benchmark này dùng để làm gì?",
    "retrieved_contexts": [
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "hallucinated content. finally, our benchmark can\nbe further paired with human annotation to assess\nwhether the llms’ output contains hallucinations,\nsince the samples in our benchmark are specially\ndesigned for testing the hallucinations of llms.\nto use our benchmark, users can run the code in\nour project repository to conduct the corresponding\nevaluation and analysis. users can use our provided\ninstructions on their own datasets to evaluate llms\n\non hallucinations.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **appendix**\n\nwe provide some extra information about our\nbenchmark as supplementary materials. the appendix is organized into three sections:\n\n  - instructions of hallucination sampling are presented in appendix a;\n\n  instructions of hallucination filtering are presented in appendix b;\n\n  - instructions of evaluation are presented in appendix c;\n\n  - details of our benchmark are presented in appendix d.\n## **a hallucination sampling**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 9 and\ntable 10, respectively.\n## **b hallucination filtering**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 11\nand table 12, respectively.\n## **c hallucination recognition**\n\nthe hallucination recognition instructions for qa,\ndialogue and summarization are shown in table 13,\ntable 14 and table 15, respectively.",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms."
    ],
    "response": "```markdown\n1.  **Mục đích của Halueval Benchmark:**\n\n    *   **Hiểu rõ hơn về nội dung và mức độ \"ảo giác\":** Halueval được tạo ra để hiểu rõ hơn về loại nội dung nào và mức độ mà LLM có xu hướng tạo ra \"ảo giác\".\n    *   **Đánh giá hiệu suất nhận biết \"ảo giác\":** Benchmark này dùng để đánh giá hiệu suất của LLM trong việc nhận biết \"ảo giác\".\n    *   **Phân tích điểm mù thông tin:** Halueval được thiết kế để thách thức khả năng nhận biết \"ảo giác\" của LLM và phân tích các điểm mù thông tin của LLM.\n    *   **Hỗ trợ nghiên cứu và phát triển phương pháp giảm thiểu \"ảo giác\":** Halueval có thể được kết hợp với chú thích của con người để đánh giá xem đầu ra của LLM có chứa \"ảo giác\" hay không, từ đó giúp phát triển các phương pháp giảm thiểu \"ảo giác\" hiệu quả.\n```",
    "reference": "Benchmark này được thiết kế đặc biệt để kiểm tra khả năng tạo thông tin sai lệch (hallucinations) của các mô hình ngôn ngữ lớn (LLMs)."
  },
  {
    "user_input": "Tại sao nên kết hợp benchmark này với đánh giá của con người?",
    "retrieved_contexts": [
      "hallucinated content. finally, our benchmark can\nbe further paired with human annotation to assess\nwhether the llms’ output contains hallucinations,\nsince the samples in our benchmark are specially\ndesigned for testing the hallucinations of llms.\nto use our benchmark, users can run the code in\nour project repository to conduct the corresponding\nevaluation and analysis. users can use our provided\ninstructions on their own datasets to evaluate llms\n\non hallucinations.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **appendix**\n\nwe provide some extra information about our\nbenchmark as supplementary materials. the appendix is organized into three sections:\n\n  - instructions of hallucination sampling are presented in appendix a;\n\n  instructions of hallucination filtering are presented in appendix b;\n\n  - instructions of evaluation are presented in appendix c;\n\n  - details of our benchmark are presented in appendix d.\n## **a hallucination sampling**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 9 and\ntable 10, respectively.\n## **b hallucination filtering**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 11\nand table 12, respectively.\n## **c hallucination recognition**\n\nthe hallucination recognition instructions for qa,\ndialogue and summarization are shown in table 13,\ntable 14 and table 15, respectively.",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation."
    ],
    "response": "```markdown\nKhông có tài liệu nào cung cấp thông tin trực tiếp về lý do tại sao nên kết hợp benchmark này với đánh giá của con người.\n```",
    "reference": "Để đánh giá xem kết quả đầu ra của LLM có chứa thông tin sai lệch hay không, việc kết hợp benchmark với đánh giá của con người sẽ giúp tăng độ chính xác."
  },
  {
    "user_input": "Độ chính xác của Claude 2 trong việc phân loại nội dung ảo giác là bao nhiêu?",
    "retrieved_contexts": [
      "44\n\n**claude 2** 69.78 64.73 57.75 75.00\n\n**claude** 67.60 64.83 53.76 73.88\n\n**davinci002** 60.05 60.81 47.77 80.42\n\n**davinci003** 49.65 68.37 48.07 80.40\n\n**gpt-3** 49.21 50.02 51.23 72.72\n\n**llama 2** 49.60 43.99 49.55 20.46\n\n**chatglm** 47.93 44.41 48.57 30.92\n\n**falcon** 39.66 29.08 42.71 18.98\n\n**vicuna** 60.34 46.35 45.62 19.48\n**alpaca** 6.68 17.55 20.63 9.54\n\ntable 5: accuracy (%) of classifying whether a sample\ncontains hallucinated contents.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "hallucinated content. finally, our benchmark can\nbe further paired with human annotation to assess\nwhether the llms’ output contains hallucinations,\nsince the samples in our benchmark are specially\ndesigned for testing the hallucinations of llms.\nto use our benchmark, users can run the code in\nour project repository to conduct the corresponding\nevaluation and analysis. users can use our provided\ninstructions on their own datasets to evaluate llms\n\non hallucinations.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future."
    ],
    "response": "```markdown\n1.  **Độ chính xác của Claude 2:**\n\n    *   Theo tài liệu, Claude 2 có độ chính xác 69.78% trong việc phân loại xem một mẫu có chứa nội dung ảo giác hay không.\n    *   Các độ chính xác khác của Claude 2 bao gồm: 64.73, 57.75, 75.00.\n```",
    "reference": "Độ chính xác của Claude 2 là 69.78 64.73 57.75 75.00."
  },
  {
    "user_input": "Trong số các mô hình được liệt kê, mô hình nào có vẻ kém chính xác nhất trong việc phân loại nội dung ảo giác?",
    "retrieved_contexts": [
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "44\n\n**claude 2** 69.78 64.73 57.75 75.00\n\n**claude** 67.60 64.83 53.76 73.88\n\n**davinci002** 60.05 60.81 47.77 80.42\n\n**davinci003** 49.65 68.37 48.07 80.40\n\n**gpt-3** 49.21 50.02 51.23 72.72\n\n**llama 2** 49.60 43.99 49.55 20.46\n\n**chatglm** 47.93 44.41 48.57 30.92\n\n**falcon** 39.66 29.08 42.71 18.98\n\n**vicuna** 60.34 46.35 45.62 19.48\n**alpaca** 6.68 17.55 20.63 9.54\n\ntable 5: accuracy (%) of classifying whether a sample\ncontains hallucinated contents.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "with respect to the hallucinated samples where\nchatgpt fails to recognize, we present the number\nof each hallucination pattern in table 6. based on\nthe results, we can observe that the hallucination\npatterns of failed samples are unevenly distributed.\nfor example, over half of failures in qa, dialogue,\nand summarization originate from the first hallucination pattern ( *i.e., comprehension*, *extrinsic-soft*,\nand *factual* ), which refers to the hallucinations that\nare factually correct but conflict with the context.\nthis indicates that llms lack or cannot associate"
    ],
    "response": "```markdown\n1.  **Mô hình kém chính xác nhất:**\n\n    *   Dựa trên bảng số liệu được cung cấp, Alpaca có vẻ là mô hình kém chính xác nhất trong việc phân loại nội dung ảo giác.\n    *   Độ chính xác của Alpaca chỉ đạt 6.68%, 17.55%, 20.63% và 9.54% trong các thử nghiệm khác nhau.\n```",
    "reference": "Alpaca có vẻ là mô hình kém chính xác nhất."
  },
  {
    "user_input": "Những mô hình ngôn ngữ lớn (LLMs) tiên tiến đã được đánh giá bằng chuẩn nào?",
    "retrieved_contexts": [
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms."
    ],
    "response": "```markdown\n1.  **Các LLMs tiên tiến được đánh giá bằng Halueval:**    *   **Các LLM mã nguồn đóng:**\n        *   GPT-3 (davinci)\n        *   InstructGPT (text-davinci-002/003)\n        *   ChatGPT (GPT-3.5-turbo)\n        *   Claude\n        *   Claude 2\n    *   **Các LLM mã nguồn mở:**        *   Alpaca (7b)\n        *   Vicuna (7b)\n        *   ChatGLM (7b)\n        *   Falcon (7b)\n        *   Llama 2-chat (7b)\n```",
    "reference": "Chúng được đánh giá bằng chuẩn halueval."
  },
  {
    "user_input": "Các thử nghiệm được thực hiện có sử dụng fine-tuning hay không?",
    "retrieved_contexts": [
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "**cot reasoning.** in previous work (wei et al.,\n2022), chain-of-thought (cot) has been proposed\nto improve the ability of llms to perform reasoning and derive the final answer by introducing a series of intermediate reasoning steps. here, besides\nproducing the recognition result, we also require\nchatgpt to generate the reasoning steps. while,\nfrom the results in table 8, we observe that generating reasoning steps can mildly improve the performance but makes the model perform worse in qa\nand dialogue ( *e.g.,* dropping from 62.59 to 59.58).\ncompared to retrieving knowledge, adding chainof-thought before output might interfere with the\nfinal judgement. while, in text summarization, generating reasoning steps improve the accuracy from\n58.53 to 61.21. the reason might be that the factual\ncontradiction between document and summary can\nbe identified through logic reasoning.\n\n**sample contrast.** we further provide ground-truth"
    ],
    "response": "```markdown\n1.  **Không có fine-tuning hoặc điều chỉnh hyper-parameter:**\n    *   Các thử nghiệm được thực hiện mà không có fine-tuning hoặc điều chỉnh hyper-parameter.\n```",
    "reference": "Không, các thử nghiệm được thực hiện mà không có fine-tuning."
  },
  {
    "user_input": "Có bao nhiêu mô hình ngôn ngữ lớn (LLM) nguồn đóng đã được thử nghiệm trong số các mô hình GPT-3, InstructGPT, ChatGPT, Claude và Claude 2?",
    "retrieved_contexts": [
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "hallucination recognition and analyze the information blind spots of llms.\nto better understand the performance of llms\nin halueval, we conduct experiments with several\nexisting powerful llms ( *e.g.,* chatgpt, gpt-3)."
    ],
    "response": "```markdown\n1.  **Năm mô hình ngôn ngữ lớn (LLM) nguồn đóng đã được thử nghiệm:**\n    *   GPT-3\n    *   InstructGPT\n    *   ChatGPT\n    *   Claude\n    *   Claude 2\n```",
    "reference": "Có năm mô hình ngôn ngữ lớn (LLM) nguồn đóng đã được thử nghiệm."
  },
  {
    "user_input": "API nào được sử dụng để tạo ra các mẫu bịa đặt?",
    "retrieved_contexts": [
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms."
    ],
    "response": "```markdown\n1.  **Azure OpenAI ChatGPT API đã được sử dụng:**\n\n    *   API này được sử dụng để tự động hóa quá trình tạo ra các mẫu bịa đặt.\n```",
    "reference": "Azure OpenAI ChatGPT API được sử dụng để tạo ra các mẫu bịa đặt."
  },
  {
    "user_input": "Số lượng token tối đa được đặt cho quá trình tạo mẫu là bao nhiêu?",
    "retrieved_contexts": [
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "**2.1** **automatic generation**\n\nour generation pipeline includes two steps: 1) diverse hallucination sampling, and 2) high-quality\nhallucination filtering. we employ chatgpt to\nexecute the creation pipeline automatically.\n\n**diverse hallucination sampling.** since a factual\ntext can be hallucinated from different aspects, we\npropose two different hallucination sampling meth\n\n6450",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "44\n\n**claude 2** 69.78 64.73 57.75 75.00\n\n**claude** 67.60 64.83 53.76 73.88\n\n**davinci002** 60.05 60.81 47.77 80.42\n\n**davinci003** 49.65 68.37 48.07 80.40\n\n**gpt-3** 49.21 50.02 51.23 72.72\n\n**llama 2** 49.60 43.99 49.55 20.46\n\n**chatglm** 47.93 44.41 48.57 30.92\n\n**falcon** 39.66 29.08 42.71 18.98\n\n**vicuna** 60.34 46.35 45.62 19.48\n**alpaca** 6.68 17.55 20.63 9.54\n\ntable 5: accuracy (%) of classifying whether a sample\ncontains hallucinated contents.",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language.",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a.",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets."
    ],
    "response": "```markdown\n1.  **Số lượng token tối đa:**\n\n    *   Số lượng token tối đa cho quá trình tạo mẫu được đặt thành 256.\n```",
    "reference": "Số lượng token tối đa được đặt là 256."
  },
  {
    "user_input": "Tại sao nhiệt độ được đặt thành 0 khi đánh giá các mô hình?",
    "retrieved_contexts": [
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "**cot reasoning.** in previous work (wei et al.,\n2022), chain-of-thought (cot) has been proposed\nto improve the ability of llms to perform reasoning and derive the final answer by introducing a series of intermediate reasoning steps. here, besides\nproducing the recognition result, we also require\nchatgpt to generate the reasoning steps. while,\nfrom the results in table 8, we observe that generating reasoning steps can mildly improve the performance but makes the model perform worse in qa\nand dialogue ( *e.g.,* dropping from 62.59 to 59.58).\ncompared to retrieving knowledge, adding chainof-thought before output might interfere with the\nfinal judgement. while, in text summarization, generating reasoning steps improve the accuracy from\n58.53 to 61.21. the reason might be that the factual\ncontradiction between document and summary can\nbe identified through logic reasoning.\n\n**sample contrast.** we further provide ground-truth",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449"
    ],
    "response": "```markdown\n1.  **Lý do đặt nhiệt độ về 0 khi đánh giá:**\n\n    *   Để giảm tính ngẫu nhiên của đầu ra.\n    *   Đảm bảo đầu ra tập trung và xác định hơn.\n```",
    "reference": "Nhiệt độ được đặt thành 0 để giảm tính ngẫu nhiên của kết quả và đảm bảo kết quả tập trung và xác định hơn."
  },
  {
    "user_input": "Mô hình ChatGPT đạt độ chính xác bao nhiêu trong việc phân loại tóm tắt văn bản thực tế và tóm tắt bịa đặt?",
    "retrieved_contexts": [
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "table 7: two hallucinated and refined examples from chatgpt. the **green** text denotes the hallucinated span, and\nthe **brown** text denotes the refined span after augmented with retrieved knowledge.\n\n\n**variants** **qa** **dialogue summarization general**\n\n**chatgpt** 62.59 72.40 58.53 86.22\n\nw/ knowledge 76.83 73.80 - 90.73\nw/ cot 59.58 71.39 61.21 86.50\n\nw/ contrast 49.19 68.67 49.",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language.",
      "**cot reasoning.** in previous work (wei et al.,\n2022), chain-of-thought (cot) has been proposed\nto improve the ability of llms to perform reasoning and derive the final answer by introducing a series of intermediate reasoning steps. here, besides\nproducing the recognition result, we also require\nchatgpt to generate the reasoning steps. while,\nfrom the results in table 8, we observe that generating reasoning steps can mildly improve the performance but makes the model perform worse in qa\nand dialogue ( *e.g.,* dropping from 62.59 to 59.58).\ncompared to retrieving knowledge, adding chainof-thought before output might interfere with the\nfinal judgement. while, in text summarization, generating reasoning steps improve the accuracy from\n58.53 to 61.21. the reason might be that the factual\ncontradiction between document and summary can\nbe identified through logic reasoning.\n\n**sample contrast.** we further provide ground-truth",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "44\n\n**claude 2** 69.78 64.73 57.75 75.00\n\n**claude** 67.60 64.83 53.76 73.88\n\n**davinci002** 60.05 60.81 47.77 80.42\n\n**davinci003** 49.65 68.37 48.07 80.40\n\n**gpt-3** 49.21 50.02 51.23 72.72\n\n**llama 2** 49.60 43.99 49.55 20.46\n\n**chatglm** 47.93 44.41 48.57 30.92\n\n**falcon** 39.66 29.08 42.71 18.98\n\n**vicuna** 60.34 46.35 45.62 19.48\n**alpaca** 6.68 17.55 20.63 9.54\n\ntable 5: accuracy (%) of classifying whether a sample\ncontains hallucinated contents."
    ],
    "response": "```markdown\n1.  **Độ chính xác của ChatGPT trong việc phân loại tóm tắt văn bản:**\n\n    *   ChatGPT chỉ đạt độ chính xác 58,53% trong việc phân loại tóm tắt văn bản thực tế và bịa đặt.\n    *   Điều này cho thấy ChatGPT gặp khó khăn trong việc phân biệt giữa các tóm tắt có căn cứ và các tóm tắt bịa đặt.\n```",
    "reference": "Mô hình ChatGPT đạt độ chính xác 58.53% trong việc phân loại tóm tắt văn bản thực tế và tóm tắt bịa đặt."
  },
  {
    "user_input": "So với GPT-3, độ chính xác của Alpaca và Vicuna trong các tác vụ tương tự như thế nào?",
    "retrieved_contexts": [
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "44\n\n**claude 2** 69.78 64.73 57.75 75.00\n\n**claude** 67.60 64.83 53.76 73.88\n\n**davinci002** 60.05 60.81 47.77 80.42\n\n**davinci003** 49.65 68.37 48.07 80.40\n\n**gpt-3** 49.21 50.02 51.23 72.72\n\n**llama 2** 49.60 43.99 49.55 20.46\n\n**chatglm** 47.93 44.41 48.57 30.92\n\n**falcon** 39.66 29.08 42.71 18.98\n\n**vicuna** 60.34 46.35 45.62 19.48\n**alpaca** 6.68 17.55 20.63 9.54\n\ntable 5: accuracy (%) of classifying whether a sample\ncontains hallucinated contents.",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters.",
      "hallucination recognition and analyze the information blind spots of llms.\nto better understand the performance of llms\nin halueval, we conduct experiments with several\nexisting powerful llms ( *e.g.,* chatgpt, gpt-3).",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449"
    ],
    "response": "```markdown\n1.  **So sánh độ chính xác của Alpaca, Vicuna và GPT-3:**\n\n    *   GPT-3 đạt độ chính xác khoảng 50% trong ba tác vụ.\n    *   Alpaca và Vicuna có hiệu suất kém hơn, thậm chí dưới mức ngẫu nhiên.\n    *   Điều này cho thấy Alpaca và Vicuna gặp nhiều khó khăn hơn so với GPT-3 trong việc phân biệt nội dung thật và nội dung bịa đặt.\n```",
    "reference": "Alpaca và Vicuna có độ chính xác thấp hơn so với GPT-3 trong các tác vụ tương tự."
  },
  {
    "user_input": "Những phương pháp nào có thể giúp cải thiện khả năng của các mô hình ngôn ngữ lớn (LLM) trong việc xác định thông tin bịa đặt trong văn bản?",
    "retrieved_contexts": [
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "**3.2.2** **improvement strategies**\n\nin this part, we design several strategies to improve\nthe ability of llms to recognize hallucination. the\nresults are shown in table 8.\n\n**knowledge retrieval.** retrieving relevant knowledge is a widely used strategy to eliminate hallucination (lewis et al., 2020; li et al., 2023a). therefore, we supply chatgpt with the knowledge facts\nretrieved from wikipedia (except for that summarization does not need external information besides",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "llms in three aspects. firstly, based on our generated and annotated samples, researchers can use\nthem to analyze what types of content llms tend\nto generate hallucinations. second, researchers can\nfurther evaluate the ability of llms to recognize\nhallucinations in the generated samples. for example, given a question and an answer, llms can\nbe asked to determine whether the answer contains"
    ],
    "response": "```markdown\n1.  **Các phương pháp cải thiện khả năng nhận biết thông tin bịa đặt của LLM:**\n\n    *   **Sử dụng truy xuất kiến thức:** Cung cấp cho LLM quyền truy cập vào các nguồn kiến thức bên ngoài như Wikipedia có thể giúp chúng căn cứ các phản hồi của mình vào các sự kiện có thể kiểm chứng.\n    *   **Thêm các bước suy luận trung gian:** Khuyến khích LLM chia nhỏ quy trình suy luận của chúng thành các bước nhỏ hơn, dễ quản lý hơn có thể giúp chúng xác định và sửa lỗi.\n    *   **Điều chỉnh hướng dẫn và liên kết với con người:** Tinh chỉnh LLM bằng phản hồi của con người và cẩn thận soạn thảo các hướng dẫn có thể cải thiện khả năng phân biệt giữa thông tin thực tế và thông tin bịa đặt của chúng.\n```",
    "reference": "Việc điều chỉnh hướng dẫn (instruction tuning) và điều chỉnh phù hợp với con người (alignment with humans) có thể giúp cải thiện khả năng của các mô hình ngôn ngữ lớn trong việc xác định thông tin bịa đặt trong văn bản."
  },
  {
    "user_input": "Phần lớn lỗi trong QA, hội thoại và tóm tắt xuất phát từ loại ảo giác nào?",
    "retrieved_contexts": [
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "table 13: instruction of hallucination recognition for question answering.\n\ni want you act as a response judge. given a dialogue history and a response, your objective is to determine if the\nprovided response contains non-factual or hallucinated information. you should give your judgement based\non the following hallucination types and the world knowledge.\n\nyou are trying to determine if the true entity in the response is replaced with a dissimilar entity.\n\n<demonstrations>\n\nyou should try your best to determine if the response contains non-factual or hallucinated information according\nto the above hallucination types. the answer you give must be “yes” or “no”.\n\n**#dialogue history#:** <here is the dialogue history>\n**#response#:** <here is the hallucinated response or right response>\n**#your judgement#:**\n\ntable 14: instruction of hallucination recognition for knowledge-grounded dialogue.\n\n6462",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets.",
      "with respect to the hallucinated samples where\nchatgpt fails to recognize, we present the number\nof each hallucination pattern in table 6. based on\nthe results, we can observe that the hallucination\npatterns of failed samples are unevenly distributed.\nfor example, over half of failures in qa, dialogue,\nand summarization originate from the first hallucination pattern ( *i.e., comprehension*, *extrinsic-soft*,\nand *factual* ), which refers to the hallucinations that\nare factually correct but conflict with the context.\nthis indicates that llms lack or cannot associate",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "## **4 related work**\n\n**hallucination in llms.** hallucination in llms is\n\nconcerning since it hinders performance and raises\nsafety risks in real-world application. to alleviate\nthis issue, prior studies have proposed to use a verification system to identify non-factual entities in\ntext summarization (zhao et al., 2020), invoke interfaces of structured data ( *e.g.,* knowledge graph,\ndatabase) to obtain related evidence (jiang et al.,\n2023; lan et al., 2022), and train a token-level fact\ncritic to recognize hallucination and rectify them\nin dialogue (dziri et al., 2021). to enhance the\nunderstanding of hallucination in llms and pro\n\n6455",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,"
    ],
    "response": "```markdown\n1.  **Các loại ảo giác gây ra phần lớn lỗi:**\n\n    *   Phần lớn lỗi trong QA (Trả lời câu hỏi), hội thoại và tóm tắt xuất phát từ các loại ảo giác sau:\n        *   **QA:** Comprehension (Hiểu).\n        *   **Hội thoại:** Extrinsic-soft.\n        *   **Tóm tắt:** Factual (Dựa trên thực tế).\n    *   Những loại ảo giác này thường đúng về mặt thực tế nhưng lại mâu thuẫn với ngữ cảnh.\n    *   Điều này cho thấy LLM thiếu khả năng liên kết thông tin hoặc không thể liên kết thông tin một cách chính xác.\n```",
    "reference": "Phần lớn lỗi trong QA, hội thoại và tóm tắt xuất phát từ loại ảo giác đầu tiên."
  },
  {
    "user_input": "Sự phân bố không đồng đều của các loại ảo giác trong các mẫu lỗi cho thấy điều gì?",
    "retrieved_contexts": [
      "with respect to the hallucinated samples where\nchatgpt fails to recognize, we present the number\nof each hallucination pattern in table 6. based on\nthe results, we can observe that the hallucination\npatterns of failed samples are unevenly distributed.\nfor example, over half of failures in qa, dialogue,\nand summarization originate from the first hallucination pattern ( *i.e., comprehension*, *extrinsic-soft*,\nand *factual* ), which refers to the hallucinations that\nare factually correct but conflict with the context.\nthis indicates that llms lack or cannot associate",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "44\n\n**claude 2** 69.78 64.73 57.75 75.00\n\n**claude** 67.60 64.83 53.76 73.88\n\n**davinci002** 60.05 60.81 47.77 80.42\n\n**davinci003** 49.65 68.37 48.07 80.40\n\n**gpt-3** 49.21 50.02 51.23 72.72\n\n**llama 2** 49.60 43.99 49.55 20.46\n\n**chatglm** 47.93 44.41 48.57 30.92\n\n**falcon** 39.66 29.08 42.71 18.98\n\n**vicuna** 60.34 46.35 45.62 19.48\n**alpaca** 6.68 17.55 20.63 9.54\n\ntable 5: accuracy (%) of classifying whether a sample\ncontains hallucinated contents.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "-----\n\n**tasks** **#sample** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 10000 2280 1378 5102 1240\n**dialogue** 10000 8330 1196 474   **summa.** 10000 2614 3562 3824   \ntable 16: number of generated samples for each hallucination pattern (p-i/ii/iii/iv). “‘summa.” is short\nfor summarization. “-” is due to that we consider three\n\npatterns in dialogue and summarization.\n\n6464\n\n\n-----",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future."
    ],
    "response": "```markdown\n1.  **Sự phân bố không đồng đều của các loại ảo giác:**\n\n    *   Sự phân bố không đồng đều của các loại ảo giác trong các mẫu lỗi (failed samples) cho thấy rằng LLM (Large Language Models) gặp khó khăn khác nhau trong việc xử lý các loại ảo giác khác nhau.\n\n    *   Ví dụ, phần lớn các lỗi trong QA, hội thoại và tóm tắt xuất phát từ các mẫu ảo giác cụ thể (Comprehension, Extrinsic-soft và Factual), cho thấy LLM có thể thiếu khả năng liên kết thông tin hoặc không thể liên kết thông tin một cách chính xác trong các tình huống này.\n```",
    "reference": "Điều này cho thấy các LLM thiếu hoặc không thể liên kết ngữ cảnh."
  },
  {
    "user_input": "Mô hình ảo giác đầu tiên đề cập đến loại ảo giác nào?",
    "retrieved_contexts": [
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "with respect to the hallucinated samples where\nchatgpt fails to recognize, we present the number\nof each hallucination pattern in table 6. based on\nthe results, we can observe that the hallucination\npatterns of failed samples are unevenly distributed.\nfor example, over half of failures in qa, dialogue,\nand summarization originate from the first hallucination pattern ( *i.e., comprehension*, *extrinsic-soft*,\nand *factual* ), which refers to the hallucinations that\nare factually correct but conflict with the context.\nthis indicates that llms lack or cannot associate",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "**instruction design.** in our approach, the key is\nto design an effective instruction for chatgpt to\ngenerate hallucinated samples. in our design, the\nhallucination sampling instruction consists of three\nimportant parts, including *intention description*,\n*hallucination pattern*, and *hallucination demon-*\n*stration*, which have been shown in table 2. the\nintention description is to characterize the role of\nthe system and define the input and objective of\nour generation. to control the type and quality of",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,"
    ],
    "response": "```markdown\n1.  **Mô hình ảo giác đầu tiên:**\n\n    *   Mô hình ảo giác đầu tiên được đề cập (trong QA, hội thoại và tóm tắt) đề cập đến các loại ảo giác sau:\n        *   **QA:** Comprehension (Hiểu).\n        *   **Hội thoại:** Extrinsic-soft.\n        *   **Tóm tắt:** Factual (Dựa trên thực tế).\n    *   Đây là những ảo giác đúng về mặt thực tế nhưng mâu thuẫn với ngữ cảnh.\n```",
    "reference": "Mô hình ảo giác đầu tiên đề cập đến các ảo giác đúng về mặt thực tế nhưng lại mâu thuẫn với ngữ cảnh."
  },
  {
    "user_input": "Các mẫu tác vụ được phân cụm thành bao nhiêu chủ đề?",
    "retrieved_contexts": [
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language.",
      "-----\n\npc2\n\n\n\n\n\n\n\n\n\n\npolice\n\n\n\n\n\n\n\n\n\n\ncar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|3 8 ce r family animal dog ficer child mother|club|\n|---|---|\n|family pc1|club seaso sterl ci 9in tyg fan7 g2am old player ent|\n|ampaign scho foo4d officialstu1100d ernmen5t6 leader party||\n\n\n\n(a) qa topics\n\n\n(b) dialogue topics\n\n\n(c) summarization topics\n\n\nfigure 2: topic distributions for qa, knowledge-grounded dialogue, and text summarization. the samples of each\ntask are classified into 10 topics, and the red circles denote the topics of failed recognized samples by chatgpt.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "-----\n\n**tasks** **#sample** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 10000 2280 1378 5102 1240\n**dialogue** 10000 8330 1196 474   **summa.** 10000 2614 3562 3824   \ntable 16: number of generated samples for each hallucination pattern (p-i/ii/iii/iv). “‘summa.” is short\nfor summarization. “-” is due to that we consider three\n\npatterns in dialogue and summarization.\n\n6464\n\n\n-----"
    ],
    "response": "```markdown\n1.  **Phân cụm chủ đề:**\n\n    *   Các mẫu tác vụ (task samples) được phân loại thành 10 chủ đề.```",
    "reference": "Các mẫu tác vụ được phân cụm thành mười chủ đề."
  },
  {
    "user_input": "ChatGPT gặp khó khăn trong những lĩnh vực chính nào?",
    "retrieved_contexts": [
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "ods to generate diverse samples. for each method,\nchatgpt follows the instruction of hallucination\n\nsampling in different manners. as shown in figure 1, the first method adopts a *one-pass* instruction following schema, where we directly feed the\ncomplete instruction (table 2) into chatgpt and\ngenerate a hallucinated answer. on the other hand,\nthe second method uses a *conversational* schema,\nwhere we teach chatgpt to successively learn part\nof the instruction and make sure it has mastered.\n\nbased on the learned instructions, chatgpt will\ngenerate another hallucinated answer. through the\ntwo different sampling strategies, we can obtain\ndiverse and multi-facet hallucinated answers for\neach question, which will be further filtered and\nselected for the most plausible and difficult one.",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) ."
    ],
    "response": "```markdown\n1.  **Các lĩnh vực ChatGPT gặp khó khăn:**\n\n    *   ChatGPT gặp khó khăn trong các chủ đề về công nghệ, khí hậu và ngôn ngữ.\n```",
    "reference": "ChatGPT gặp khó khăn trong các lĩnh vực công nghệ, khí hậu và ngôn ngữ."
  },
  {
    "user_input": "Trong bộ dữ liệu QA, ChatGPT không nhận ra bao nhiêu mẫu cho kiểu ảo giác p-iii?",
    "retrieved_contexts": [
      "-----\n\n**tasks** **#failed** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 3109 1559 245 278 1027\n**dialogue** 891 465 344 82  **summarization** 3868 3106 705 57  \ntable 6: number of samples where chatgpt fails to\nrecognize for each hallucination pattern (p-i/ii/iii/iv).",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from"
    ],
    "response": "```markdown\n1.  **Số lượng mẫu QA mà ChatGPT không nhận ra cho kiểu ảo giác p-iii:**\n\n    *   Trong bộ dữ liệu QA, ChatGPT không nhận ra 1027 mẫu cho kiểu ảo giác p-iii.\n```",
    "reference": "1027"
  },
  {
    "user_input": "Đối với bộ dữ liệu hội thoại (dialogue), ChatGPT thất bại trong việc nhận dạng kiểu ảo giác p-ii bao nhiêu lần?",
    "retrieved_contexts": [
      "-----\n\n**tasks** **#failed** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 3109 1559 245 278 1027\n**dialogue** 891 465 344 82  **summarization** 3868 3106 705 57  \ntable 6: number of samples where chatgpt fails to\nrecognize for each hallucination pattern (p-i/ii/iii/iv).",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "with respect to the hallucinated samples where\nchatgpt fails to recognize, we present the number\nof each hallucination pattern in table 6. based on\nthe results, we can observe that the hallucination\npatterns of failed samples are unevenly distributed.\nfor example, over half of failures in qa, dialogue,\nand summarization originate from the first hallucination pattern ( *i.e., comprehension*, *extrinsic-soft*,\nand *factual* ), which refers to the hallucinations that\nare factually correct but conflict with the context.\nthis indicates that llms lack or cannot associate"
    ],
    "response": "```markdown\n1.  **Số lần ChatGPT thất bại trong việc nhận dạng kiểu ảo giác p-ii trong bộ dữ liệu hội thoại (dialogue):**\n\n    *   ChatGPT thất bại 82 lần trong việc nhận dạng kiểu ảo giác p-ii trong bộ dữ liệu hội thoại (dialogue).\n```",
    "reference": "82"
  },
  {
    "user_input": "Trong các tác vụ tóm tắt (summarization), ChatGPT thất bại bao nhiêu lần so với tổng số tác vụ?",
    "retrieved_contexts": [
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "-----\n\n**tasks** **#failed** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 3109 1559 245 278 1027\n**dialogue** 891 465 344 82  **summarization** 3868 3106 705 57  \ntable 6: number of samples where chatgpt fails to\nrecognize for each hallucination pattern (p-i/ii/iii/iv).",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "table 7: two hallucinated and refined examples from chatgpt. the **green** text denotes the hallucinated span, and\nthe **brown** text denotes the refined span after augmented with retrieved knowledge.\n\n\n**variants** **qa** **dialogue summarization general**\n\n**chatgpt** 62.59 72.40 58.53 86.22\n\nw/ knowledge 76.83 73.80 - 90.73\nw/ cot 59.58 71.39 61.21 86.50\n\nw/ contrast 49.19 68.67 49."
    ],
    "response": "```markdown\n1.  **Số lần ChatGPT thất bại trong việc nhận dạng các kiểu ảo giác trong tác vụ tóm tắt (summarization):**\n\n    *   Tổng số mẫu trong tác vụ tóm tắt (summarization): 3868.\n    *   Số lần ChatGPT thất bại trong việc nhận dạng các kiểu ảo giác (p-i/ii/iii/iv): 3106.\n```",
    "reference": "3106 trên tổng số 3868 tác vụ."
  },
  {
    "user_input": "Chiến lược nào thường được sử dụng để giảm thiểu hiện tượng ảo giác trong các mô hình ngôn ngữ lớn?",
    "retrieved_contexts": [
      "**3.2.2** **improvement strategies**\n\nin this part, we design several strategies to improve\nthe ability of llms to recognize hallucination. the\nresults are shown in table 8.\n\n**knowledge retrieval.** retrieving relevant knowledge is a widely used strategy to eliminate hallucination (lewis et al., 2020; li et al., 2023a). therefore, we supply chatgpt with the knowledge facts\nretrieved from wikipedia (except for that summarization does not need external information besides",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449"
    ],
    "response": "```markdown\n1.  **Các chiến lược cải thiện khả năng nhận biết ảo giác của LLM:**\n\n    *   **Truy xuất tri thức (Knowledge retrieval):** Cung cấp cho ChatGPT các dữ kiện tri thức được truy xuất từ Wikipedia.\n```",
    "reference": "Lấy thông tin liên quan là một chiến lược phổ biến để loại bỏ ảo giác."
  },
  {
    "user_input": "Wikipedia được sử dụng để cung cấp thông tin cho ChatGPT. Điều này ngụ ý gì về độ tin cậy của Wikipedia?",
    "retrieved_contexts": [
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "chatgpt cannot access the web to obtain the official data. after providing official information\nretrieved from world bank, the refined span displays answers that contain the correct information.\nthe above two examples illustrate that retrieving\nknowledge related to queries can help chatgpt significantly reduce the hallucinations in the response,\nespecially those factual errors.",
      "**3.2.2** **improvement strategies**\n\nin this part, we design several strategies to improve\nthe ability of llms to recognize hallucination. the\nresults are shown in table 8.\n\n**knowledge retrieval.** retrieving relevant knowledge is a widely used strategy to eliminate hallucination (lewis et al., 2020; li et al., 2023a). therefore, we supply chatgpt with the knowledge facts\nretrieved from wikipedia (except for that summarization does not need external information besides",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "**labeler details.** annotating the hallucination\nin chatgpt responses is a very challenging task,\nwhich requires good reading comprehension skills\nand using search engine to look up relevant information for judgement. thus, from an initial pool\nof labeler candidates, we select labelers who are\ngood at english passage reading with at least an\nundergraduate-level education. besides, following\n(ouyang et al., 2022), we have labelers annotate a\nsmall number of test examples and measure their\nagreement with the labels of researchers, and finally\nwe choose thirty human labelers with the highest\nagreement scores. we report fleiss’s kappa ( *κ* ) to\nindicate the reliability of agreement between human labelers. we compute *κ* on 5 *,* 000 annotated\nsamples and obtain *κ* = 0 *.* 811 ( 0 *.* 80 *≤* *κ ≤* 1 *.* 00 )\nshowing a perfect agreement."
    ],
    "response": "Không có tài liệu nào đề cập trực tiếp đến độ tin cậy của Wikipedia.\n```",
    "reference": "Điều này ngụ ý rằng Wikipedia được coi là một nguồn thông tin đáng tin cậy."
  },
  {
    "user_input": "Việc cung cấp kiến thức ảnh hưởng như thế nào đến độ chính xác nhận dạng của ChatGPT trong các tác vụ QA, và cụ thể là tăng từ bao nhiêu lên bao nhiêu?",
    "retrieved_contexts": [
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "chatgpt cannot access the web to obtain the official data. after providing official information\nretrieved from world bank, the refined span displays answers that contain the correct information.\nthe above two examples illustrate that retrieving\nknowledge related to queries can help chatgpt significantly reduce the hallucinations in the response,\nespecially those factual errors.",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,"
    ],
    "response": "Việc cung cấp kiến thức giúp tăng đáng kể độ chính xác nhận dạng của ChatGPT trong các tác vụ QA, ví dụ: tăng từ 62,59% lên 76,83%.\n\n1.  **Cải thiện độ chính xác:**\n\n    *   Khi được cung cấp kiến thức, độ chính xác nhận dạng của ChatGPT tăng lên đáng kể.\n    *   Ví dụ cụ thể: Trong tác vụ QA (Question Answering), độ chính xác tăng từ 62,59% lên 76,83%.\n2.  **Tác động khác nhau trên các tác vụ:**\n\n    *   Mức độ cải thiện khác nhau giữa các tác vụ.\n    *   Trong khi QA có sự cải thiện đáng kể, thì đối thoại chỉ cải thiện nhẹ.\n3.  **Nguyên nhân của sự khác biệt:**\n\n    *   Các mẫu ảo giác phổ biến trong đối thoại (ví dụ: extrinsicsoft/hard) có thể không dễ dàng được xác định chỉ bằng cách kết hợp kiến thức bên ngoài.\n```",
    "reference": "Độ chính xác nhận dạng của ChatGPT tăng đáng kể, ví dụ như tăng từ 62.59% lên 76.83% trong QA."
  },
  {
    "user_input": "Việc trang bị cho các mô hình ngôn ngữ lớn (LLMs) kiến thức bên ngoài có tác dụng gì đối với khả năng nhận biết thông tin sai lệch?",
    "retrieved_contexts": [
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance."
    ],
    "response": "Việc trang bị cho các mô hình ngôn ngữ lớn (LLMs) kiến thức bên ngoài có thể tăng cường đáng kể khả năng nhận biết thông tin sai lệch (hallucinations).\n\n1.  **Tăng cường khả năng nhận biết:**\n\n    *   Trang bị cho LLMs kiến thức bên ngoài có thể giúp chúng nhận biết thông tin sai lệch tốt hơn.\n    *   Điều này đặc biệt đúng với các truy vấn chung và phản hồi từ ChatGPT.\n2.  **Kết quả nghiên cứu:**\n\n    *   Các thí nghiệm đã chứng minh rằng việc cung cấp kiến thức rõ ràng và thêm các bước suy luận trung gian có thể cải thiện hiệu suất của LLMs trong việc nhận biết thông tin sai lệch.\n```",
    "reference": "Việc trang bị cho LLMs kiến thức bên ngoài có thể giúp tăng cường khả năng nhận biết thông tin sai lệch."
  },
  {
    "user_input": "Mục đích của việc sử dụng Chain-of-Thought (CoT) là gì?",
    "retrieved_contexts": [
      "**cot reasoning.** in previous work (wei et al.,\n2022), chain-of-thought (cot) has been proposed\nto improve the ability of llms to perform reasoning and derive the final answer by introducing a series of intermediate reasoning steps. here, besides\nproducing the recognition result, we also require\nchatgpt to generate the reasoning steps. while,\nfrom the results in table 8, we observe that generating reasoning steps can mildly improve the performance but makes the model perform worse in qa\nand dialogue ( *e.g.,* dropping from 62.59 to 59.58).\ncompared to retrieving knowledge, adding chainof-thought before output might interfere with the\nfinal judgement. while, in text summarization, generating reasoning steps improve the accuracy from\n58.53 to 61.21. the reason might be that the factual\ncontradiction between document and summary can\nbe identified through logic reasoning.\n\n**sample contrast.** we further provide ground-truth",
      "-----\n\ni want you act as a response judge. given a dialogue history, two responses, and related knowledge, your\nobjective is to select the best and correct response without hallucination and non-factual information. here are\nsome examples:\n\n...\n\n<demonstrations>\n\n...\n\nyou should try your best to select the best and correct response. if the two responses are the same, you can\nrandomly choose one. if both responses are incorrect, choose the better one. you must select a response from\nthe provided two responses.\n\n**#knowledge#:** <here is the related knowledge>\n**#dialogue history#:** <here is the dialogue history>\n**#response 1#:** <here is the hallucinated response generated by the first channel>\n**#response 2#:** <here is the hallucinated response generated by the second channel>\n**#your choice#:**",
      "-----\n\ni want you act as an assistant in a conversation with human. given a dialogue history, the true response, and\nrelated knowledge, your objective is to write a hallucinated response that sounds plausible but is factually\nincorrect. you should write the hallucinated response using the following method (each with some examples):\n\nor\nyou are trying to write a response to human but you replace the true entity with a dissimilar entity.\n\n<demonstrations>\n\nor\n\nyou should try your best to make the response become hallucinated.\n\n**#knowledge#:** <here is the related knowledge>\n**#dialogue history#:** <here is the dialogue history>\n**#true response#:** <here is the true response of the dialogue history>\n**#hallucinated response#:**",
      "table 11: instruction of hallucination filtering for knowledge-grounded dialogue.\n\ni want you act as a summary judge. given a document and two summaries, your objective is to select the best\nand correct summary without hallucination and non-factual information. here are some examples:\n\n...\n\n<demonstrations>\n\n...\n\nyou should try your best to select the best and correct summary. if both summaries are incorrect, choose the\nbetter one. you must select a summary from the provided two summaries.\n\n**#document#:** <here is the test document>\n**#summary 1#:** <here is the hallucinated summary generated by the first channel>\n**#summary 2#:** <here is the hallucinated summary generated by the second channel>\n**#your choice#:**\n\ntable 12: instruction of hallucination filtering for text summarization.\n\n6461",
      "-----\n\ni want you act as an answer judge. given a question and an answer, your objective is to determine if the\nprovided answer contains non-factual or hallucinated information. you should give your judgement based\non the following hallucination types and the world knowledge.\n\nyou are trying to determine if the answer misunderstands the question context and intention.\n\n<demonstrations>\n\nyou are trying to determine if the answer cannot be inferred from the knowledge correctly.\n\n<demonstrations>\n\nyou should try your best to determine if the answer contains non-factual or hallucinated information according\nto the above hallucination types. the answer you give must be “yes” or “no”.\n\n**#question#:** <here is the test question>\n**#answer#:** <here is the hallucinated answer or right answer>\n**#your judgement#:**",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449"
    ],
    "response": "Mục đích của việc sử dụng Chain-of-Thought (CoT) là cải thiện khả năng suy luận của các mô hình ngôn ngữ lớn (LLMs) bằng cách đưa ra một loạt các bước suy luận trung gian để đạt được câu trả lời cuối cùng.\n\n1.  **Cải thiện khả năng suy luận:**\n\n    *   CoT được đề xuất để cải thiện khả năng suy luận của LLMs.\n    *   Bằng cách giới thiệu một loạt các bước suy luận trung gian.\n2.  **Kết quả thực tế:**\n\n    *   Việc tạo ra các bước suy luận có thể cải thiện nhẹ hiệu suất.\n    *   Tuy nhiên, nó có thể làm cho mô hình hoạt động kém hơn trong QA và đối thoại.\n    *   Trong tóm tắt văn bản, việc tạo ra các bước suy luận cải thiện độ chính xác.\n```",
    "reference": "Chain-of-Thought (CoT) được đề xuất để cải thiện khả năng suy luận của các mô hình ngôn ngữ lớn (LLM) và đưa ra câu trả lời cuối cùng bằng cách giới thiệu một loạt các bước suy luận trung gian."
  },
  {
    "user_input": "Việc tạo ra các bước suy luận ảnh hưởng như thế nào đến hiệu suất của mô hình trong các tác vụ trả lời câu hỏi và đối thoại?",
    "retrieved_contexts": [
      "**cot reasoning.** in previous work (wei et al.,\n2022), chain-of-thought (cot) has been proposed\nto improve the ability of llms to perform reasoning and derive the final answer by introducing a series of intermediate reasoning steps. here, besides\nproducing the recognition result, we also require\nchatgpt to generate the reasoning steps. while,\nfrom the results in table 8, we observe that generating reasoning steps can mildly improve the performance but makes the model perform worse in qa\nand dialogue ( *e.g.,* dropping from 62.59 to 59.58).\ncompared to retrieving knowledge, adding chainof-thought before output might interfere with the\nfinal judgement. while, in text summarization, generating reasoning steps improve the accuracy from\n58.53 to 61.21. the reason might be that the factual\ncontradiction between document and summary can\nbe identified through logic reasoning.\n\n**sample contrast.** we further provide ground-truth",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "or normal output ( *e.g.,* an answer) of each sample\nfor classification. the evaluation instructions of\nqa, dialogue, and summarization are presented in\ntable 13, table 14 and table 15 in appendix c.",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations."
    ],
    "response": "Việc tạo ra các bước suy luận có thể ảnh hưởng tiêu cực đến hiệu suất của mô hình trong các tác vụ trả lời câu hỏi (QA) và đối thoại.\n\n1.  **Ảnh hưởng tiêu cực:**\n\n    *   Theo tài liệu, việc tạo ra các bước suy luận có thể làm cho mô hình hoạt động kém hơn trong QA và đối thoại.\n    *   Ví dụ, hiệu suất có thể giảm từ 62.59% xuống 59.58%.\n2.  **Nguyên nhân có thể:**\n\n    *   Việc thêm Chain-of-Thought (CoT) trước khi đưa ra kết quả cuối cùng có thể gây trở ngại cho việc đánh giá cuối cùng.\n```",
    "reference": "Việc tạo ra các bước suy luận có thể làm giảm hiệu suất của mô hình trong các tác vụ trả lời câu hỏi và đối thoại."
  },
  {
    "user_input": "Tại sao việc tạo ra các bước suy luận lại cải thiện độ chính xác trong tóm tắt văn bản, nhưng không cải thiện trong trả lời câu hỏi?",
    "retrieved_contexts": [
      "**cot reasoning.** in previous work (wei et al.,\n2022), chain-of-thought (cot) has been proposed\nto improve the ability of llms to perform reasoning and derive the final answer by introducing a series of intermediate reasoning steps. here, besides\nproducing the recognition result, we also require\nchatgpt to generate the reasoning steps. while,\nfrom the results in table 8, we observe that generating reasoning steps can mildly improve the performance but makes the model perform worse in qa\nand dialogue ( *e.g.,* dropping from 62.59 to 59.58).\ncompared to retrieving knowledge, adding chainof-thought before output might interfere with the\nfinal judgement. while, in text summarization, generating reasoning steps improve the accuracy from\n58.53 to 61.21. the reason might be that the factual\ncontradiction between document and summary can\nbe identified through logic reasoning.\n\n**sample contrast.** we further provide ground-truth",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "-----\n\npc2\n\n\n\n\n\n\n\n\n\n\npolice\n\n\n\n\n\n\n\n\n\n\ncar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|3 8 ce r family animal dog ficer child mother|club|\n|---|---|\n|family pc1|club seaso sterl ci 9in tyg fan7 g2am old player ent|\n|ampaign scho foo4d officialstu1100d ernmen5t6 leader party||\n\n\n\n(a) qa topics\n\n\n(b) dialogue topics\n\n\n(c) summarization topics\n\n\nfigure 2: topic distributions for qa, knowledge-grounded dialogue, and text summarization. the samples of each\ntask are classified into 10 topics, and the red circles denote the topics of failed recognized samples by chatgpt.",
      "table 7: two hallucinated and refined examples from chatgpt. the **green** text denotes the hallucinated span, and\nthe **brown** text denotes the refined span after augmented with retrieved knowledge.\n\n\n**variants** **qa** **dialogue summarization general**\n\n**chatgpt** 62.59 72.40 58.53 86.22\n\nw/ knowledge 76.83 73.80 - 90.73\nw/ cot 59.58 71.39 61.21 86.50\n\nw/ contrast 49.19 68.67 49."
    ],
    "response": "Việc tạo ra các bước suy luận cải thiện độ chính xác trong tóm tắt văn bản nhưng không cải thiện trong trả lời câu hỏi có thể là do:\n\n1.  **Tóm tắt văn bản:**\n\n    *   Sự mâu thuẫn thực tế giữa tài liệu và bản tóm tắt có thể được xác định thông qua suy luận logic.\n2.  **Trả lời câu hỏi:**\n\n    *   Việc thêm Chain-of-Thought (CoT) trước khi đưa ra kết quả cuối cùng có thể gây trở ngại cho việc đánh giá cuối cùng.\n```",
    "reference": "Có thể là do việc suy luận logic giúp xác định các mâu thuẫn thực tế giữa văn bản gốc và bản tóm tắt, điều này quan trọng trong tóm tắt văn bản, nhưng không quan trọng bằng trong trả lời câu hỏi."
  },
  {
    "user_input": "Việc phân biệt giữa các mẫu đúng và các mẫu bịa đặt mang lại kết quả như thế nào?",
    "retrieved_contexts": [
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "examples for chatgpt to test whether it can distinguish the right sample from the hallucinated sample. as we can see from table 8, distinguishing\nbetween right and hallucinated samples achieves\nthe worst results. we hypothesize that our generated hallucinated samples have a high similarity\nto the real samples, thus making llms confused\nto distinguish them. this test also indicates that\nour benchmark is very challenging in hallucination\nevaluation for llms.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a.",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "44\n\n**claude 2** 69.78 64.73 57.75 75.00\n\n**claude** 67.60 64.83 53.76 73.88\n\n**davinci002** 60.05 60.81 47.77 80.42\n\n**davinci003** 49.65 68.37 48.07 80.40\n\n**gpt-3** 49.21 50.02 51.23 72.72\n\n**llama 2** 49.60 43.99 49.55 20.46\n\n**chatglm** 47.93 44.41 48.57 30.92\n\n**falcon** 39.66 29.08 42.71 18.98\n\n**vicuna** 60.34 46.35 45.62 19.48\n**alpaca** 6.68 17.55 20.63 9.54\n\ntable 5: accuracy (%) of classifying whether a sample\ncontains hallucinated contents.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future."
    ],
    "response": "Việc phân biệt giữa các mẫu đúng và các mẫu bịa đặt (hallucinated samples) thường cho kết quả kém nhất.\n\n1.  **Kết quả kém:**\n\n    *   Việc phân biệt giữa các mẫu đúng và các mẫu bịa đặt thường cho kết quả kém nhất.\n    *   Điều này cho thấy rằng các mẫu bịa đặt được tạo ra có độ tương đồng cao với các mẫu thật.\n2.  **Độ tương đồng cao:**\n\n    *   Các LLM có thể bị nhầm lẫn khi phân biệt các mẫu đúng và mẫu bịa đặt.\n    *   Các mẫu bịa đặt có độ tương đồng cao với các mẫu thật, gây khó khăn cho LLM trong việc phân biệt.\n3.  **Thách thức:**\n\n    *   Điều này cũng cho thấy rằng benchmark dùng để đánh giá khả năng nhận biết \"ảo giác\" (hallucination) của LLM là một thách thức lớn.\n```",
    "reference": "Việc phân biệt giữa các mẫu đúng và các mẫu bịa đặt mang lại kết quả tệ nhất."
  },
  {
    "user_input": "Tại sao các LLM lại gặp khó khăn trong việc phân biệt giữa các mẫu thật và các mẫu bịa đặt?",
    "retrieved_contexts": [
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "examples for chatgpt to test whether it can distinguish the right sample from the hallucinated sample. as we can see from table 8, distinguishing\nbetween right and hallucinated samples achieves\nthe worst results. we hypothesize that our generated hallucinated samples have a high similarity\nto the real samples, thus making llms confused\nto distinguish them. this test also indicates that\nour benchmark is very challenging in hallucination\nevaluation for llms.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language.",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "## **4 related work**\n\n**hallucination in llms.** hallucination in llms is\n\nconcerning since it hinders performance and raises\nsafety risks in real-world application. to alleviate\nthis issue, prior studies have proposed to use a verification system to identify non-factual entities in\ntext summarization (zhao et al., 2020), invoke interfaces of structured data ( *e.g.,* knowledge graph,\ndatabase) to obtain related evidence (jiang et al.,\n2023; lan et al., 2022), and train a token-level fact\ncritic to recognize hallucination and rectify them\nin dialogue (dziri et al., 2021). to enhance the\nunderstanding of hallucination in llms and pro\n\n6455"
    ],
    "response": "Các LLM gặp khó khăn trong việc phân biệt giữa các mẫu thật và các mẫu bịa đặt vì những lý do sau:\n\n1.  **Độ tương đồng cao:**\n\n    *   Các mẫu bịa đặt được tạo ra có độ tương đồng rất cao với các mẫu thật. Điều này khiến LLM khó phân biệt sự khác biệt giữa chúng.\n2.  **Sự khác biệt tinh tế:**\n\n    *   Sự khác biệt giữa mẫu thật và mẫu bịa đặt có thể nằm ở những chi tiết nhỏ hoặc các thông tin thực tế quan trọng. LLM có thể bỏ qua những chi tiết này.\n3.  **Thông tin sai lệch:**\n\n    *   Các LLM có thể gặp khó khăn trong việc xác định thông tin sai lệch, đặc biệt nếu thông tin đó có vẻ hợp lý hoặc phù hợp với ngữ cảnh.\n4.  **Thiếu kiến thức:**\n\n    *   LLM có thể thiếu kiến thức nền tảng hoặc khả năng suy luận cần thiết để xác định liệu một mẫu có chứa thông tin bịa đặt hay không.\n5.  **Đánh lừa:**\n\n    *   Các mẫu bịa đặt được thiết kế để đánh lừa LLM, làm cho chúng khó nhận ra sự khác biệt so với các mẫu thật.\n```",
    "reference": "Các LLM gặp khó khăn vì các mẫu bịa đặt được tạo ra có độ tương đồng cao với các mẫu thật."
  },
  {
    "user_input": "Bài kiểm tra này cho thấy điều gì về tiêu chuẩn đánh giá (benchmark) trong việc đánh giá khả năng tạo thông tin sai lệch (hallucination) của LLM?",
    "retrieved_contexts": [
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "hallucinated content. finally, our benchmark can\nbe further paired with human annotation to assess\nwhether the llms’ output contains hallucinations,\nsince the samples in our benchmark are specially\ndesigned for testing the hallucinations of llms.\nto use our benchmark, users can run the code in\nour project repository to conduct the corresponding\nevaluation and analysis. users can use our provided\ninstructions on their own datasets to evaluate llms\n\non hallucinations.",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) ."
    ],
    "response": "Bài kiểm tra này cho thấy rằng tiêu chuẩn đánh giá (benchmark) trong việc đánh giá khả năng tạo thông tin sai lệch (hallucination) của LLM còn nhiều thách thức.1.  **Thách thức trong việc đánh giá:**\n\n    *   Việc LLM gặp khó khăn trong việc phân biệt các mẫu thật và mẫu bịa đặt cho thấy rằng việc đánh giá khả năng nhận biết thông tin sai lệch của LLM là một vấn đề phức tạp.\n2.  **Độ khó của benchmark:**\n\n    *   Benchmark cần phải đủ khó để thách thức khả năng của LLM trong việc phân biệt thông tin thật và thông tin sai lệch.\n3.  **Cần cải thiện benchmark:**\n\n    *   Cần cải thiện các benchmark hiện tại để chúng có thể đánh giá chính xác hơn khả năng của LLM trong việc nhận biết và tránh tạo ra thông tin sai lệch.\n4.  **Thiết kế benchmark:**\n\n    *   Việc thiết kế các benchmark hiệu quả đòi hỏi sự hiểu biết sâu sắc về cách LLM tạo ra thông tin sai lệch và các yếu tố có thể ảnh hưởng đến khả năng nhận biết của chúng.\n```",
    "reference": "Bài kiểm tra này cho thấy tiêu chuẩn đánh giá rất khó khăn trong việc đánh giá khả năng tạo thông tin sai lệch của LLM."
  },
  {
    "user_input": "Ví dụ nào được đề cập trong đoạn văn về việc ChatGPT tạo ra thông tin sai lệch?",
    "retrieved_contexts": [
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "table 7: two hallucinated and refined examples from chatgpt. the **green** text denotes the hallucinated span, and\nthe **brown** text denotes the refined span after augmented with retrieved knowledge.\n\n\n**variants** **qa** **dialogue summarization general**\n\n**chatgpt** 62.59 72.40 58.53 86.22\n\nw/ knowledge 76.83 73.80 - 90.73\nw/ cot 59.58 71.39 61.21 86.50\n\nw/ contrast 49.19 68.67 49.",
      "chatgpt cannot access the web to obtain the official data. after providing official information\nretrieved from world bank, the refined span displays answers that contain the correct information.\nthe above two examples illustrate that retrieving\nknowledge related to queries can help chatgpt significantly reduce the hallucinations in the response,\nespecially those factual errors."
    ],
    "response": "Đoạn văn đề cập đến hai ví dụ về việc ChatGPT tạo ra thông tin sai lệch:\n\n1.  **Ngày ký Tuyên ngôn Độc lập:** ChatGPT đưa ra thông tin sai về thời gian ký Tuyên ngôn Độc lập (\"July 4, 1776 - Declaration of Independence signing\" là sai).\n2.  **Tỷ lệ tăng trưởng GDP của Trung Quốc và Ấn Độ:** ChatGPT đưa ra số liệu không chính xác về tỷ lệ tăng trưởng GDP của Trung Quốc và Ấn Độ. Điều này là do ChatGPT không thể truy cập web để lấy dữ liệu chính thức.\n```",
    "reference": "ChatGPT đã đưa ra ngày sai về thời điểm ký Tuyên ngôn Độc lập, cụ thể là \"4 tháng 7 năm 1776 - ký Tuyên ngôn Độc lập\"."
  },
  {
    "user_input": "Ngoài ví dụ về Tuyên ngôn Độc lập, ChatGPT còn đưa ra thông tin sai lệch nào khác được đề cập trong đoạn văn?",
    "retrieved_contexts": [
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "|---|---|---|\n|chatgpt response|1. july 4, 1776 - declaration of independence sign- ing 2. april 12, 1861 - start of the civil war 3. august 18, 1920 - 19th amendment ratification gr- anting women the right to vote 4. december 7, 1941 - pearl harbor attack during wo- rld war ii 5. september 11, 2001 - terrorist attacks on new york city and washington d.c.|as an ai language model, i cannot create visualization but i can provide you with the following information: according to world bank data, india and china have shown significant economic growth in the past two de- cades. in 1998, india’s gdp growth rate was 4.8%, while china’s gdp growth rate was 9.3%. in the following years, the gdp growth rate fluctuat- ed, but china maintained a higher growth rate than india. by 2019, china’s gdp growth rate was 6.1%, while india’s gdp growth rate was 4.2%.|",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "chatgpt cannot access the web to obtain the official data. after providing official information\nretrieved from world bank, the refined span displays answers that contain the correct information.\nthe above two examples illustrate that retrieving\nknowledge related to queries can help chatgpt significantly reduce the hallucinations in the response,\nespecially those factual errors.",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language."
    ],
    "response": "Đoạn văn cũng đề cập đến một ví dụ khác về thông tin sai lệch do ChatGPT tạo ra, đó là:\n\n*   **Tỷ lệ tăng trưởng GDP của Trung Quốc và Ấn Độ:** ChatGPT cung cấp dữ liệu không chính xác về tỷ lệ tăng trưởng GDP của hai quốc gia này.\n```",
    "reference": "ChatGPT đã đưa ra các số liệu tăng trưởng GDP không chính xác của Trung Quốc và Ấn Độ."
  },
  {
    "user_input": "ChatGPT có thể truy cập internet để lấy dữ liệu chính thức không?",
    "retrieved_contexts": [
      "chatgpt cannot access the web to obtain the official data. after providing official information\nretrieved from world bank, the refined span displays answers that contain the correct information.\nthe above two examples illustrate that retrieving\nknowledge related to queries can help chatgpt significantly reduce the hallucinations in the response,\nespecially those factual errors.",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "**3.2.2** **improvement strategies**\n\nin this part, we design several strategies to improve\nthe ability of llms to recognize hallucination. the\nresults are shown in table 8.\n\n**knowledge retrieval.** retrieving relevant knowledge is a widely used strategy to eliminate hallucination (lewis et al., 2020; li et al., 2023a). therefore, we supply chatgpt with the knowledge facts\nretrieved from wikipedia (except for that summarization does not need external information besides",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "related knowledge to identify the factual hallucination in the generated text. to further understand\nthe failures of chatgpt, we visualize the topics of\nthose failed samples via latent dirichlet allocation\n(lda) (blei et al., 2003). as shown in figure 2\nand figure 3, we cluster all task samples into ten\ntopics and mark the topics of failed samples as red.\nwe find that the hallucination of llms is topicsensitive. for example, the frequent topics in qa\ninclude film, school, and company. while, chatgpt mainly fails to recognize those samples in\nthe topics of film, company, and band. for user\nqueries and chatgpt responses, the top five topics\ninclude story, health, language, technology, and\ncomputer. chatgpt mainly faces challenges in\ntopics of technology, climate, and language.",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for"
    ],
    "response": "Không, ChatGPT không thể truy cập internet để lấy dữ liệu chính thức. Điều này được đề cập trong ngữ cảnh ví dụ về việc ChatGPT đưa ra thông tin không chính xác về tỷ lệ tăng trưởng GDP của Trung Quốc và Ấn Độ.",
    "reference": "Không, ChatGPT không thể truy cập internet để lấy dữ liệu chính thức."
  },
  {
    "user_input": "Việc truy xuất kiến thức liên quan đến truy vấn có lợi ích gì cho ChatGPT?",
    "retrieved_contexts": [
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "chatgpt cannot access the web to obtain the official data. after providing official information\nretrieved from world bank, the refined span displays answers that contain the correct information.\nthe above two examples illustrate that retrieving\nknowledge related to queries can help chatgpt significantly reduce the hallucinations in the response,\nespecially those factual errors.",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "**3.2.2** **improvement strategies**\n\nin this part, we design several strategies to improve\nthe ability of llms to recognize hallucination. the\nresults are shown in table 8.\n\n**knowledge retrieval.** retrieving relevant knowledge is a widely used strategy to eliminate hallucination (lewis et al., 2020; li et al., 2023a). therefore, we supply chatgpt with the knowledge facts\nretrieved from wikipedia (except for that summarization does not need external information besides",
      "**cot reasoning.** in previous work (wei et al.,\n2022), chain-of-thought (cot) has been proposed\nto improve the ability of llms to perform reasoning and derive the final answer by introducing a series of intermediate reasoning steps. here, besides\nproducing the recognition result, we also require\nchatgpt to generate the reasoning steps. while,\nfrom the results in table 8, we observe that generating reasoning steps can mildly improve the performance but makes the model perform worse in qa\nand dialogue ( *e.g.,* dropping from 62.59 to 59.58).\ncompared to retrieving knowledge, adding chainof-thought before output might interfere with the\nfinal judgement. while, in text summarization, generating reasoning steps improve the accuracy from\n58.53 to 61.21. the reason might be that the factual\ncontradiction between document and summary can\nbe identified through logic reasoning.\n\n**sample contrast.** we further provide ground-truth",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance."
    ],
    "response": "1.  **Giảm thiểu Hallucination:** Việc truy xuất kiến thức liên quan giúp ChatGPT giảm đáng kể tình trạng \"hallucination\" (tạo ra thông tin sai lệch) trong phản hồi của nó, đặc biệt là các lỗi liên quan đến sự thật.\n2.  **Cải thiện Độ Chính Xác:** Bằng cách cung cấp thông tin chính thức và đáng tin cậy, ChatGPT có thể sửa các thông tin sai lệch đã tạo ra trước đó và đưa ra câu trả lời chính xác hơn.\n3.  **Tăng Khả Năng Nhận Biết Hallucination:** Trang bị cho LLM (Mô hình ngôn ngữ lớn) kiến thức bên ngoài có thể tăng cường khả năng nhận biết \"hallucination\". Ví dụ, độ chính xác của ChatGPT trong việc nhận biết \"hallucination\" tăng đáng kể (ví dụ: tăng từ 62,59% lên 76,83% trong QA).\n```",
    "reference": "Việc truy xuất kiến thức liên quan đến truy vấn có thể giúp ChatGPT giảm đáng kể các lỗi sai, đặc biệt là các lỗi sai về mặt thực tế."
  },
  {
    "user_input": "Điều gì xảy ra khi ChatGPT được cung cấp thông tin chính thức, ví dụ như thông tin từ Ngân hàng Thế giới?",
    "retrieved_contexts": [
      "chatgpt cannot access the web to obtain the official data. after providing official information\nretrieved from world bank, the refined span displays answers that contain the correct information.\nthe above two examples illustrate that retrieving\nknowledge related to queries can help chatgpt significantly reduce the hallucinations in the response,\nespecially those factual errors.",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "**3.3** **case study**\n\nin the above, we have observed that providing external knowledge can be beneficial for llms to mitigate and recognize hallucinations. to demonstrate\nthe effectiveness of knowledge retrieval in mitigating hallucinations, we present two hallucinated\nresponses from chatgpt and refined responses\nafter augmented with retrieved knowledge in table 7. in the first example, the generated span ( *i.e.,*\n“july 4, 1776 - declaration of independence signing”) contains hallucinated information because it\ngives a wrong time of declaration of independence\nsigning. by providing retrieved information about\ndeclaration of independence signing, chatgpt is\nable to correct the hallucinated span and give the\nright information. analogously, in the second example, chatgpt gives incorrect gdp growth rates\nof china and india, which is due to that api-based",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "table 5 presents the accuracy of evaluated llms\nto classify whether the sample output contains hallucinated information. our findings indicate that\nllms are still poor at identifying hallucination\nwhich might be implicit in text. for example,\nthe state-of-the-art chatgpt model cannot distin\nguish between factual and hallucinated summary\nand only achieves 58.53% accuracy in text summarization, which is barely above chance. moreover,\ngpt-3 obtains just about random chance of 50%\naccuracy across three tasks, and alpaca or vicuna\neven performs worse (well below random chance).\nwe hypothesize that llms perform poorly because\nthe hallucinated sample we generate looks highly\nsimilar with ground-truth ones but differs in the\nkey factual spans. as we can see, from gpt-3 to\ninstructgpt and chatgpt, instruction tuning and\nalignment with humans can strength the ability of\nllms in identifying the hallucinations in text.",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for"
    ],
    "response": "Khi ChatGPT được cung cấp thông tin chính thức, ví dụ như thông tin từ Ngân hàng Thế giới, kết quả là:\n\n1.  **Thông tin được cải thiện:** Các thông tin được tinh chỉnh hiển thị các câu trả lời chứa thông tin chính xác.\n2.  **Giảm Hallucination:** Việc truy xuất kiến thức liên quan đến các truy vấn có thể giúp ChatGPT giảm đáng kể các \"hallucination\" trong phản hồi, đặc biệt là các lỗi sai về mặt sự thật.\n3.  **Sửa chữa thông tin sai lệch:** ChatGPT có thể sửa các thông tin sai lệch đã tạo ra trước đó và đưa ra thông tin chính xác. Ví dụ, ChatGPT có thể sửa lỗi về thời gian ký Tuyên ngôn Độc lập hoặc đưa ra tỷ lệ tăng trưởng GDP chính xác của Trung Quốc và Ấn Độ sau khi được cung cấp thông tin phù hợp.\n```",
    "reference": "Khi ChatGPT được cung cấp thông tin chính thức từ các nguồn như Ngân hàng Thế giới, các câu trả lời của nó sẽ chính xác hơn."
  },
  {
    "user_input": "Tại sao hiện tượng ảo giác (hallucination) trong các mô hình ngôn ngữ lớn (LLM) lại đáng lo ngại?",
    "retrieved_contexts": [
      "## **4 related work**\n\n**hallucination in llms.** hallucination in llms is\n\nconcerning since it hinders performance and raises\nsafety risks in real-world application. to alleviate\nthis issue, prior studies have proposed to use a verification system to identify non-factual entities in\ntext summarization (zhao et al., 2020), invoke interfaces of structured data ( *e.g.,* knowledge graph,\ndatabase) to obtain related evidence (jiang et al.,\n2023; lan et al., 2022), and train a token-level fact\ncritic to recognize hallucination and rectify them\nin dialogue (dziri et al., 2021). to enhance the\nunderstanding of hallucination in llms and pro\n\n6455",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use"
    ],
    "response": "Hiện tượng ảo giác (hallucination) trong các mô hình ngôn ngữ lớn (LLM) đáng lo ngại vì:\n\n1.  **Cản trở hiệu suất:** \"Hallucination\" làm giảm hiệu suất của LLM.\n2.  **Gây ra rủi ro về an toàn:** \"Hallucination\" làm tăng rủi ro về an toàn trong các ứng dụng thực tế.\n3.  **Tạo ra nội dung sai lệch:** ChatGPT có thể tạo ra nội dung sai lệch liên quan đến các chủ đề cụ thể bằng cách dựng lên các thông tin không thể kiểm chứng, với tỷ lệ khoảng 19,5% các phản hồi.\n4.  **Khó phân biệt:** LLM thường gặp khó khăn trong việc phân biệt giữa các mẫu đúng và các mẫu \"ảo giác,\" cho thấy mức độ tương đồng cao giữa chúng.\n```",
    "reference": "Hiện tượng ảo giác trong các LLM đáng lo ngại vì nó cản trở hiệu suất và làm tăng rủi ro an toàn trong các ứng dụng thực tế."
  },
  {
    "user_input": "Các nghiên cứu trước đây đã đề xuất những phương pháp nào để giảm thiểu hiện tượng ảo giác trong các mô hình ngôn ngữ lớn?",
    "retrieved_contexts": [
      "## **4 related work**\n\n**hallucination in llms.** hallucination in llms is\n\nconcerning since it hinders performance and raises\nsafety risks in real-world application. to alleviate\nthis issue, prior studies have proposed to use a verification system to identify non-factual entities in\ntext summarization (zhao et al., 2020), invoke interfaces of structured data ( *e.g.,* knowledge graph,\ndatabase) to obtain related evidence (jiang et al.,\n2023; lan et al., 2022), and train a token-level fact\ncritic to recognize hallucination and rectify them\nin dialogue (dziri et al., 2021). to enhance the\nunderstanding of hallucination in llms and pro\n\n6455",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449"
    ],
    "response": "Các nghiên cứu trước đây đã đề xuất các phương pháp sau để giảm thiểu hiện tượng ảo giác trong các mô hình ngôn ngữ lớn (LLM):\n\n1.  **Sử dụng hệ thống xác minh:** Sử dụng một hệ thống xác minh để xác định các thực thể không có thật trong tóm tắt văn bản (text summarization).2.  **Sử dụng giao diện dữ liệu có cấu trúc:** Sử dụng các giao diện của dữ liệu có cấu trúc (ví dụ: biểu đồ tri thức, cơ sở dữ liệu) để thu thập bằng chứng liên quan.\n3.  **Đào tạo Fact Critic cấp độ Token:** Đào tạo một \"fact critic\" ở cấp độ token để nhận biết \"hallucination\" và sửa chúng trong hội thoại.\n4.  **Cung cấp kiến thức bên ngoài:** Cung cấp cho LLM kiến thức bên ngoài có thể giúp chúng nhận biết \"hallucination\".\n5.  **Thêm các bước suy luận trung gian:** Việc thêm các bước suy luận trung gian có thể giúp LLM nhận ra \"hallucination\".\n```",
    "reference": "Các nghiên cứu trước đây đã đề xuất sử dụng hệ thống xác minh để xác định các thực thể phi thực tế trong tóm tắt văn bản, sử dụng các giao diện của dữ liệu có cấu trúc để có được bằng chứng liên quan và đào tạo một 'fact critic' ở cấp độ token để nhận ra và khắc phục ảo giác trong đối thoại."
  },
  {
    "user_input": "Mục tiêu chung của việc sử dụng hệ thống xác minh, khai thác dữ liệu có cấu trúc và đào tạo 'fact critic' trong bối cảnh các mô hình ngôn ngữ lớn là gì?",
    "retrieved_contexts": [
      "## **4 related work**\n\n**hallucination in llms.** hallucination in llms is\n\nconcerning since it hinders performance and raises\nsafety risks in real-world application. to alleviate\nthis issue, prior studies have proposed to use a verification system to identify non-factual entities in\ntext summarization (zhao et al., 2020), invoke interfaces of structured data ( *e.g.,* knowledge graph,\ndatabase) to obtain related evidence (jiang et al.,\n2023; lan et al., 2022), and train a token-level fact\ncritic to recognize hallucination and rectify them\nin dialogue (dziri et al., 2021). to enhance the\nunderstanding of hallucination in llms and pro\n\n6455",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) ."
    ],
    "response": "Không có tài liệu nào trong số các tài liệu được cung cấp trực tiếp trả lời câu hỏi này.",
    "reference": "Mục tiêu chung là giảm thiểu hiện tượng ảo giác và nâng cao độ tin cậy của thông tin do các mô hình ngôn ngữ lớn tạo ra."
  },
  {
    "user_input": "Công trình nghiên cứu này tập trung vào điều gì liên quan đến các mô hình ngôn ngữ lớn (LLMs)?",
    "retrieved_contexts": [
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "llms in three aspects. firstly, based on our generated and annotated samples, researchers can use\nthem to analyze what types of content llms tend\nto generate hallucinations. second, researchers can\nfurther evaluate the ability of llms to recognize\nhallucinations in the generated samples. for example, given a question and an answer, llms can\nbe asked to determine whether the answer contains"
    ],
    "response": "Công trình nghiên cứu này tập trung vào các vấn đề sau liên quan đến các mô hình ngôn ngữ lớn (LLMs):\n\n1.  **Giới thiệu Halueval:** Halueval là một bộ sưu tập lớn các mẫu \"hallucination\" được tạo và chú thích bởi con người, dùng để đánh giá hiệu suất của LLMs trong việc nhận biết \"hallucination\".\n2.  **Phân tích các loại nội dung:** Nghiên cứu sử dụng Halueval để phân tích các loại nội dung mà LLMs có xu hướng tạo ra \"hallucination\".\n3.  **Đánh giá khả năng nhận biết \"hallucination\":** Nghiên cứu đánh giá khả năng của LLMs trong việc nhận biết \"hallucination\" trong các mẫu được tạo ra.\n4.  **Đề xuất các chiến lược:** Nghiên cứu đề xuất một số chiến lược để giúp LLMs nhận biết \"hallucination\".\n5.  **Nhận thấy LLMs gặp khó khăn:** Các LLMs hiện tại thường không nhận ra \"hallucination\" trong văn bản và có xu hướng tạo ra nội dung \"hallucination\".\n6.  **Đánh giá các phương pháp:** Nghiên cứu đánh giá việc cung cấp kiến thức bên ngoài hoặc thêm các bước suy luận có thể giúp LLMs nhận biết \"hallucination\".\n7.  **Xác định các loại nội dung và mức độ:** Nghiên cứu nhằm mục đích xác định loại nội dung nào và LLMs có xu hướng \"hallucination\" đến mức độ nào.\n8.  **Tạo mẫu tự động:** Nghiên cứu đề xuất một khuôn khổ hai giai đoạn (lấy mẫu, sau đó lọc) để tự động tạo các mẫu \"hallucination\" trên quy mô lớn.\n9.  **Chú thích của con người:** Nghiên cứu sử dụng người chú thích để đánh giá \"hallucination\" trong các phản hồi của ChatGPT cho các truy vấn của người dùng.\n10. **ChatGPT có khả năng tạo nội dung \"hallucination\":** ChatGPT có khả năng tạo nội dung \"hallucination\" liên quan đến các chủ đề cụ thể bằng cách tạo ra thông tin không thể kiểm chứng (khoảng 19,5% phản hồi).\n11. **Các LLMs hiện tại gặp khó khăn lớn:** Các LLMs hiện tại gặp khó khăn lớn trong việc nhận biết \"hallucination\" trong văn bản.",
    "reference": "Công trình này tập trung vào việc xây dựng một chuẩn đánh giá ảo giác cho các mô hình ngôn ngữ lớn (LLMs)."
  },
  {
    "user_input": "Dữ liệu được đề cập trong đoạn văn có thể giúp ích gì trong việc nghiên cứu về các mô hình ngôn ngữ lớn?",
    "retrieved_contexts": [
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "the source document). by providing knowledge,\nthe recognition accuracy of chatgpt increases significantly ( *e.g.,* increasing from 62.59 to 76.83 in\nqa), while the performance improvement in dialogue is mild. we hypothesize that the common\nhallucination patterns in dialogue ( *i.e.,* extrinsicsoft/hard) cannot be simply identified via incorporating external knowledge. for those general user\nqueries and chatgpt responses, we discover that\nproviding external knowledge does have a significant benefit. thus, equipping llms with external\nknowledge can largely enhance their abilities to\nrecognize hallucinations.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future."
    ],
    "response": "Dữ liệu được đề cập trong đoạn văn có thể giúp ích trong việc nghiên cứu về các mô hình ngôn ngữ lớn (LLMs) theo những cách sau:\n\n1.  **Đánh giá \"hallucination\":** Dữ liệu này có thể được sử dụng như một nền tảng công khai để đánh giá và so sánh khả năng của các LLMs khác nhau trong việc giải quyết vấn đề \"hallucination\".\n2.  **Xác định điểm mù của LLMs:** Dữ liệu có thể giúp xác định các điểm mù của LLMs trong việc giải quyết \"hallucination\".\n3.  **Nghiên cứu nguyên nhân:** Dữ liệu có thể hỗ trợ nghiên cứu để phân tích nguyên nhân của \"hallucination\" trong các tác vụ khác nhau và điều tra mối quan hệ giữa chúng.\n4.  **Hiểu loại nội dung:** Dữ liệu có thể giúp hiểu loại nội dung nào và LLMs có xu hướng \"hallucination\" đến mức độ nào.\n5.  **Phát triển LLMs hiệu quả hơn:** Dữ liệu có thể mở đường cho việc xây dựng các LLMs hiệu quả và đáng tin cậy hơn trong tương lai.\n6.  **Cải thiện khả năng nhận biết \"hallucination\":** Dữ liệu có thể được sử dụng để phát triển và đánh giá các chiến lược giúp LLMs nhận biết \"hallucination\".\n7.  **Phục vụ như một chuẩn mực công khai:** Tập dữ liệu có thể phục vụ như một chuẩn mực công khai để thể hiện những điểm mù của LLM trong việc giải quyết \"hallucination\".",
    "reference": "Dữ liệu này có thể giúp bộc lộ những điểm yếu của các mô hình ngôn ngữ lớn trong việc giải quyết vấn đề ảo giác."
  },
  {
    "user_input": "Xu hướng chung của các nghiên cứu được đề cập trong đoạn văn là gì?",
    "retrieved_contexts": [
      "## **1 introduction**\n\nthe advent of large language models (llms) (zhao\net al., 2023) has ushered in a paradigm shift in natural language processing (nlp), making unprecedented progress in text generation and understanding (brown et al., 2020; li et al., 2021). the remarkable language ability makes llms core in a\nnumber of products with millions of users, such as\nthe coding assistant copilot and recent chatgpt.\ndespite these prominent capabilities of llms\ntrained on large text corpus, recent work has shown\nthat llms are prone to suffer from *hallucination*\n*generations* across various applications (ji et al.,\n\n*∗* equal contributions\n\n*†* corresponding author",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "llms in three aspects. firstly, based on our generated and annotated samples, researchers can use\nthem to analyze what types of content llms tend\nto generate hallucinations. second, researchers can\nfurther evaluate the ability of llms to recognize\nhallucinations in the generated samples. for example, given a question and an answer, llms can\nbe asked to determine whether the answer contains",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com"
    ],
    "response": "Xu hướng chung của các nghiên cứu được đề cập trong đoạn văn là:\n\n1.  **Phân tích nguyên nhân của \"hallucination\":** Nhiều nỗ lực đang được thực hiện để phân tích nguyên nhân của \"hallucination\" trong các tác vụ khác nhau và điều tra mối quan hệ giữa chúng.\n2.  **Đánh giá \"hallucination\":** Một loạt các công trình tập trung vào việc đánh giá \"hallucination\" của các mô hình trong các tác vụ xử lý ngôn ngữ tự nhiên (NLP) khác nhau.\n3.  **Xây dựng benchmark đánh giá \"hallucination\":** Nghiên cứu tập trung vào việc xây dựng một benchmark đánh giá \"hallucination\" cho các LLMs.",
    "reference": "Xu hướng chung là tập trung vào việc đánh giá ảo giác của các mô hình trong các nhiệm vụ xử lý ngôn ngữ tự nhiên khác nhau."
  },
  {
    "user_input": "Mục đích của benchmark AIS (Attributable to Identified Sources) là gì?",
    "retrieved_contexts": [
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "## **appendix**\n\nwe provide some extra information about our\nbenchmark as supplementary materials. the appendix is organized into three sections:\n\n  - instructions of hallucination sampling are presented in appendix a;\n\n  instructions of hallucination filtering are presented in appendix b;\n\n  - instructions of evaluation are presented in appendix c;\n\n  - details of our benchmark are presented in appendix d.\n## **a hallucination sampling**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 9 and\ntable 10, respectively.\n## **b hallucination filtering**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 11\nand table 12, respectively.\n## **c hallucination recognition**\n\nthe hallucination recognition instructions for qa,\ndialogue and summarization are shown in table 13,\ntable 14 and table 15, respectively."
    ],
    "response": "Benchmark AIS (Attributable to Identified Sources) có mục đích đánh giá xem các tài liệu nguồn có hỗ trợ đầu ra của các mô hình tạo văn bản hay không.",
    "reference": "Benchmark AIS đánh giá xem các tài liệu nguồn có hỗ trợ đầu ra của các mô hình tạo văn bản hay không."
  },
  {
    "user_input": "Một trong những hạn chế của các benchmark hiện tại là gì?",
    "retrieved_contexts": [
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for"
    ],
    "response": "Một trong những hạn chế của các benchmark hiện tại là chúng không kiểm tra xem các tài liệu nguồn có thực sự hỗ trợ đầu ra của các mô hình tạo văn bản hay không.",
    "reference": "Các benchmark hiện tại thường chỉ tập trung vào các tác vụ đơn lẻ và các mô hình nhỏ, điều này hạn chế khả năng áp dụng rộng rãi của chúng."
  },
  {
    "user_input": "Phương pháp nào được sử dụng để tạo ra các mẫu 'ảo giác' (hallucinated samples) trong benchmark Halueval?",
    "retrieved_contexts": [
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449"
    ],
    "response": "Dựa trên các tài liệu được cung cấp, đây là các phương pháp được sử dụng để tạo ra các mẫu \"ảo giác\" (hallucinated samples) trong benchmark Halueval:\n\n1.  **Tạo mẫu tự động (Automatic Generation):**\n\n*   Sử dụng ChatGPT để tạo ra các mẫu \"ảo giác\" trên quy mô lớn.\n*   Áp dụng quy trình hai bước: \"sampling-then-filtering\" (tạo mẫu rồi lọc).2.  **Chú thích của con người (Human Annotation):**\n\n*   Thuê người để chú thích các \"ảo giác\" trong các phản hồi của ChatGPT đối với các truy vấn của người dùng.\n\n3.  **Sử dụng dữ liệu Alpaca:**\n\n*   Sử dụng tập dữ liệu điều chỉnh hướng dẫn Alpaca (52k) để chọn các truy vấn người dùng có khả năng tạo ra \"ảo giác\" cao nhất.\n\n4.  **Hai phương pháp lấy mẫu khác nhau:**\n\n*   **One-Pass:** Cung cấp hướng dẫn đầy đủ ngay từ đầu.\n*   **Conversational:** Hướng dẫn được đưa ra tăng dần để đảm bảo sự hiểu biết.",
    "reference": "Một quy trình tự động hai bước hoàn toàn dựa trên các mô hình ngôn ngữ lớn (LLMs) được sử dụng để tạo ra các mẫu 'ảo giác'."
  },
  {
    "user_input": "Halueval là gì?",
    "retrieved_contexts": [
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "**3.1** **experimental setup**\n\n**evaluation models.** we evaluate several state-of\nthe-art llms in halueval benchmark. first, we\nexperiment on five closed-source llms, including\nopenai’s gpt-3 ( davinci ) (brown et al., 2020),\ninstructgpt ( text-davinci-002/003 ) (ouyang\net al., 2022), chatgpt ( gpt-3.5-turbo ) and anthropic’s claude and claude 2 models, which can\nonly be accessed through their apis. besides, we\nalso evaluate five prevalent open-source llms, including alpaca (7b) (taori et al., 2023), vicuna\n(7b) (chiang et al., 2023), chatglm (7b) (zeng\net al., 2022), falcon (7b) (tii, 2023), and llama\n2-chat (7b) (touvron et al., 2023). our experiments were performed without fine-tuning or engaging in the tuning of hyper-parameters."
    ],
    "response": "Dựa trên các tài liệu được cung cấp, đây là những thông tin về Halueval:\n\n1.  **Định nghĩa:** Halueval là một benchmark đánh giá khả năng của các mô hình ngôn ngữ lớn (LLM) trong việc nhận biết các \"ảo giác\" (hallucinations).\n\n2.  **Mục tiêu:** Halueval được tạo ra để hiểu *những loại nội dung nào* và *mức độ* LLM có xu hướng tạo ra \"ảo giác\".\n\n3.  **Cấu trúc:**\n\n*   Halueval bao gồm 35.000 mẫu \"ảo giác\"/bình thường để phân tích và đánh giá LLM.\n*   Bao gồm 5.000 truy vấn người dùng *chung* với phản hồi của ChatGPT.\n*   Bao gồm 30.000 ví dụ *dành riêng cho từng tác vụ* từ ba tác vụ: trả lời câu hỏi, đối thoại dựa trên kiến thức và tóm tắt văn bản.\n\n4.  **Phương pháp tạo mẫu:** Halueval sử dụng quy trình hai bước tự động để tạo ra các mẫu \"ảo giác\" để đánh giá, hoàn toàn dựa trên LLM.\n\n5.  **Đóng góp:** Halueval có thể tạo điều kiện thuận lợi cho nghiên cứu trong việc hiểu những loại nội dung nào và LLM có xu hướng tạo ra \"ảo giác\" đến mức độ nào, cuối cùng mở đường cho việc xây dựng các LLM hiệu quả và đáng tin cậy hơn trong tương lai.",
    "reference": "Halueval là một bộ sưu tập quy mô lớn gồm các mẫu ảo giác được tạo và chú thích bởi con người, dùng để đánh giá hiệu suất của các mô hình ngôn ngữ lớn (LLM) trong việc nhận biết ảo giác."
  },
  {
    "user_input": "Phương pháp hai bước được đề xuất để tạo mẫu quy mô lớn có bao gồm việc chọn lọc mẫu không?",
    "retrieved_contexts": [
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms.",
      "**2.1** **automatic generation**\n\nour generation pipeline includes two steps: 1) diverse hallucination sampling, and 2) high-quality\nhallucination filtering. we employ chatgpt to\nexecute the creation pipeline automatically.\n\n**diverse hallucination sampling.** since a factual\ntext can be hallucinated from different aspects, we\npropose two different hallucination sampling meth\n\n6450",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use"
    ],
    "response": "Có, phương pháp hai bước được đề xuất để tạo mẫu quy mô lớn bao gồm cả việc chọn lọc mẫu. Cụ thể, nó bao gồm hai bước: tạo mẫu đa dạng và lọc mẫu chất lượng cao.",
    "reference": "Có, phương pháp này bao gồm việc chọn lọc và chọn ra các mẫu khó."
  },
  {
    "user_input": "Mục tiêu cuối cùng của việc nghiên cứu và xây dựng các công cụ đánh giá ảo giác trong mô hình ngôn ngữ lớn là gì?",
    "retrieved_contexts": [
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "table 8: accuracy (%) of chatgpt equipped with three\nimprovement strategies.\n\nmote the unification of research efforts, there are\nmany active endeavors to analyze the causes of hallucination in different tasks and investigate their\nrelationship (zheng et al., 2023; das et al., 2023;\ncao et al., 2022). our work is closely related to\nthese work, but we focus on building a hallucination evaluation benchmark for llms. our dataset\n\ncan serve as a public platform for exhibiting the\nblind spots of llms in solving hallucination.\n\n**hallucination evaluation.** another line of work\n\nfocusing on evaluating the hallucination of models\nin different nlp tasks (dziri et al., 2022b; gupta\net al., 2022; dziri et al., 2022a; rashkin et al., 2021;\nli et al., 2023b). for instance, the begin benchmark (dziri et al., 2022b) classifies the utterances\ngenerated by dialogue systems into three categories,",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "# **halueval: a large-scale hallucination evaluation benchmark** **for large language models**\n## junyi li [1,3,4] [∗], xiaoxue cheng [1], wayne xin zhao [∗] [1,4] [†], jian-yun nie [3] and ji-rong wen [1,2,4] 1 gaoling school of artificial intelligence, renmin university of china 2 school of information, renmin university of china 3 diro, université de montréal 4 beijing key laboratory of big data management and analysis methods lijunyi@ruc.edu.cn chengxiaoxue3@gmail.com batmanfly@gmail.com",
      "table 1: an example from alpaca (taori et al., 2023)\nshowing that chatgpt might generate hallucinated contents ( **green** ) that cannot be verified by existing source.\n\n2023; bang et al., 2023; sun et al., 2023), where\nthe generated content is either in conflict with existing source or cannot be verified by the available\nknowledge resources. the issue of hallucination\nmakes the deployment of llms potentially risky in\nreal-world applications. most exiting work mainly\nfocuses on investigating the causes of hallucination\nfor specific tasks and small language models (cao\net al., 2022; zheng et al., 2023; das et al., 2023).\nhowever, it still remains unclear *what types of con-*\n\n*tent* and *to which extent* llms tend to hallucinate."
    ],
    "response": "Mục tiêu cuối cùng của việc nghiên cứu và xây dựng các công cụ đánh giá ảo giác trong mô hình ngôn ngữ lớn (LLM) là:\n\n1.  **Hiểu rõ hơn về nội dung và mức độ ảo giác:** Xác định loại nội dung mà LLM có xu hướng tạo ra ảo giác và mức độ thường xuyên của hiện tượng này.\n\n2.  **Xây dựng LLM hiệu quả và đáng tin cậy hơn:** Tạo điều kiện cho việc xây dựng các LLM có khả năng cung cấp thông tin chính xác và đáng tin cậy hơn trong tương lai.\n\n3.  **Phát triển các chiến lược nhận biết ảo giác:** Đề xuất các chiến lược giúp LLM nhận biết và giảm thiểu ảo giác.\n\n4.  **Phục vụ như một nền tảng công khai:** Cung cấp một nền tảng để thể hiện những điểm mù của LLM trong việc giải quyết ảo giác.",
    "reference": "Mục tiêu cuối cùng là xây dựng các mô hình ngôn ngữ lớn hiệu quả và đáng tin cậy hơn."
  },
  {
    "user_input": "Điều gì giới hạn chất lượng của các mẫu ảo giác được tạo ra?",
    "retrieved_contexts": [
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "our key findings can be summarized as follows:\n\n  - first, chatgpt is likely to generate hallucinated content by fabricating unverifiable information in its responses ( *i.e.,* about 19 *.* 5% responses).\nthe hallucinated texts from chatgpt cover topics\nincluding language, climate, and technology.\n\n - second, existing llms face significant challenges to identify the hallucinations in the generated text, even for chatgpt which is used to generate these hallucinated samples ( *e.g.,* only 62 *.* 59%\naccuracy for chatgpt in question answering).\n\n  - finally, the deficient performance of llms in\nrecognizing hallucinations can be improved by providing explicit knowledge and adding intermediate\nreasoning steps. while, contrasting hallucinated\nsamples with ground-truth makes llms more confused and leads to worse performance.",
      "conceive effective methods to alleviate it.\nfurthermore, for the task-specific examples, we\ndesign an automatic two-stage approach to generate\nhallucinated samples. first, based on existing task\ndatasets ( *e.g.,* hotpotqa) as seed data, we employ\nchatgpt to generate hallucinated samples with\ntwo styles of task-specific instructions, *i.e.,* onepass and conversational. we expect that these two\nmethods will generate diverse hallucinated samples from different aspects. second, to select the\nmost plausible and difficult hallucinated sample\nfor llms evaluation, we elaborate the filtering instruction enhanced by ground-truth examples and\nleverage chatgpt for sample selection. through\nthe proposed sampling-then-filtering approach, we\ncan generate a hallucinated counterpart for each\nspecific task example. these hallucinated samples\nare designed to challenge the ability of llms in\nhallucination recognition and analyze the information blind spots of llms.",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "## **abstract**\n\nlarge language models (llms), such as chatgpt, are prone to generate hallucinations, *i.e.,*\ncontent that conflicts with the source or cannot\nbe verified by the factual knowledge. to understand *what types of content* and *to which ex-*\n*tent* llms are apt to hallucinate, we introduce\nthe **hal** l **you** cination **eval** uation benchmark for",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future."
    ],
    "response": "Chất lượng của các mẫu ảo giác được tạo ra bị giới hạn bởi:\n\n1.  **Khả năng của ChatGPT:** Chất lượng của các mẫu ảo giác phụ thuộc vào khả năng của ChatGPT trong việc tuân theo các hướng dẫn phức tạp để tạo ra các mẫu ảo giác.\n\n2.  **Kiểm soát chất lượng:** Mặc dù quy trình lọc ảo giác được thiết kế chất lượng cao, nhưng vẫn cần kiểm soát chất lượng đối với việc tạo ra các mẫu ảo giác.",
    "reference": "Chất lượng của các mẫu ảo giác bị giới hạn bởi khả năng của ChatGPT trong việc tuân theo các hướng dẫn phức tạp để lấy mẫu ảo giác."
  },
  {
    "user_input": "Quá trình lọc ảo giác chất lượng cao có đủ để đảm bảo chất lượng của các mẫu ảo giác không?",
    "retrieved_contexts": [
      "## **6 limitations**\n\nin our approach, we leverage a llm, *i.e.,* chatgpt,\nto automatically generate the hallucinated samples.\ntherefore, the quality of our hallucinated samples\nis limited by the capacity of chatgpt in following\nthe complex instruction of hallucination sampling.\nalthough we design the high-quality hallucination\nfiltering process, it is still necessary to apply quality\ncontrol to the generation of hallucinated samples.\nbesides, our benchmark focuses on evaluating the\nability of llms in recognizing the hallucinations in\ntext but does not investigate the underlying reasons\nbehind the appearance of hallucinations like prior\nwork (zheng et al., 2023; das et al., 2023).\n\nas for the potential issue, since the hallucinated\nsamples in our benchmark looks highly similar to\nthe ground-truth samples, which might be misused\nfor an unexpected purpose than we planned. to\nalleviate this issue, we should monitor and regulate\nthe spread and usage of our benchmark.",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "**implementation details.** we execute the generation process of hallucinated samples using azure\nopenai chatgpt api. we use a temperature of\n1 *.* 0 to generate samples and set the maximum number of tokens for generation to 256 . moreover, we\nset the frequency penalty to zero and top- *p* to 1 *.* 0 .\nfor evaluation, we set the temperature to zero for\nall models to reduce output randomness and ensure\nmore focused and deterministic outputs.\nin the following, we first conduct hallucination\nrecognition experiments, then propose several potentially useful strategies to improve the recognition, and finally we perform qualitative analysis to\nunderstand the hallucination in llms.",
      "to select the most plausible and difficult hallucinated samples from the above two sampling methods. as shown in table 3, we design the instruction\nof hallucination filtering enhanced by ground-truth\nanswers to select the best answer from two hallucinated candidates. in the instruction of filtering,\nthe demonstration includes the ground-truth correct\nanswer ( *e.g.,* you.s. highway 60) and a hallucinated\ncounterpart ( *e.g.,* you.s. highway 70). while, in the\ntest example, we input two hallucinated answers.\nfollowing the demonstrations, we expect chatgpt\nto select one of the hallucinated answers that is the",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from"
    ],
    "response": "Không, mặc dù quy trình lọc ảo giác được thiết kế chất lượng cao, nhưng vẫn cần kiểm soát chất lượng đối với việc tạo ra các mẫu ảo giác. Điều này cho thấy rằng quá trình lọc không hoàn toàn đảm bảo chất lượng và cần được bổ sung bằng các biện pháp kiểm soát khác.",
    "reference": "Không, vẫn cần phải áp dụng kiểm soát chất lượng cho việc tạo ra các mẫu ảo giác, mặc dù đã có quá trình lọc chất lượng cao."
  },
  {
    "user_input": "Điểm chuẩn này tập trung vào điều gì liên quan đến ảo giác và nó khác với các nghiên cứu trước đây như thế nào?",
    "retrieved_contexts": [
      "## **2 the halueval benchmark**\n\nas the goal of halueval is to understand *what types*\n*of content* and *to which extent* llms tend to hallucinate, the benchmark contains a myriad of correct\nsamples and their hallucinated counterparts. this\ncollection is created via two ways, *i.e.,* automatic\ngeneration and human annotation.",
      "## **5 conclusion**\n\nwe introduce halueval, a large-scale collection of\ngenerated and human-annotated hallucinated samples for evaluating the performance of llms in\nrecognizing hallucinations. to automatically generate large-scale samples, we propose a two-step\n\n\n6456\n\n\n-----\n\napproach, *i.e.,* sampling-then-filtering. we first introduce two different sampling methods to generate\ndiverse samples using instructions and then filter\nand select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations\nof chatgpt responses given user queries. we find\nthat, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated\ncontent. finally, we suggest several strategies to\nhelp llms recognize hallucinations. our benchmark can facilitate research in understanding what\ntypes of content and to which extent llms tend to\nhallucinate, ultimately paving the way for building\nmore effective and reliable llms in the future.",
      "*tent* and *to which extent* llms tend to hallucinate.\n\nto facilitate research in this direction, we present\nthe **hal** l **you** cination **eval** uation benchmark for large\nlanguage models ( **halueval** ): a large collection\nof 35,000 hallucinated/normal samples for llms\nanalysis and evaluation. halueval includes 5,000\n*general* user queries with chatgpt responses and\n30,000 *task-specific* examples from three tasks, *i.e.,*\nquestion answering, knowledge-grounded dialogue,\nand text summarization. the construction pipeline\nof halueval is depicted in figure 1. for general\nuser queries, we adopt the 52 k instruction tuning\ndataset from alpaca (taori et al., 2023) for human annotation. to further screen out user queries\nwhere llms are most likely to produce hallucinations, we use chatgpt to sample three responses\nfor each query and only retain 5 *,* 000 queries with\nthe lowest similarity among three responses. ac\n\n6449",
      "large language models ( **halueval** ), a large\ncollection of generated and human-annotated\nhallucinated samples for evaluating the performance of llms in recognizing hallucination.\nto generate these samples automatically, we\npropose a two-stage framework, *i.e., sampling-*\n*then-filtering* . besides, we hire some human\nlabelers to annotate the hallucinations in chat\ngpt responses. the empirical results suggest\nthat chatgpt is likely to generate hallucinated\ncontent related to specific topics by fabricating\nunverifiable information ( *i.e.,* about 19 *.* 5% responses). moreover, existing llms face great\nchallenges in recognizing the hallucinations in\ntexts. however, our experiments also prove\nthat providing external knowledge or adding\nreasoning steps can help llms recognize hallucinations. our benchmark can be accessed at\n\n[https://github.com/rucaibox/halueval](https://github.com/rucaibox/halueval) .",
      "*i.e.,* fully attributable, not fully attributable, and\ngeneric; and the attributable to identified sources\n(ais) benchmark (rashkin et al., 2021) assesses\nwhether the source documents support the output of\ntext generation models. though these benchmarks\ncan serve as decent evaluation platform, they are\npenurious in only focusing on single tasks ( *e.g.,*\ndialogue) and small models ( *e.g.,* dpr). besides,\nseveral metrics have been proposed to quantify hallucination, such as parent (dhingra et al., 2019)\nfor measuring *n* -gram lexical entailment in table-totext generation and true (honovich et al., 2022)\ncomputes the example-level area under the roc\ncurve. in this work, our halueval benchmark includes general user queries and chatgpt responses\nand proposes a two-step automatic process to generate hallucinated samples for evaluation, which is\ncompletely based on llms."
    ],
    "response": "Điểm chuẩn Halueval tập trung vào việc đánh giá hiệu suất của các LLM trong việc nhận biết ảo giác, đồng thời tìm hiểu loại nội dung và mức độ mà LLM có xu hướng tạo ra ảo giác.\n\n1.  **Halueval so với các nghiên cứu trước đây:**\n    *   **Phạm vi:** Halueval bao gồm các truy vấn người dùng chung và phản hồi ChatGPT, đồng thời đề xuất quy trình tự động hai bước để tạo các mẫu ảo giác để đánh giá, hoàn toàn dựa trên LLM. Các tiêu chuẩn khác có thể chỉ tập trung vào các tác vụ đơn lẻ (ví dụ: đối thoại) và các mô hình nhỏ (ví dụ: DPR).\n    *   **Nội dung:** Halueval chứa một bộ sưu tập lớn các mẫu ảo giác/bình thường được tạo và chú thích bởi con người để đánh giá hiệu suất của LLM trong việc nhận biết ảo giác. Nó bao gồm 5.000 truy vấn người dùng chung và 30.000 ví dụ cụ thể theo nhiệm vụ từ ba nhiệm vụ: trả lời câu hỏi, đối thoại dựa trên kiến thức và tóm tắt văn bản.\n    *   **Phương pháp:** Halueval sử dụng hai phương pháp lấy mẫu khác nhau để tạo ra các mẫu đa dạng bằng cách sử dụng các hướng dẫn, sau đó lọc và chọn các mẫu khó. Ngoài ra, các người dán nhãn có trình độ được mời chú thích các ảo giác trong phản hồi ChatGPT cho các truy vấn của người dùng.\n    *   **Mục tiêu:** Halueval hướng đến việc tạo điều kiện cho nghiên cứu về việc tìm hiểu loại nội dung và mức độ mà LLM có xu hướng tạo ra ảo giác, cuối cùng mở đường cho việc xây dựng các LLM hiệu quả và đáng tin cậy hơn trong tương lai.",
    "reference": "Điểm chuẩn này tập trung vào việc đánh giá khả năng của LLM trong việc nhận biết ảo giác trong văn bản, nhưng không điều tra các lý do cơ bản đằng sau sự xuất hiện của ảo giác, khác với các nghiên cứu trước đây."
  },
  {
    "user_input": "Hướng dẫn lấy mẫu ảo giác được trình bày ở phần nào của phụ lục?",
    "retrieved_contexts": [
      "## **appendix**\n\nwe provide some extra information about our\nbenchmark as supplementary materials. the appendix is organized into three sections:\n\n  - instructions of hallucination sampling are presented in appendix a;\n\n  instructions of hallucination filtering are presented in appendix b;\n\n  - instructions of evaluation are presented in appendix c;\n\n  - details of our benchmark are presented in appendix d.\n## **a hallucination sampling**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 9 and\ntable 10, respectively.\n## **b hallucination filtering**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 11\nand table 12, respectively.\n## **c hallucination recognition**\n\nthe hallucination recognition instructions for qa,\ndialogue and summarization are shown in table 13,\ntable 14 and table 15, respectively.",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a.",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets.",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "table 2: instruction of hallucination sampling for question answering. the blue text denotes the intention descrip\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration.",
      "to select the most plausible and difficult hallucinated samples from the above two sampling methods. as shown in table 3, we design the instruction\nof hallucination filtering enhanced by ground-truth\nanswers to select the best answer from two hallucinated candidates. in the instruction of filtering,\nthe demonstration includes the ground-truth correct\nanswer ( *e.g.,* you.s. highway 60) and a hallucinated\ncounterpart ( *e.g.,* you.s. highway 70). while, in the\ntest example, we input two hallucinated answers.\nfollowing the demonstrations, we expect chatgpt\nto select one of the hallucinated answers that is the"
    ],
    "response": "Hướng dẫn lấy mẫu ảo giác được trình bày trong **phụ lục A**.",
    "reference": "Hướng dẫn lấy mẫu ảo giác được trình bày trong phụ lục A."
  },
  {
    "user_input": "Bảng nào cung cấp hướng dẫn nhận dạng ảo giác cho các nhiệm vụ như QA, hội thoại và tóm tắt?",
    "retrieved_contexts": [
      "## **appendix**\n\nwe provide some extra information about our\nbenchmark as supplementary materials. the appendix is organized into three sections:\n\n  - instructions of hallucination sampling are presented in appendix a;\n\n  instructions of hallucination filtering are presented in appendix b;\n\n  - instructions of evaluation are presented in appendix c;\n\n  - details of our benchmark are presented in appendix d.\n## **a hallucination sampling**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 9 and\ntable 10, respectively.\n## **b hallucination filtering**\n\nthe hallucination sampling instructions for dialogue and summarization are shown in table 11\nand table 12, respectively.\n## **c hallucination recognition**\n\nthe hallucination recognition instructions for qa,\ndialogue and summarization are shown in table 13,\ntable 14 and table 15, respectively.",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets.",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a.",
      "table 2: instruction of hallucination sampling for question answering. the blue text denotes the intention descrip\ntion, the red text denotes the hallucination pattern, and the green text denotes the hallucination demonstration."
    ],
    "response": "Hướng dẫn nhận dạng ảo giác cho QA (Question Answering), đối thoại và tóm tắt được thể hiện trong các bảng sau:\n\n1.  Bảng 13: Hướng dẫn nhận dạng ảo giác cho QA.\n2.  Bảng 14: Hướng dẫn nhận dạng ảo giác cho đối thoại.3.  Bảng 15: Hướng dẫn nhận dạng ảo giác cho tóm tắt.\n\nCác bảng này nằm trong **phụ lục C**.",
    "reference": "Bảng 13, 14 và 15 cung cấp hướng dẫn nhận dạng ảo giác cho QA, hội thoại và tóm tắt."
  },
  {
    "user_input": "Có bao nhiêu mẫu được tạo ra cho kiểu ảo giác p-iv trong nhiệm vụ qa?",
    "retrieved_contexts": [
      "-----\n\n**tasks** **#sample** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 10000 2280 1378 5102 1240\n**dialogue** 10000 8330 1196 474   **summa.** 10000 2614 3562 3824   \ntable 16: number of generated samples for each hallucination pattern (p-i/ii/iii/iv). “‘summa.” is short\nfor summarization. “-” is due to that we consider three\n\npatterns in dialogue and summarization.\n\n6464\n\n\n-----",
      "hallucinated samples, we introduce the hallucination pattern and demonstration, which are related\nto the seed task ( *e.g.,* qa in table 2). the few-shot\ndemonstrations can help the system to understand\nthe hallucination pattern. in this paper, we automatically generate hallucinated samples for three tasks,\n*i.e.,* question answering, knowledge-grounded dialogue, and text summarization. specifically, we\nconsider four types of hallucination patterns for\nquestion answering ( *i.e., comprehension*, *factual-*\n*ness*, *specificity*, and *inference* ) (zheng et al., 2023),\nthree types of hallucination patterns for knowledgegrounded dialogue ( *i.e., extrinsic-soft*, *extrinsic-*\n*hard*, and *extrinsic-grouped* ) (das et al., 2023),\nand three types of hallucination patterns for text\nsummarization ( *i.e., factual*, *non-factual*, and *in-*\n*trinsic* ) (cao et al., 2022). for these three tasks,\nwe first randomly sample 30 *,* 000 instances from",
      "with the automatic two-step generation process in\nsection 2.1, we produce a total of 30 *,* 000 hallucinated samples with 10 *,* 000 examples for each task\nof qa, dialogue, and summarization. we show\nthe number of generated samples for each hallucination pattern in table 16 at the appendix d.\nmoreover, we manually annotate 5 *,* 000 chatgpt\nresponses for general user queries in section 2.2.\nwe present a qa example and an annotated query\nand response example in table 4. among the annotated chatgpt responses, 977 responses are labeled as containing hallucination ( 19 *.* 5% ). finally,\nwe present the topic distributions of our generated\ntask-specific samples and annotated chatgpt responses in figure 2 and figure 3, ranging from\nfilm, sports to school, computer, technology, etc.\nwith our benchmark, researchers can use it to\ninvestigate or mitigate the hallucination issue for\nllms in three aspects. firstly, based on our generated and annotated samples, researchers can use",
      "most plausible and closest to the right answer. finally, the selected hallucinated sample is hard to be\nidentified, which are further used to evaluate llms\nin hallucination recognition. the instructions of\nhallucination filtering for dialogue and summarization are shown in table 11-12 in the appendix b.\nthrough the *sampling-then-filtering* process, we\nend up generating a total of 30 *,* 000 hallucinated\nsamples for the three tasks. our approach can also\nbe adapted to other tasks and datasets.",
      "-----\n\n**tasks** **#failed** **p-i** **p-ii** **p-iii** **p-iv**\n\n**qa** 3109 1559 245 278 1027\n**dialogue** 891 465 344 82  **summarization** 3868 3106 705 57  \ntable 6: number of samples where chatgpt fails to\nrecognize for each hallucination pattern (p-i/ii/iii/iv).",
      "we first randomly sample 30 *,* 000 instances from\nthe training set of hotpotqa (yang et al., 2018),\nopendialkg (moon et al., 2019), and cnn/daily\nmail (see et al., 2017), and then generate their hallucinated examples. the hallucination sampling\ninstructions for dialogue and summarization can be\nfound in table 9-10 in the appendix a."
    ],
    "response": "1.  **Tổng số mẫu**: Có tổng cộng 10,000 mẫu được tạo ra cho nhiệm vụ qa.\n2.  **Số lượng mẫu cho kiểu ảo giác p-iv**: Trong số đó, có 1,240 mẫu được tạo ra cho kiểu ảo giác p-iv. (tham khảo bảng 16)",
    "reference": "Có 1240 mẫu được tạo ra cho kiểu ảo giác p-iv trong nhiệm vụ qa."
  }
]