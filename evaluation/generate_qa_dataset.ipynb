{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899321da",
   "metadata": {},
   "source": [
    "### **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5ccd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up one directory level from the notebook folder\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd33c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PersonalProjects\\RAGChatbot\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from src.utils import dump_json\n",
    "from src.rag import (\n",
    "  chunk_pdf, \n",
    "  get_llm\n",
    ")\n",
    "\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb5072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "pdf_file = \"../data/halueval.pdf\"\n",
    "chunk_size = 1000\n",
    "chunk_overlap = int(0.1 * chunk_size)\n",
    "\n",
    "# Create QA folder\n",
    "qa_folder_path = \"../data/qa\"\n",
    "if not os.path.exists(qa_folder_path):\n",
    "  os.makedirs(qa_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5268af4",
   "metadata": {},
   "source": [
    "### **Generate QA dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb9091",
   "metadata": {},
   "source": [
    "**Chunk document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7967f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_pdf(pdf_file=pdf_file, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Retain only meaningful chunks\n",
    "remove_chunks = [4, 7, 8, 15, 16, 21, 27, 49, 50, 51, 52, 53, 60, 61, 62, 63, 64, 65, 66]\n",
    "chunks = [chunk for i, chunk in enumerate(chunks) if i not in remove_chunks and len(chunk) > 200] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0e1d9",
   "metadata": {},
   "source": [
    "**Define prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_gen_prompt = PromptTemplate(\n",
    "  template=\"\"\"\n",
    "    You are an expert at creating Vietnamese question-answer pairs to evaluate a RAG system.\n",
    "    Given the following document, generate three relevant question-answer pairs that mimic realistic questions a user might ask in a search query.\n",
    "    \n",
    "    Requirements:\n",
    "      - Include different question types (factual, inferential, analytical).\n",
    "      - Vary in difficulty (basic recall, complex reasoning).\n",
    "      - Test understanding of key concepts, relationships, and implications.\n",
    "    \n",
    "    The questions should be directly answerable from the documentation and should not require any external knowledge.\n",
    "    The questions and answers MUST NOT include phrases like \"theo đoạn văn\" or \"theo tài liệu\" or \"theo ngữ cảnh\".\n",
    "    The questions should sound natural and self-contained, as if a real user is asking without having seen the original text.\n",
    "    The questions and answers MUST be in Vietnamese.\n",
    "    The questions MUST NOT be too long or too short.\n",
    "    \n",
    "    Structure QA pairs following this format: {generate_format}\n",
    "\n",
    "    Document: {chunk}\n",
    "  \"\"\",\n",
    "  input_variables=[\"generate_format\", \"chunk\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cd9bb",
   "metadata": {},
   "source": [
    "**Define output format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df40ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTypeEnum(str, Enum):\n",
    "  factual = \"factual\"\n",
    "  inferential = \"inferential\"\n",
    "  analytical = \"analytical\"\n",
    "\n",
    "class QAPairMetaData(BaseModel):\n",
    "  question_type: QTypeEnum = Field(\n",
    "    description=\"Question type\"\n",
    "  )\n",
    "  required_context: str = Field(\n",
    "    description=\"Specific quote string from the chunk needed to answer the question\"\n",
    "  )\n",
    "  reasoning: str = Field(\n",
    "    description=\"A brief description of how you arrived at the answer from the context\"\n",
    "  )\n",
    "\n",
    "class QAPair(BaseModel):\n",
    "  metadata: QAPairMetaData\n",
    "  question: str\n",
    "  answer: str\n",
    "\n",
    "class QAPairList(BaseModel):\n",
    "  qa_pairs: List[QAPair]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97ee97",
   "metadata": {},
   "source": [
    "**Generate QA pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gen_output(gen_result: str, output_path: str):\n",
    "  \"\"\"\n",
    "    Process output & Dump to JSON file\n",
    "  \"\"\"\n",
    "  qa_pairs = ast.literal_eval(re.sub(r\"```json\\n|\\n```\", \"\", gen_result.content).strip())[\"qa_pairs\"]\n",
    "  for pair in qa_pairs:\n",
    "    dump_json(data=pair, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d166f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_llm()\n",
    "qa_gen_chain = qa_gen_prompt | llm\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "  # Avoid rate limit\n",
    "  if i != 0 and i % 15 == 0:\n",
    "    time.sleep(60)\n",
    "\n",
    "  gen_result = qa_gen_chain.invoke({\"generate_format\": QAPairList.model_json_schema(), \"chunk\": chunk})\n",
    "  process_gen_output(result=gen_result, output_path=f\"{qa_folder_path}/raw_qa_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30daff4a",
   "metadata": {},
   "source": [
    "### **Filter QA dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd319c",
   "metadata": {},
   "source": [
    "The generated question-answer pairs may contain some imperfections; therefore, we will use another model to filter and retain only high-quality pairs for evaluating the RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb21a3",
   "metadata": {},
   "source": [
    "**Define prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e326549",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_filter_prompt = PromptTemplate(\n",
    "  template=\"\"\"\n",
    "    You are an expert at filtering high-quality generated question-answer pairs to evaluate a RAG system.\n",
    "    Given a question-answer pair and it's relevant document, assess whether it meets the following quality criteria.\n",
    "\n",
    "    The quality criteria:\n",
    "      - Language Constraint: The question and answer MUST be in Vietnamese, allowing technical terms to be in English.\n",
    "      - Context Independence: Avoid phrases like \"theo đoạn văn\" or \"theo tài liệu\" or \"theo ngữ cảnh\"; the question and answer MUST be self-contained.\n",
    "      - Question Clarity: The question MUST be well-formed, unambiguous, grammatically correct, and natural—like something a real user would ask.\n",
    "      - Hallucination: The answer MUST be fully grounded in the provided document. If any part of the answer is not explicitly stated or logically inferred from the document, or appears to be guessed or assumed by the LLM, it is considered a hallucination and should be rejected.\n",
    "      - Answer Quality: The answer MUST be accurate, complete, concise, and grammatically correct.\n",
    "\n",
    "    Structure your response following this format: {filter_format}\n",
    "    \n",
    "    Document chunk: {chunk} \n",
    "\n",
    "    Now evaluate the question-answer pair below:\n",
    "    {qa_pair}\n",
    "  \"\"\",\n",
    "  input_variables=[\"filter_format\", \"qa_pair\", \"chunk\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fef993",
   "metadata": {},
   "source": [
    "**Define output format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityCriteria(str, Enum):\n",
    "  language_constraint = \"language_constraint\"\n",
    "  context_independence = \"context_independence\"\n",
    "  question_clarity = \"question_clarity\"\n",
    "  answerability = \"answerability\"\n",
    "  answer_quality = \"answer_quality\"\n",
    "\n",
    "class QAEval(BaseModel):\n",
    "  reasoning: str = Field(description=\"A brief explanation of your judge\")\n",
    "  violate_criteria: List[QualityCriteria] = Field(description=\"A list of quality criteria that are violated\")\n",
    "  judge: int = Field(description=\"0: if the QA pair violates one of those criteria, 1: if the QA pair meets all criteria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6ea32",
   "metadata": {},
   "source": [
    "**Filter QA pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c6b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_filter_output(filter_result: str, output_path: str):\n",
    "  clean_output = re.sub(r\"```json\\n|\\n```\", \"\", filter_result.content).strip()\n",
    "  data = ast.literal_eval(clean_output)\n",
    "  dump_json(data=data, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generated QA pairs\n",
    "with open(file=f\"{qa_folder_path}/raw_qa_dataset.json\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "  qa_pairs = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706945e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_chain = qa_filter_prompt | llm\n",
    "\n",
    "# Evaluate each QA pair\n",
    "for i, pair in enumerate(qa_pairs):\n",
    "  # Avoid rate limit\n",
    "  if i != 0 and i % 15 == 0:\n",
    "    time.sleep(60)\n",
    "  \n",
    "  question = pair[\"question\"]\n",
    "  answer = pair[\"answer\"]\n",
    "  qa_pair_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "  filter_result = filter_chain.invoke({\"filter_format\": QAEval.model_json_schema(), \"qa_pair\": qa_pair_text, \"chunk\": chunks[i // 3]})\n",
    "  process_filter_output(filter_result=filter_result, output_path=f\"{qa_folder_path}/filter_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c140108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtering results\n",
    "with open(file=f\"{qa_folder_path}/filter_results.json\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "  filter_results = json.load(file)\n",
    "\n",
    "assert len(qa_pairs) == len(filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f102a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_type</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>violate_criteria</th>\n",
       "      <th>judge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Halueval được tạo ra để làm gì?</td>\n",
       "      <td>Halueval là một chuẩn đánh giá quy mô lớn về h...</td>\n",
       "      <td>factual</td>\n",
       "      <td>The question and answer are both in Vietnamese...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Có bao nhiêu nhà nghiên cứu đã tham gia vào cô...</td>\n",
       "      <td>Có sáu nhà nghiên cứu đã tham gia vào công trì...</td>\n",
       "      <td>inferential</td>\n",
       "      <td>The question is clear and in Vietnamese. The a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Đại học Renmin của Trung Quốc tham gia vào ngh...</td>\n",
       "      <td>Đại học Renmin của Trung Quốc tham gia thông q...</td>\n",
       "      <td>analytical</td>\n",
       "      <td>The question and answer are both in Vietnamese...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Các mô hình ngôn ngữ lớn như ChatGPT có xu hướ...</td>\n",
       "      <td>Các mô hình ngôn ngữ lớn như ChatGPT có xu hướ...</td>\n",
       "      <td>factual</td>\n",
       "      <td>The question and answer are both in Vietnamese...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nội dung do các mô hình ngôn ngữ lớn tạo ra đư...</td>\n",
       "      <td>Nội dung được coi là 'ảo giác' khi nó mâu thuẫ...</td>\n",
       "      <td>inferential</td>\n",
       "      <td>The question and answer are both in Vietnamese...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                    Halueval được tạo ra để làm gì?   \n",
       "1  Có bao nhiêu nhà nghiên cứu đã tham gia vào cô...   \n",
       "2  Đại học Renmin của Trung Quốc tham gia vào ngh...   \n",
       "3  Các mô hình ngôn ngữ lớn như ChatGPT có xu hướ...   \n",
       "4  Nội dung do các mô hình ngôn ngữ lớn tạo ra đư...   \n",
       "\n",
       "                                              answer question_type  \\\n",
       "0  Halueval là một chuẩn đánh giá quy mô lớn về h...       factual   \n",
       "1  Có sáu nhà nghiên cứu đã tham gia vào công trì...   inferential   \n",
       "2  Đại học Renmin của Trung Quốc tham gia thông q...    analytical   \n",
       "3  Các mô hình ngôn ngữ lớn như ChatGPT có xu hướ...       factual   \n",
       "4  Nội dung được coi là 'ảo giác' khi nó mâu thuẫ...   inferential   \n",
       "\n",
       "                                           reasoning violate_criteria  judge  \n",
       "0  The question and answer are both in Vietnamese...               []      1  \n",
       "1  The question is clear and in Vietnamese. The a...               []      1  \n",
       "2  The question and answer are both in Vietnamese...               []      1  \n",
       "3  The question and answer are both in Vietnamese...               []      1  \n",
       "4  The question and answer are both in Vietnamese...               []      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Dataframe for the QA pairs\n",
    "qa_df = pd.DataFrame(qa_pairs)\n",
    "qa_df[\"question_type\"] = qa_df[\"metadata\"].apply(lambda item: item[\"question_type\"])\n",
    "qa_df = qa_df.drop(columns=\"metadata\")\n",
    "\n",
    "# Create a Dataframe for the filtering results\n",
    "filter_df = pd.DataFrame(filter_results)\n",
    "filter_df = qa_df.merge(right=filter_df, how=\"inner\", right_index=True, left_index=True)\n",
    "filter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674a8c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rejected QA pairs: 8\n",
      "\n",
      "=====================================================\n",
      "Chunk: **3.2.2** **improvement strategies**\n",
      "\n",
      "in this part, we design several strategies to improve\n",
      "the ability of llms to recognize hallucination. the\n",
      "results are shown in table 8.\n",
      "\n",
      "**knowledge retrieval.** retrieving relevant knowledge is a widely used strategy to eliminate hallucination (lewis et al., 2020; li et al., 2023a). therefore, we supply chatgpt with the knowledge facts\n",
      "retrieved from wikipedia (except for that summarization does not need external information besides\n",
      "\n",
      "Question: Ngoại trừ tóm tắt, những loại nhiệm vụ nào khác có thể hưởng lợi từ thông tin bên ngoài?\n",
      "Answer: Các nhiệm vụ khác ngoài tóm tắt có thể hưởng lợi từ thông tin bên ngoài.\n",
      "Violated criteria: answer_quality\n",
      "Reasoning: The answer is incomplete and does not provide specific tasks that benefit from external information. The question is clear, but the answer is too vague and does not fulfill the prompt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select rejected QA pairs\n",
    "rejected_df = filter_df[filter_df[\"judge\"] == 0]\n",
    "print(f\"Number of rejected QA pairs: {rejected_df.shape[0]}\\n\")\n",
    "\n",
    "# Print some rejected QA pairs\n",
    "for index, row in rejected_df.sample(n=1).iterrows():\n",
    "  print(\"=====================================================\")\n",
    "  print(f\"Chunk: {chunks[index // 3]}\\n\")\n",
    "  print(f\"Question: {row.question}\")\n",
    "  print(f\"Answer: {row.answer}\")\n",
    "  print(f\"Violated criteria: {', '.join(row.violate_criteria)}\")\n",
    "  print(f\"Reasoning: {row.reasoning}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a50c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store QA pairs in a JSON file\n",
    "quality_df = filter_df[filter_df[\"judge\"] == 1]\n",
    "quality_df = quality_df.drop(columns=[\"reasoning\", \"violate_criteria\", \"judge\"])\n",
    "quality_df.to_json(f\"{qa_folder_path}/qa_dataset.json\", orient=\"records\", force_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
