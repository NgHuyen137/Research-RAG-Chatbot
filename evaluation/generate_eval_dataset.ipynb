{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "793bfdd3",
   "metadata": {},
   "source": [
    "### **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd335d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f68870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from uuid import uuid4\n",
    "from src.utils import dump_json\n",
    "from src.workflow import get_graph_builder\n",
    "from src.rag import (\n",
    "  embed_pdf, \n",
    "  get_llm, \n",
    "  get_embedding_function, \n",
    "  get_rerank_function\n",
    ")\n",
    "\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1255961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Configuration passed to the Graph\n",
    "def create_config() -> dict:\n",
    "  llm = get_llm()\n",
    "  embedding_function = get_embedding_function()\n",
    "  rerank_function = get_rerank_function()\n",
    "  config = {\n",
    "    \"configurable\": {\n",
    "      \"thread_id\": str(uuid4()),\n",
    "      \"llm\": llm,\n",
    "      \"embedding_function\": embedding_function,\n",
    "      \"rerank_function\": rerank_function\n",
    "    }\n",
    "  }\n",
    "  return config\n",
    "\n",
    "config = create_config()\n",
    "\n",
    "# Define constants\n",
    "pdf_file = \"../data/halueval.pdf\"\n",
    "eval_folder_path = \"../data/eval\"\n",
    "exp_folder_path = f\"../data/exp/{str(uuid4())}\"\n",
    "chunk_size = 1000\n",
    "chunk_overlap = int(0.1 * chunk_size)\n",
    "\n",
    "# Create experiment folder\n",
    "os.makedirs(exp_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6d49d",
   "metadata": {},
   "source": [
    "### **Create evaluation dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d07fb",
   "metadata": {},
   "source": [
    "**Load the QA dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f45b5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=f\"{eval_folder_path}/qa_dataset.json\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "  qa_pairs = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac74ed2",
   "metadata": {},
   "source": [
    "**Construct the evaluation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda27252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# Init model\n",
    "llm = config[\"configurable\"][\"llm\"]\n",
    "\n",
    "# Init Graph\n",
    "graph_builder = get_graph_builder()\n",
    "\n",
    "# Embed document\n",
    "embed_pdf(\n",
    "  pdf_file=pdf_file,\n",
    "  config=config,\n",
    "  chunk_size=chunk_size,\n",
    "  chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0088c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_and_retrieved_contexts(question: str) -> tuple:\n",
    "  response = \"\"\n",
    "  input_message = HumanMessage(content=question)\n",
    "  for msg, metadata in graph_builder.stream({\"messages\": [input_message]}, config, stream_mode=\"messages\"):\n",
    "    if metadata[\"langgraph_node\"] == \"chatbot\":\n",
    "      response += msg.content\n",
    "  \n",
    "  output = (\n",
    "    response,\n",
    "    graph_builder.get_state(config).values[\"retrieved_docs\"]\n",
    "  )\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "520f9024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# Generate responses for each QA pair\n",
    "eval_dataset = []\n",
    "\n",
    "for i, pair in enumerate(qa_pairs):\n",
    "  # Avoid rate limit\n",
    "  if i != 0 and i % 7 == 0:\n",
    "    time.sleep(60)\n",
    "    \n",
    "  response, retrieved_contexts = get_response_and_retrieved_contexts(pair[\"question\"])\n",
    "  data = {\n",
    "    \"user_input\": pair[\"question\"],\n",
    "    \"retrieved_contexts\": retrieved_contexts,\n",
    "    \"response\": response,\n",
    "    \"reference\": pair[\"answer\"]\n",
    "  }\n",
    "  eval_dataset.append(data)\n",
    "  dump_json(data=data, output_path=f\"{exp_folder_path}/eval_dataset.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
